{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ba446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#   This work is part of the Core Imaging Library (CIL) developed by CCPi \n",
    "#   (Collaborative Computational Project in Tomographic Imaging), with \n",
    "#   substantial contributions by UKRI-STFC and University of Manchester.\n",
    "\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "#   Copyright 2021 UKRI-STFC\n",
    "#   Authored by: Evangelos Papoutsellis (UKRI-STFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361e939b-553e-4ae2-aa7e-9a01c456b0f7",
   "metadata": {},
   "source": [
    "<h1><center> Dynamic Sparse CT </center></h1>\n",
    "\n",
    "In this demo, we focus on different reconstruction methods for sparse dynamic CT using an open-access dynamic dataset available from [Heikkilä_et_al_Zenodo](https://zenodo.org/record/3696817#.YKTRP5MzZp9). The aim is to demonstrate how to increase\n",
    "the temporal resolution, or to reduce the radiation dose of a CT scan, without sacrificing\n",
    "the quality of the reconstructions.\n",
    "\n",
    "The gel phantom simulates diffusion of liquids inside plant stems, namely the flow of iodine-based contrast agents used in high resolution tomographic X-ray imaging of plants. In order to test different reconstruction methods, this radiation resistant phantom with similar diffusion properties was constructed. For more information, please see [Heikkilä_et_al](https://arxiv.org/abs/2003.02841).\n",
    "\n",
    "\n",
    "<h2><center><u> Learning objectives </u></center></h2>   \n",
    "\n",
    "- Create a **2D + channels** acquisition geometry for the dynamic tomographic data. Channels correspond to different time steps (frames).\n",
    "\n",
    "\n",
    "\n",
    "- Create **sparse data** using the `Slicer` processor.\n",
    "\n",
    "\n",
    "\n",
    "- Run FBP reconstruction for every time-channel.\n",
    "\n",
    "\n",
    "\n",
    "- Setup PDHG for 2 different regularisers:\n",
    "\n",
    "    * Spatiotemporal Total Variation: regularisation applied equally along the spatial and temporal dimensions (channels).\n",
    "        \n",
    "    * **Directional Total Variation (dTV)**: regularisation applied for each channel using a reference image reference.\n",
    "\n",
    "We first import all the necessary libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb69cd-f82a-4d1d-8291-7be94724866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from cil.framework import AcquisitionGeometry\n",
    "\n",
    "from cil.optimisation.algorithms import PDHG\n",
    "from cil.optimisation.operators import GradientOperator, BlockOperator\n",
    "from cil.optimisation.functions import IndicatorBox, BlockFunction, L2NormSquared, MixedL21Norm\n",
    "\n",
    "from cil.io import NEXUSDataWriter, NEXUSDataReader\n",
    "\n",
    "from cil.processors import Slicer\n",
    "\n",
    "from cil.plugins.astra.operators import ProjectionOperator\n",
    "from cil.plugins.astra.processors import FBP\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_dTV, FGP_TV\n",
    "\n",
    "from cil.utilities.display import show2D, show_geometry\n",
    "from cil.utilities.jupyter import islicer\n",
    "\n",
    "from utilities_dynamic_ct import read_frames, read_extra_frames\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e59272-ca91-4a6f-8892-74d3d877df83",
   "metadata": {},
   "source": [
    "# Data information: Gel phantom\n",
    "\n",
    "The sample is an agarose-gel phantom, perfused with a liquid contrast agent in a 50ml Falcon test tube (ø 29 × 115mm). The aim of this experiment was to simulate diffusion of liquids inside plant stems, which cannot withstand high radiation doses from a denser set of measurement angles. After the agarose solidified, five intact plastic straws were made into the gel and filled with 20% sucrose solution to guarantee the diffusion by directing osmosis to the gel body.\n",
    "\n",
    "Every measurement consists of 360 projections with 282 detector bins obtained from a flat-panel circular-scan cone-beam microCT-scanner. Only the central slice is provided, resulting in a 2D __fanbeam geometry__. The primary measurements consisted of 17 consecutive time frames, with initial stage of no contrast agent followed by steady increase and diffusion into the gel body over time. Data is given in two different resolutions corresponding to reconstructions of size:\n",
    "\n",
    "* 256 x 256: **GelPhantomData_b4.mat**\n",
    "* 512 x 512: **GelPhantomData_b2.mat**\n",
    "\n",
    "For this notebook, a 256x256 resolution is selected. In addition to the primary measurements, a more densely sampled measurements from the first time step and an additional 18th time step are provided in **GelPhantom_extra_frames.mat**\n",
    "\n",
    "* Pre-scan: **720 projections**\n",
    "* Post-scan: **1600 projections**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216910e5",
   "metadata": {},
   "source": [
    "## Load and read dynamic data (mat files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(\"/mnt/materials/SIRF/Fully3D/CIL/GelPhantom\")\n",
    "data_mat = \"GelPhantomData_b4\"\n",
    "file_info = read_frames(path, data_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec43085-b0cf-405b-85d6-81b1f2dffc48",
   "metadata": {},
   "source": [
    "From the `file_info` variable, we have all the information in order to define our acquisition geometry and create our CIL acquisition data.\n",
    "\n",
    "**Note that the pixel size of the detector is wrong. The correct pixel size should be doubled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48d1ca-3d74-4ce3-84df-7a3ae1230c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sinograms + metadata\n",
    "sinograms = file_info['sinograms']\n",
    "frames = sinograms.shape[0]\n",
    "angles = file_info['angles']\n",
    "distanceOriginDetector = file_info['distanceOriginDetector']\n",
    "distanceSourceOrigin = file_info['distanceSourceOrigin']\n",
    "# Correct the pixel size\n",
    "pixelSize = 2*file_info['pixelSize']\n",
    "numDetectors = file_info['numDetectors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e9acc-2839-4c4f-8f7c-f8abcb3fa3fc",
   "metadata": {},
   "source": [
    "## Exercise 1: Create acquisition and image geometries\n",
    "\n",
    "For this dataset, we have a 2D cone geometry with 17 time channels. Using the metadata above, we can define the acquisition geometry `ag` with \n",
    "\n",
    "\n",
    "```python    \n",
    "    \n",
    "    ag = AcquisitionGeometry.create_Cone2D(source_position = [0, distanceSourceOrigin],\n",
    "                                           detector_position = [0, -distanceOriginDetector])\\\n",
    "                                        .set_panel(numDetectors, pixelSize)\\\n",
    "                                        .set_channels(frames)\\\n",
    "                                        .set_angles(angles, angle_unit=\"radian\")\\\n",
    "                                        .set_labels(['channel','angle', 'horizontal'])\n",
    "\n",
    "    \n",
    "```\n",
    "\n",
    "For the image geometry `ig` we use the following code and crop our image domain to `[256,256]`:\n",
    "\n",
    "```python    \n",
    "    \n",
    "    ig = ag.get_ImageGeometry()\n",
    "    \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e8a23-f9f1-4c2f-b299-38c1ae4ecbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create acquisition + image geometries\n",
    "ag = AcquisitionGeometry.create_Cone2D(source_position = ...,\n",
    "                                       detector_position = ...)\\\n",
    "                                    .set_panel(numDetectors, ...)\\\n",
    "                                    .set_channels(...)\\\n",
    "                                    .set_angles(angles, angle_unit=\"radian\")\\\n",
    "                                    .set_labels(['channel','angle', 'horizontal'])\n",
    "\n",
    "ig = ag.get_ImageGeometry()\n",
    "ig.voxel_num_x = 256\n",
    "ig.voxel_num_y = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c1fd4",
   "metadata": {},
   "source": [
    "## Exercise 1: Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0fdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create acquisition + image geometries\n",
    "ag = AcquisitionGeometry.create_Cone2D(source_position = [0, distanceSourceOrigin],\n",
    "                                       detector_position = [0, -distanceOriginDetector])\\\n",
    "                                    .set_panel(numDetectors, pixelSize)\\\n",
    "                                    .set_channels(frames)\\\n",
    "                                    .set_angles(angles, angle_unit=\"radian\")\\\n",
    "                                    .set_labels(['channel','angle', 'horizontal'])\n",
    "\n",
    "ig = ag.get_ImageGeometry()\n",
    "ig.voxel_num_x = 256\n",
    "ig.voxel_num_y = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb5244-f172-44f6-8e54-dd87626e1805",
   "metadata": {},
   "source": [
    "Then, we create an `AcquisitionData` by allocating space from the acquisition geometry `ag`. This is filled with every **sinogram per time channel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc31ca-d229-4721-9310-09745d644cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ag.allocate()\n",
    "for i in range(frames):\n",
    "   data.fill(sinograms[i], channel = i) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdaffa7-02fd-44b4-8f1b-c2dc50e41c67",
   "metadata": {},
   "source": [
    "## Show acquisition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790295f8-877c-406e-bbf2-7c93020885b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "islicer(data, direction=0, cmap=\"inferno\", title=\"Time frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6605f74-304a-4c5c-86c8-fec75eef8e0b",
   "metadata": {},
   "source": [
    "## Show acquisition geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4563c07-a745-4ca5-b306-346587721b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_geometry(ag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000a898-da61-4bac-ab5a-ab13b910589d",
   "metadata": {},
   "source": [
    "## Create Sparse CT data\n",
    "\n",
    "In order to simulate a sparse acquisition, from the total of 360 projections we select a number of projections depending on the size of the `step`:\n",
    "\n",
    "- step = 1 --> 360/1 projections\n",
    "- step = 5 --> 360/5 = 72 projections\n",
    "- step = 10 --> 360/10 = 36 projections\n",
    "- step = 20 --> 360/20 = 18 projections\n",
    "\n",
    "We create the sparse data using the `Slicer` processor. For every case, we show the dynamic data for 4 different time frames and save them using the `NEXUSDataWriter` processor in the `SparseData` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a613e-44ab-40fe-aff2-906abdd98ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save Sparse Data with different angular sampling: 18, 36, 72, 360 projections\n",
    "for step in [1, 5, 10, 20]:\n",
    "    \n",
    "    name_proj = \"data_{}\".format(int(360/step))\n",
    "    new_data = Slicer(roi={'angle':(0,360,step)})(data)\n",
    "    ag = new_data.geometry\n",
    "    \n",
    "    show2D(new_data, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",\n",
    "                       cmap=\"inferno\", title=\"Projections {}\".format(int(360/step)), size=(25, 20))    \n",
    "    \n",
    "    writer = NEXUSDataWriter(file_name = \"SparseData/\"+name_proj+\".nxs\",\n",
    "                         data = new_data)\n",
    "    writer.write()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa28bc0-8b06-4abb-ae0d-67fba045d2f2",
   "metadata": {},
   "source": [
    "For the rest of the notebook, we use the sparse acquisition data `data_36`, i.e., only **36 projections** and perform the following:\n",
    "\n",
    "- FBP reconstruction per time frame.\n",
    "- Spatiotemporal TV reconstruction.\n",
    "- Directional Total variation.\n",
    "\n",
    "For the other cases, you can change the value of the `num_proj` below. Available data are: `data_18`, `data_36`, `data_72` and `data_360`.\n",
    "\n",
    "<a id='num_proj'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65ce57-be14-4994-bf69-ee5b6b7c4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proj = 36\n",
    "reader = NEXUSDataReader(file_name = \"SparseData/data_{}.nxs\".format(num_proj) )\n",
    "data = reader.load_data()  \n",
    "ag = data.geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db351f1-809d-4661-b274-75a5d2049afb",
   "metadata": {},
   "source": [
    "## Channelwise FBP\n",
    "\n",
    "For the **channelwise** FBP reconstruction, we perform the following steps\n",
    "\n",
    "- Allocate a space using the full image geometry (2D+channels) `ig` geometry. \n",
    "- Extract the 2D acquisition and image geometries, using `ag.subset(channel=0)` and `ig.subset(channel=0)`.\n",
    "- Run FBP reconstruction using the 2D sinogram data for every time frame.\n",
    "- Fill the 2D FBP reconstruction with respect to the `channel=i` using the `fill` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adba47-2b6e-4510-8d83-00aab10d504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbp_recon = ig.allocate()\n",
    "\n",
    "ag2D = ag.subset(channel=0)\n",
    "ig2D = ig.subset(channel=0)\n",
    "\n",
    "for i in range(ig.channels):\n",
    "    \n",
    "    tmp = FBP(ig2D, ag2D)(data.subset(channel=i))\n",
    "    fbp_recon.fill(tmp, channel=i)\n",
    "    print(\"Finish FBP reconstruction for time frame {}\".format(i), end='\\r')\n",
    "        \n",
    "show2D(fbp_recon, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=\"Projections {}\".format(i), size=(25, 20))\n",
    "    \n",
    "name_fbp = \"FBP_projections_{}\".format(num_proj)\n",
    "writer = NEXUSDataWriter(file_name = \"FBP_reconstructions/\"+name_fbp+\".nxs\",\n",
    "                     data = fbp_recon)\n",
    "writer.write()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abe4be-8b5a-47a1-b066-cc74ad700bc1",
   "metadata": {},
   "source": [
    "## Exercise 2: Total Variation reconstruction\n",
    "\n",
    "For the TV reconstruction, we use the **Explicit formulation** of the PDHG algorithm. See the [PDHG notebook](../Week2/03_PDHG.ipynb) for more information.\n",
    "\n",
    "- Define the `ProjectionOperator` and the `GradientOperator` using `correlation=SpaceChannels`.\n",
    "\n",
    "\n",
    "```python    \n",
    "\n",
    "        A = ProjectionOperator(ig, ag, 'gpu')        \n",
    "        Grad = GradientOperator(ig, correlation = \"SpaceChannels\") \n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "- Use the `BlockOperator` to define the operator $K$. \n",
    "\n",
    "    \n",
    "\n",
    "```python    \n",
    "\n",
    "        K = BlockOperator(A, Grad)        \n",
    "            \n",
    "```\n",
    "\n",
    "\n",
    "- Use the `BlockFunction` to define the function $\\mathcal{F}$ that contains the fidelity term `L2NormSquared(b=data)` and the regularisation term `alpha_tv * MixedL21Norm()`, with `alpha_tv = 0.00063`. Finally, use the `IndicatorBox(lower=0.0)` to enforce a non-negativity constraint for the function $\\mathcal{G}$.\n",
    "\n",
    "\n",
    "```python    \n",
    "\n",
    "        F = BlockFunction(0.5*L2NormSquared(b=data), alpha_tv * MixedL21Norm()) \n",
    "        G = IndicatorBox(lower=0)\n",
    "            \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b8d75-0be2-4d6f-b3e7-06b937a5d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ...        \n",
    "Grad = ...\n",
    "\n",
    "K = BlockOperator(A, Grad)\n",
    "\n",
    "alpha_tv = 0.00054\n",
    "F = BlockFunction(..., alpha_tv * ...)  \n",
    "G = IndicatorBox(lower=0)\n",
    "\n",
    "normK = K.norm()\n",
    "sigma = 1./normK\n",
    "tau = 1./normK\n",
    "\n",
    "pdhg_tv = PDHG(f = ..., g = ..., operator=..., max_iteration = 500,\n",
    "            update_objective_interval = 100)    \n",
    "pdhg_tv.run(verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe2dde",
   "metadata": {},
   "source": [
    "## Exercise 2: Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ba912",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ProjectionOperator(ig, ag, 'gpu')        \n",
    "Grad = GradientOperator(ig, correlation = \"SpaceChannels\") \n",
    "\n",
    "K = BlockOperator(A, Grad)\n",
    "\n",
    "alpha_tv = 0.00054\n",
    "F = BlockFunction(0.5*L2NormSquared(b=data), alpha_tv * MixedL21Norm())  \n",
    "G = IndicatorBox(lower=0)\n",
    "\n",
    "normK = K.norm()\n",
    "sigma = 1./normK\n",
    "tau = 1./normK\n",
    "\n",
    "pdhg_tv = PDHG(f = F, g = G, operator=K, max_iteration = 500,\n",
    "            update_objective_interval = 100)    \n",
    "pdhg_tv.run(verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859d906-95dc-4288-ad5c-38091897b280",
   "metadata": {},
   "source": [
    "## Show TV reconstruction for 4 different time frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fd5a6-d608-49f8-bea2-ab6b90a2fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_sinos = [\"Time-frame {}\".format(i) for i in [0,5,10,16]]\n",
    "\n",
    "show2D(pdhg_tv.solution, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))\n",
    "    \n",
    "name_tv = \"TV_reconstruction_projections_{}\".format(num_proj)\n",
    "writer = NEXUSDataWriter(file_name = \"TV_reconstructions/\"+name_tv+\".nxs\",\n",
    "                     data = pdhg_tv.solution)\n",
    "writer.write()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd8bd6-93df-4d40-878f-db7a89da9e98",
   "metadata": {},
   "source": [
    "## Directional Total Variation\n",
    "\n",
    "For our final reconstruction, we use a **structure-based prior**, namely the directional Total Variation (dTV) introduced in [Ehrhardt MJ, Arridge SR](https://doi.org/10.1109/tip.2013.2277775).\n",
    "\n",
    "In comparison with the Total variation regulariser, \n",
    "$$\n",
    "\\mathrm{TV}(u) = \\|\\nabla u\\|_{2,1} = \\sum |\\nabla u\\|_{2},\n",
    "$$\n",
    "\n",
    "in the Direction Total variation, a weight in front of the gradient is used based on a **reference image**. This acts as prior information from which edge structures are propagated into the reconstruction process. For example, an image from another modality, e.g., MRI, , can be used in the PET reconstruction, see [Ehrhardt2016](https://ieeexplore.ieee.org/document/7452643/), [Ehrhardt2016MRI](https://epubs.siam.org/doi/10.1137/15M1047325). Another popular setup, is to use either both modalities or even channels in a joint reconstruction problem simultaneously, improving significantly the quality of the image, see for instance [Knoll et al](https://ieeexplore.ieee.org/document/7466848), [Kazantsev_2018](https://doi.org/10.1088/1361-6420/aaba86).\n",
    "\n",
    "**Definition:** The dTV regulariser of the image $u$ given the reference image $v$ is defined as \n",
    "\n",
    "<a id='dynamic_dTV'></a>\n",
    "$$\\begin{equation}\n",
    "d\\mathrm{TV}(u,v)  := \\|D_{v}\\nabla u\\|_{2,1} = \\sum_{i,j=1}^{M,N} \\big(|D_{v}\\nabla u|_{2}\\big)_{i,j},\n",
    "\\label{dTV_definition}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the weight $D_{v}$ depends on the normalised gradient $\\xi_{v}$ of the reference image $v$, \n",
    "\n",
    "$$\\begin{equation}\n",
    "D_{v} = \\mathbb{I}_{2\\times2} - \\xi_{v}\\xi_{v}^T, \\quad \\xi_{v} = \\frac{\\nabla v}{\\sqrt{\\eta^{2} + |\\nabla v|_{2}^{2}}}, \\quad \\eta>0.\n",
    "\\label{weight_D}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In this dynamic sparse CT framework, we apply the dTV regulariser for each time frame $t$ which results to the following minimisation problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "u^{*}_{t} = \\underset{u}{\\operatorname{argmin}}  \\, \\frac{1}{2}\\| A_\\text{sc} u_{t}  - b_{t} \\|^{2} + \\alpha \\, d\\mathrm{TV}(u_{t}, v_{t})\\quad \\mbox{(Dynamic dTV)}, \\label{dynamic_dtv_problem}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $A_\\text{sc}$, $b_{t}$, $u^{*}_{t}$, denote the single channel `ProjectionOperator`, the sinogram data and the reconstructed image for the time frame $t$ respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d62bab-eacc-45be-ac6d-29d79fbccbd5",
   "metadata": {},
   "source": [
    "## Reference images\n",
    "\n",
    "In terms of the reference images $(v_{t})_{t=0}^{16}$, we are going to use the FBP reconstructions of the additional tomographic data. There are two datasets in `GelPhantom_extra_frames.mat` with dense sampled measurements from the first and last (18th) time steps:\n",
    "\n",
    "- Pre-scan data with 720 projections\n",
    "- Post-scan data with 1600 projections\n",
    "\n",
    "We first read the matlab files and create the following acquisition data:\n",
    "\n",
    "- data_pre_scan\n",
    "- data_post_scan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf186561-29ef-4b18-a10b-ae8d2db502f7",
   "metadata": {},
   "source": [
    "## Read matlab files for the extra frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a75d1f-ae7c-4f0b-ac1c-844247166922",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.abspath(\"/mnt/materials/SIRF/Fully3D/CIL/GelPhantom\")\n",
    "data_mat_extra = \"GelPhantom_extra_frames\"\n",
    "\n",
    "pre_scan_info = read_extra_frames(path, data_mat_extra, \"GelPhantomFrame1_b4\")\n",
    "post_scan_info = read_extra_frames(path, data_mat_extra, \"GelPhantomFrame18_b4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1984f-5e70-41cc-82c3-a3a4c59cf6c2",
   "metadata": {},
   "source": [
    "## Acquisition geometry for the 1st frame: 720 projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97e40a-98b4-4436-a147-e2c9aab73e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag2D_pre_scan = AcquisitionGeometry.create_Cone2D(source_position = [0,   pre_scan_info['distanceSourceOrigin']],\n",
    "                                       detector_position = [0, -pre_scan_info['distanceOriginDetector']])\\\n",
    "                                    .set_panel(num_pixels = pre_scan_info['numDetectors'], pixel_size = 2*pre_scan_info['pixelSize'])\\\n",
    "                                    .set_angles(pre_scan_info['angles'])\\\n",
    "                                    .set_labels(['angle', 'horizontal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a41a1-b787-4e3b-a627-48da216d1464",
   "metadata": {},
   "source": [
    "## Acquisition geometry for the 18th frame: 1600 projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1183f-91d2-4d1a-89df-480f2fd53d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag2D_post_scan = AcquisitionGeometry.create_Cone2D(source_position = [0,   post_scan_info['distanceSourceOrigin']],\n",
    "                                       detector_position = [0, -post_scan_info['distanceOriginDetector']])\\\n",
    "                                    .set_panel(num_pixels = post_scan_info['numDetectors'], pixel_size = 2*post_scan_info['pixelSize'])\\\n",
    "                                    .set_angles(post_scan_info['angles'])\\\n",
    "                                    .set_labels(['angle', 'horizontal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c760c-28b6-4e89-a941-eb1cbbc4657b",
   "metadata": {},
   "source": [
    "## Create Acquisition data: Pre-scan, Post-scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7c17b1-231a-4f19-ba31-05812985422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_scan = ag2D_pre_scan.allocate()\n",
    "data_pre_scan.fill(pre_scan_info['sinograms'])\n",
    "\n",
    "data_post_scan = ag2D_post_scan.allocate()\n",
    "data_post_scan.fill(post_scan_info['sinograms'])\n",
    "\n",
    "\n",
    "show2D([data_pre_scan,data_post_scan], title=[\"Pre-scan 720 projections\", \"Post-scan 1600 projections\"], cmap=\"inferno\", size=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d73521-626f-479d-a667-23d73aabb388",
   "metadata": {},
   "source": [
    "## FBP reconstruction (Reference images)\n",
    "\n",
    "For the FBP reconstruction of the pre/post scan we use the 2D image geometry, `ig2D` and the corresponding acquisition geometries `ag2D_pre_scan` and `ag2D_post_scan`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df7512c",
   "metadata": {},
   "source": [
    "## Exercise 3: Perform FBP to obtain the reference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac4fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbp_recon_pre_scan = FBP(..., ag2D_pre_scan)(...)\n",
    "fbp_recon_post_scan = FBP(ig2D, ...)(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29778abc",
   "metadata": {},
   "source": [
    "## Exercise 3: Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd67b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbp_recon_pre_scan = FBP(ig2D, ag2D_pre_scan)(data_pre_scan)\n",
    "fbp_recon_post_scan = FBP(ig2D, ag2D_post_scan)(data_post_scan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab93a30e",
   "metadata": {},
   "source": [
    "## Show and save the reference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62345b-bf6b-4a1e-88b9-97ef8b34ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D([fbp_recon_pre_scan,fbp_recon_post_scan], \n",
    "       title=[\"FBP: Pre-scan\", \"FBP: Post-scan\"], cmap=\"inferno\", origin=\"upper\", fix_range=(0,0.065))\n",
    "\n",
    "name_fbp_pre_scan = \"FBP_pre_scan\"\n",
    "writer = NEXUSDataWriter(file_name = \"FBP_reconstructions/\"+name_fbp_pre_scan+\".nxs\",\n",
    "                     data = fbp_recon_pre_scan)\n",
    "writer.write() \n",
    "\n",
    "name_fbp_post_scan = \"FBP_post_scan\"\n",
    "writer = NEXUSDataWriter(file_name = \"FBP_reconstructions/\"+name_fbp_post_scan+\".nxs\",\n",
    "                     data = fbp_recon_post_scan)\n",
    "writer.write() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da9dea-48cd-4b1a-b4f7-cab06bbf5c8f",
   "metadata": {},
   "source": [
    "## Edge information from the normalised gradient $\\,\\xi_{v}$\n",
    "\n",
    "In the following we compute the normalised gradient $\\,\\xi_{v}$ for the two reference images using different $\\eta$ values:\n",
    "\n",
    "$$\\xi_{v} = \\frac{\\nabla v}{\\sqrt{\\eta^{2} + |\\nabla v|_{2}^{2}}}, \\quad \\eta>0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed73e9-7851-4d2e-a93b-8f974d316af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xi_vector_field(image, eta):\n",
    "    \n",
    "    ig = image.geometry\n",
    "    ig.voxel_size_x = 1.\n",
    "    ig.voxel_size_y = 1.\n",
    "    G = GradientOperator(ig)\n",
    "    numerator = G.direct(image)\n",
    "    denominator = np.sqrt(eta**2 + numerator.get_item(0)**2 + numerator.get_item(1)**2)\n",
    "    xi = numerator/denominator\n",
    "                \n",
    "    return (xi.get_item(0)**2 + xi.get_item(1)**2).sqrt()\n",
    "\n",
    "etas = [0.001, 0.005]\n",
    "\n",
    "xi_post_scan = []\n",
    "xi_pre_scan = []\n",
    "\n",
    "for i in etas:\n",
    "    \n",
    "    xi_post_scan.append(xi_vector_field(fbp_recon_post_scan, i))\n",
    "    xi_pre_scan.append(xi_vector_field(fbp_recon_pre_scan, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72cb2a0-de7e-4abe-a2d8-328bc9114b6a",
   "metadata": {},
   "source": [
    "## Edge information from the pre-scan and post-scan reference images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ffe34-e99f-468d-903c-fb21925ed2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_etas = [\"$\\eta$ = {}\".format(eta) for eta in etas]\n",
    "show2D(xi_pre_scan, cmap=\"inferno\", title=title_etas, origin=\"upper\", num_cols=2, size=(10,10))\n",
    "show2D(xi_post_scan, cmap=\"inferno\", title=title_etas, origin=\"upper\", num_cols=2,size=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e561a-8246-4dc8-ae5c-57b648c6fa87",
   "metadata": {},
   "source": [
    "## Directional Total variation reconstruction \n",
    "\n",
    "In total we have 17 time frames, and we need 17 reference images.  Due to a slight movement of the sample at the beginning of the experiment, we apply the pre-scan reference image for the first time frame and use the post-scan reference image for the remaining time frames. \n",
    "\n",
    "One could apply other configurations for the reference image in the intermediate time frames. For example, in order to reconstruct the $(t+1)$th time frame, one could use the $t$th time frame reconstruction as reference. A more sophisticated reference selection approach is applied in hyperspectral computed tomography in [Kazantsev_2018](https://iopscience.iop.org/article/10.1088/1361-6420/aaba86)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bec37-f388-4178-880a-6ba95c0f1e39",
   "metadata": {},
   "source": [
    "## Setup and run the PDHG algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a0c6a-96ee-4ed5-805f-3df8a031005f",
   "metadata": {},
   "source": [
    "In order to solve the [Dynamic dTV](#dynamic_dTV) problem, we use the implicit formulation of the PDHG algorithm, where the **Fast Gradient Projection** algorithm, under the **dTV** regulariser, is used for the inner proximal problem.\n",
    "\n",
    "- We first define the single slice `ProjectionOperator` using the 2D image and acquisition geometries and compute the `sigma` and `tau` stepsizes.\n",
    "\n",
    "\n",
    "```python    \n",
    "\n",
    "    K = ProjectionOperator(ig2D, ag2D, 'gpu') \n",
    "\n",
    "    normK = K.norm()\n",
    "    sigma = 1./normK\n",
    "    tau = 1./normK   \n",
    "    \n",
    "```\n",
    "\n",
    "\n",
    "- We allocate space for the dTV reconstruction, i.e., `dtv_recon` using the full image geometry `ig`.\n",
    "\n",
    "\n",
    "\n",
    "- Use the following values: `max_iteration=100`, `alpha_dtv = 0.0072`, `eta=0.005`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Loop over all the time frames (`tf`) and update :\n",
    "\n",
    "    - the fidelity term `0.5 * L2NormSquared(b=data.subset(channel=tf)` for the function $\\mathcal{F}$,        \n",
    "        \n",
    "    - the regularisation term ( $\\mathcal{G}$ ) using the `FGP_dTV` function class from the CIL plugin of the CCPi-Regularisation Toolkit. For `tf=0` the pre-scan reference is used and for `tf>0` the post-scan reference is used.    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75e61f7-eb7d-4354-945c-d575e116afc7",
   "metadata": {},
   "source": [
    "## Define single slice projection operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f3c694-ed1c-43d5-90b7-29088c0c8459",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = ProjectionOperator(ig2D, ag2D, 'gpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94f4a5-d428-43df-8dc1-961b1004c545",
   "metadata": {},
   "source": [
    "## Parameters for the dTV regulariser and the PDHG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9af47c-f786-4d99-9ff7-d936a2e35910",
   "metadata": {},
   "outputs": [],
   "source": [
    "normK = K.norm()\n",
    "sigma = 1./normK\n",
    "tau = 1./normK  \n",
    "\n",
    "dtv_recon = ig.allocate()\n",
    "\n",
    "max_iterations = 100\n",
    "alpha_dtv = 0.0072\n",
    "eta = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e7e1f-7991-4dd8-816a-2ea76430f620",
   "metadata": {},
   "source": [
    "## Loop over all channels and update:\n",
    "\n",
    "* the acquisition data in the `L2NormSquared` fidelity term,\n",
    "* the reference image in the `FGP_dTV` regulariser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbb0d7-a484-4fb7-8f2f-212e9e6237c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tf in range(ig.channels):\n",
    "    \n",
    "    F = 0.5 * L2NormSquared(b=data.subset(channel=tf))\n",
    "    \n",
    "    if tf==0:        \n",
    "        G = alpha_dtv * FGP_dTV(reference = fbp_recon_pre_scan, eta=eta, device='gpu')  \n",
    "    else:        \n",
    "        G = alpha_dtv * FGP_dTV(reference = fbp_recon_post_scan, eta=eta, device='gpu')\n",
    "        \n",
    "   \n",
    "    pdhg_dtv = PDHG(f=F, g=G, operator = K, tau = tau, sigma = sigma,\n",
    "            max_iteration = max_iterations, update_objective_interval=100)\n",
    "    pdhg_dtv.run(verbose = 0)\n",
    "    \n",
    "    clear_output(wait=True) \n",
    "    \n",
    "    print(\"Finish dTV regularisation for the frame {} with {} projections\\n\".format(tf,num_proj), end='\\r')\n",
    "    \n",
    "    dtv_recon.fill(pdhg_dtv.solution, channel=tf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2eaf6c-0479-4f78-8670-6a68f5074a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(dtv_recon, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1a631-aa8d-4f66-83a4-84782dab1721",
   "metadata": {},
   "source": [
    "## FBP vs TV vs dTV reconstructions vs FBP (360 projections).\n",
    "\n",
    "For our final comparison, we reconstruct the full dataset, i.e., 360 projections and compare it with the FBP, TV and dTV reconstruction with 36 projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71de54-8b99-4bd8-aea0-2de88afc4709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with 360 projections\n",
    "reader = NEXUSDataReader(file_name = \"SparseData/data_360.nxs\")\n",
    "data_360 = reader.load_data()\n",
    "ag2D = data_360.geometry.subset(channel=0)\n",
    "\n",
    "# Perform channelwise FBP reconstruction\n",
    "fbp_recon_360 = ig.allocate()\n",
    "for i in range(ig.channels):    \n",
    "    tmp = FBP(ig2D, ag2D)(data_360.subset(channel=i))\n",
    "    fbp_recon_360.fill(tmp, channel=i)     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d70544d-ca1c-4e70-8732-30f724940ac7",
   "metadata": {},
   "source": [
    "## Show FBP, TV, dTV with 36 projections and FBP with 360 projections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35380b-d9c7-407c-acbd-0c74f9245a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(fbp_recon,slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))\n",
    "\n",
    "show2D(pdhg_tv.solution,slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))\n",
    "\n",
    "show2D(dtv_recon,slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))\n",
    "\n",
    "show2D(fbp_recon_360,slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccef18b",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "As an additional **Exercise**, you can try other configurations, namely the datasets `data_18` and `data_72` by changing the [number of projections](#num_proj). The optimal regularisation parameters for spatiotemporal TV and dTV are reported in Table 1 in [Papoutsellis et al](https://arxiv.org/pdf/2102.06126.pdf#page=7)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1cb920-fcec-4c29-924b-af4222039ee1",
   "metadata": {},
   "source": [
    "<h1><center>Conclusions</center></h1>\n",
    "\n",
    "In this notebook, we presented three different reconstruction methods for undersampled dynamic tomographic data. The **channelwise FBP** and the **Spatiotemporal TV** and **Directional TV** regularisation. We focused on the reconstruction of the dataset with 36 projections out of 360 projections. \n",
    "\n",
    "So far, we presented how to reconstruct two different multichannel datasets:\n",
    "\n",
    "* **[Colour](01_Colour_Processing.ipynb)**: 3 channels that encodes information for the red, green and blue colours.\n",
    "* **Dynamic**: 17 channels that encode temporal information from our acquisition tomographic data.\n",
    "\n",
    "In the next notebook, we focus on a 4D hyperspectral dataset where the channel dimension contains spectral energy information acquired from an energy-sensitive X-ray detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d1df8-6948-467a-9759-53153c8eac1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
