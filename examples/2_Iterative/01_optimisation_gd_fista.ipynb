{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#  Copyright 2019 - 2022 United Kingdom Research and Innovation\n",
    "#  Copyright 2019 - 2022 The University of Manchester\n",
    "#  Copyright 2019 - 2022 Technical University of Denmark\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "#   Authored by:    Jakob S. JÃ¸rgensen (DTU)\n",
    "#                   Gemma Fardell (UKRI-STFC)     \n",
    "#                   Laura Murgatroyd (UKRI-STFC)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of optimisation-based reconstruction in CIL\n",
    "\n",
    "### The case is a 3D parallel-beam synchrotron dataset of a steel wire.\n",
    "\n",
    "First we import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import algorithms, operators and functions from CIL optimisation module\n",
    "from cil.optimisation.algorithms import GD, FISTA, PDHG\n",
    "from cil.optimisation.operators import BlockOperator, GradientOperator,\\\n",
    "                                       GradientOperator\n",
    "from cil.optimisation.functions import IndicatorBox, MixedL21Norm, L2NormSquared, \\\n",
    "                                       BlockFunction, L1Norm, LeastSquares, \\\n",
    "                                       OperatorCompositionFunction, TotalVariation, \\\n",
    "                                       ZeroFunction\n",
    "\n",
    "# Import CIL Processors for preprocessing\n",
    "from cil.processors import CentreOfRotationCorrector, Slicer, TransmissionAbsorptionConverter\n",
    "\n",
    "# Import CIL display function\n",
    "from cil.utilities.display import show2D\n",
    "\n",
    "# Import from CIL ASTRA plugin\n",
    "from cil.plugins.astra import ProjectionOperator\n",
    "\n",
    "# Import FBP from CIL recon class\n",
    "from cil.recon import FBP\n",
    "\n",
    "#Import Total Variation from the regularisation toolkit plugin\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_TV\n",
    "\n",
    "# All external imports\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as in the notebook [1_Introduction/03_preprocessing](../1_Introduction/03_preprocessing.ipynb), we load the steel-wire demonstration data provided as part of CIL, carry out some preprocessing and FBP reconstructions for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example data set\n",
    "from cil.utilities.dataexample import SYNCHROTRON_PARALLEL_BEAM_DATA\n",
    "data_sync = SYNCHROTRON_PARALLEL_BEAM_DATA.get()\n",
    "\n",
    "# Preprocessing\n",
    "scale = data_sync.get_slice(vertical=20).mean()\n",
    "data_sync = data_sync/scale\n",
    "data_sync = TransmissionAbsorptionConverter()(data_sync)\n",
    "data_sync = CentreOfRotationCorrector.xcorrelation(slice_index='centre')(data_sync)\n",
    "\n",
    "# Crop data and reorder for ASTRA backend\n",
    "data90 = Slicer(roi={'angle':(0,90), \n",
    "                     'horizontal':(20,140,1)})(data_sync)\n",
    "data90.reorder(order='astra')\n",
    "\n",
    "# Set up and run FBP for 90-angle dataset\n",
    "recon90 = FBP(data90, backend='astra').run(verbose=0)\n",
    "\n",
    "# Set up and run FBP for 15-angle dataset\n",
    "data15 = Slicer(roi={'angle': (0,90,6)})(data90)\n",
    "recon15 = FBP(data15, backend='astra').run(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom display function to reuse for visualizing all reconstructions consistently\n",
    "sx = 44\n",
    "sz = 103\n",
    "ca1 = -0.01\n",
    "ca2 =  0.11\n",
    "slices = [('horizontal_x',sx),('vertical',sz)]\n",
    "myshow = lambda vol : show2D(vol, \n",
    "                             slice_list=slices, \n",
    "                             cmap='inferno', \n",
    "                             fix_range=(ca1,ca2), \n",
    "                             origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a slice of the 90-degree FBP reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(recon90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the 15-projection FBP reconstruction, which contains severe streak artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(recon15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation-based reconstruction is based on a fully discretized model that is conventionally assumed to be linear:\n",
    "$$Au = b$$\n",
    "where $A$  is the linear operator known as the system matrix representing forward projection of an image to its sinogram, $b$ is the sinogram data, and $u$ is the unknown image to be reconstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to need is the LinearOperator representing forward and back-projections. We set up the ProjectionOperator from the CIL-ASTRA plugin by passing the 15-projection acquisition geometry, and image geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = data15.geometry\n",
    "ig = ag.get_ImageGeometry()\n",
    "A = ProjectionOperator(ig, ag, device=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to work with the 15-projection dataset here and refer to it by `b` for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, as we normally have noise, model errors and other inconsistencies in the data, we cannot expect a solution exists to $Au = b$.  We therefore relax the problem and aim to find a solution that is as close as possible to fitting the data. This is conventionally measured in a least-squares sense in that we solve the least-squares problem\n",
    "$$ \\min_u \\|Au - b \\|_2^2 $$\n",
    "where\n",
    "$$\\|y \\|_2^2 = \\sum_i y_i^2.$$\n",
    "The function that is to be minimized is called the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction is the image $u$ that is the solution to the optimisation problem, i.e., that results in the lowest possible value of the objective function, in this case of the (squared) residual norm $\\|Au - b \\|_2^2$. In order to find the solution we use an iterative optimisation algorithm. Many exist, perhaps the most basic one is the gradient descent method, which is available in CIL as the GD algorithm. To set it up we need to specify the objective function in terms of a CIL Function. For least-squares this is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = LeastSquares(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In iterative algorithms we must provide an initial point from which to start, here we choose the zero image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = ig.allocate(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f1` is a CIL Function and CIL Functions can be evaluated for particular input images, for example we can evaluate it (which is the starting objective value of the optimisation problem)  for `x0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIL Functions provide a number of methods that are used by optimisation algorithms, most notably, if a function is smooth (continuously differentiable), then a CIL Function provides the `gradient` method using which the gradient of the function can be evaluated at a particular input image. For example we can evaluate the gradient at `x0` and since it contains an element for each voxel, we can display it as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(f1.gradient(x0),slice_list=slices,origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the gradient descent algorithm, we specify:\n",
    " - `initial` - the initial point \n",
    " - `objective_function` - the objective function\n",
    " - `step_size` - whether to use a fixed step size or a back-tracking line search (None)\n",
    " - `max_iteration` - the maximal number of iterations to run\n",
    " - `update_objective_interval` - how often to evaluate the objective function value\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_LS = GD(initial=x0, \n",
    "             objective_function=f1, \n",
    "             step_size=None, \n",
    "             max_iteration=1000, \n",
    "             update_objective_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the algorithm is set up, we can run it for a specified number of iterations and here using `verbose=1` to print out progress information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_LS.run(300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, the latest image/solution in the algorithm can be shown as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myGD_LS.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a basic least-squares example. We can specify more involved optimisation problems by combining multiple CIL Functions through addition, scalar multiplication as well as composition with CIL operators. For example, as an alternative to using CGLS to solve the Tikhonov problem with gradient operator D, i.e.,  \n",
    "$$\\min_u \\|Au-b\\|_2^2 + \\alpha^2\\|Du\\|_2^2$$\n",
    "This is covered in detail by the next notebook [02_tikhonov_block_framework.ipynb](02_tikhonov_block_framework.ipynb)\n",
    "\n",
    "We can set this objective function up step by step. First, we set again the least-squares data fitting term as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = LeastSquares(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the operator `D` in the regularisation term and the value of the regularisation parameter `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = GradientOperator(ig)\n",
    "alpha = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the regularisation term by composing the squared L2-norm with the operator D as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = OperatorCompositionFunction(L2NormSquared(),D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we form the full optimisation problem from the components defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f1 + (alpha**2)*f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can set up a gradient descent algorithm to solve this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_tikh = GD(initial=x0, \n",
    "               objective_function=f, \n",
    "               step_size=None, \n",
    "               max_iteration=1000, \n",
    "               update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_tikh.run(200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myGD_tikh.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we solve the Tikhonov problem using CGLS. As an exercise  you can compare the result and performance of the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many useful reconstruction methods involve minimisation of functions that are NOT smooth and in those cases we need dedicated optimisation algorithms for non-smooth problems. In this notebook we consider optimisation problems that can be written in the form\n",
    "$$\\min_u f(u) + g(u)  $$\n",
    "where $f$ is a smooth function as before, but $g$ may now be non-smooth. $g$ further needs to be \"simple\", in a certain sense, namely it should have a proximal mapping that is easy to evaluate. Proximal mapping can be thought of a generalisation of the gradient for a non-smooth function.\n",
    "\n",
    "For this problem class the algorithm FISTA (Fast iterative shrinkage thresholding algorithm) can be employed. It is also known as the accelerated proximal gradient method.\n",
    "\n",
    "We consider a couple of examples for different functions $g$. First we consider again least-squares but with a non-negativity constraint on all pixels. This problem can be written \n",
    "$$\\min_u \\|Au-b\\|_2^2 + I_{C}(u) $$\n",
    "where $I_{C}(u)$ is a special convex function known as an indicator function, which takes on the value 0 in its convex domain C (which we here take to be the set of images with only nonnegative pixel values), and the (extended) value of $\\infty$ outside its domain. This can be specified in CIL using an `IndicatorBox` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = LeastSquares(A, b)\n",
    "G = IndicatorBox(lower=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A FISTA algorithm instance can be set up similarly to the GD algorithm but specifying the $f$ and $g$ functions separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTANN = FISTA(f=F, \n",
    "                  g=G, \n",
    "                  initial=x0, \n",
    "                  max_iteration=1000,\n",
    "                  update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run it and display the resulting solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTANN.run(300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myFISTANN.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the non-negativity constraint, as expected, prevents any negative values. Furthermore, this has a positive effect of removing some of the streak artifacts in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is sparsity regularisation which we can achieve by choosing $g$ as the L1-norm multiplied by a regularisation parameter $\\alpha$ to balance the strength of fitting to the data and enforcing regularisation:\n",
    "$$g(u) = \\alpha\\|u\\|_1 = \\alpha\\sum_u |u_i| $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 30\n",
    "G = a1*L1Norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we set up a FISTA algorithm instance and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTAL1 = FISTA(f=F, \n",
    "                  g=G, \n",
    "                  initial=x0, \n",
    "                  max_iteration=1000, \n",
    "                  update_objective_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTAL1.run(300,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the solution of L1 regularised least-squares produced by FISTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myFISTAL1.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all small values of the background, and the lowest-density parts of the sample, have been forced to be zero by the sparsity regularisation term, keeping only the pixel values of the largest magnitude. Sparsity regularisation does not directly enforce smoothing, which is seen in the image by neighbouring pixel values being rather different in the part of the image that is not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a better option is to enforce sparsity of the image gradient. This is known as Total Variation (TV) regularisation and tends to enforce piecewise constant areas separated by sharp edges. Recall that for example Tikhonov regularisation may reduce noise but tends to blur edges, so TV may have an advantage if the image to be reconstructed is known to consist of relatively homogeneous areas separated by sharp edges. In CIL, TV is available as the `TotalVariation` function and from the CCPi regularisation toolkit as `FGP_TV` which can be run on the `GPU`. We can set up and solve the TV-regularised problem in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTV = 0.02*FGP_TV(device='gpu') \n",
    "myFISTATV = FISTA(f=F, \n",
    "                  g=GTV, \n",
    "                  initial=x0 ,\n",
    "                  max_iteration=1000, \n",
    "                  update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the proximal mapping of Total Variation is not simple but needs to be evaluated numerically, but this is handled by the `TotalVariation` and `FGP_TV` functions, however it does take a while to run which is why we use the `GPU` implementation on real data (approximately 3 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTATV.run(200,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the slices of the TV reconstruction by FISTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myFISTATV.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that TV-regularisation successfully compensates for the streak artifacts caused by few projections, suppresses noise and preserves sharp edges in the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more flexible algorithm for non-smooth problems is the Primal Dual Hybrid Gradient (PDHG) algorithm, which also goes under other names such as the Chambolle-Pock algorithm. In PDHG we can split complicated functionals into simpler parts for which the proximal mapping can be evaluated. PDHG will be covered in more detail in a separate notebook [03_PDHG.ipynb](03_PDHG.ipynb), here it is demonstrated how to set up the same TV-regularised problem we just solved with FISTA. Note how `BlockFunctions` and `BlockOperators` are used to specify multiple terms/operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.02\n",
    "F = BlockFunction(L2NormSquared(b=b), \n",
    "                  alpha*MixedL21Norm())\n",
    "K = BlockOperator(A, \n",
    "                  D)\n",
    "G = ZeroFunction()\n",
    "myPDHG = PDHG(f=F, \n",
    "              g=G, \n",
    "              operator=K, \n",
    "              max_iteration=10000, \n",
    "              update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm for a specified number of iterations with increased verbosity/amount of printing to screen.\n",
    "\n",
    "Here we initially run for 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPDHG.run(500,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the TV-regularised solution obtained by the PDHG Algorithm and note it is identical with the one from FISTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myPDHG.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIL Algorithms can record history of objective values (primal and dual for PDHG) for monitoring convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(myFISTATV.iterations[1:], myFISTATV.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.dual_objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.primal_dual_gap[1:])\n",
    "plt.ylim((1e0,1e5))\n",
    "plt.legend(['FISTA','PDHG primal','PDHG dual','PDHG gap'])\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Objective value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the two algorithms achieve approximately the same (primal) objective value. PDHG in addition supplies the dual objective, and the primal-dual gap (difference of primal of dual objectives) which helps for monitoring convergence. This primal-dual gap tends to zero as the algorithm approaches the solution. \n",
    "\n",
    "Although in practice fewer iterations are often sufficient for a visually converged image to see a very well converged primal-dual gap we want to run for more iterations. It will take approximately 5 minutes to run an additional 4500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPDHG.run(4500,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myPDHG.solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(myFISTATV.iterations[1:], myFISTATV.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.dual_objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.primal_dual_gap[1:])\n",
    "plt.ylim((1e0,1e5))\n",
    "plt.legend(['FISTA','PDHG primal','PDHG dual','PDHG gap'])\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Objective value')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43cbf82c2f716cd564b762322e13d4dbd881fd8a341d231fe608abc3118da208"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cil_22.0.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
