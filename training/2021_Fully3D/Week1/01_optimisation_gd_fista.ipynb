{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of optimisation-based reconstruction in CIL\n",
    "\n",
    "### The case is a 3D parallel-beam synchrotron dataset of a steel wire.\n",
    "\n",
    "First we import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all CIL framework components needed\n",
    "from cil.framework import ImageData, ImageGeometry, AcquisitionData, AcquisitionGeometry, BlockDataContainer\n",
    "\n",
    "# Import algorithms, operators and functions from CIL optimisation module\n",
    "from cil.optimisation.algorithms import CGLS, GD, FISTA, PDHG\n",
    "from cil.optimisation.operators import BlockOperator, GradientOperator, IdentityOperator, \\\n",
    "                                       GradientOperator, FiniteDifferenceOperator\n",
    "from cil.optimisation.functions import IndicatorBox, MixedL21Norm, L2NormSquared, \\\n",
    "                                       BlockFunction, L1Norm, LeastSquares, \\\n",
    "                                       OperatorCompositionFunction, TotalVariation, \\\n",
    "                                       ZeroFunction\n",
    "\n",
    "# Import CIL Processors for preprocessing\n",
    "from cil.processors import CentreOfRotationCorrector, Slicer, TransmissionAbsorptionConverter\n",
    "\n",
    "# Import CIL display function\n",
    "from cil.utilities.display import show2D\n",
    "\n",
    "# Import from CIL ASTRA plugin\n",
    "from cil.plugins.astra.processors import FBP, AstraBackProjector3D\n",
    "from cil.plugins.astra.operators import ProjectionOperator\n",
    "\n",
    "# All external imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as in the notebook 03_preprocessing from week 1, we load the steel-wire demonstration data provided as part of CIL, carry out some preprocessing and FBP reconstructions for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example data set\n",
    "from cil.utilities.dataexample import SYNCHROTRON_PARALLEL_BEAM_DATA\n",
    "data_sync = SYNCHROTRON_PARALLEL_BEAM_DATA.get()\n",
    "\n",
    "# Preprocessing\n",
    "scale = data_sync.get_slice(vertical=20).mean()\n",
    "data_sync = data_sync/scale\n",
    "data_sync = TransmissionAbsorptionConverter()(data_sync)\n",
    "data_sync = CentreOfRotationCorrector.xcorrelation(slice_index='centre')(data_sync)\n",
    "\n",
    "# Crop data and reorder for ASTRA backend\n",
    "data90 = Slicer(roi={'angle':(0,90), \n",
    "                     'horizontal':(20,140,1)})(data_sync)\n",
    "data90.reorder(order='astra')\n",
    "\n",
    "# Set up and run FBP for 90-angle dataset\n",
    "ag90 = data90.geometry\n",
    "ig = ag90.get_ImageGeometry()\n",
    "recon90 = FBP(ig, ag90, device='gpu')(data90)\n",
    "\n",
    "# Set up and run FBP for 15-angle dataset\n",
    "data15 = Slicer(roi={'angle': (0,90,6)})(data90)\n",
    "ag = data15.geometry\n",
    "recon15 = FBP(ig, ag, device='gpu')(data15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom display function to reuse for visualizing all reconstructions consistently\n",
    "sx = 44\n",
    "sz = 103\n",
    "ca1 = -0.01\n",
    "ca2 =  0.11\n",
    "myshow = lambda vol : show2D(vol, \n",
    "                             slice_list=[('horizontal_x',sx),('vertical',sz)], \n",
    "                             cmap='inferno', \n",
    "                             fix_range=(ca1,ca2), \n",
    "                             origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a slices of the 90-degree FBP reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(recon90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the 15-projection FBP reconstruction, which contains severe streak artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(recon15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation-based reconstruction is based on a fully discretized model that is conventionally assumed to be linear:\n",
    "\\\\[Au = b \\\\]\n",
    "where \\\\(A\\\\)  is the linear operator known as the system matrix representing forward projection of an image to its sinogram, \\\\(b\\\\) is the sinogram data, and \\\\(u\\\\) is the unknown image to be reconstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to need is the LinearOperator representing forward and back-projections. We set up the ProjectionOperator from the CIL-ASTRA plugin by passing the 15-projection image and acquisition geometries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ProjectionOperator(ig, ag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to work with the 15-projection dataset here and refer to it by `b` for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, as we normally have noise, model errors and other inconsistencies in the data, we cannot expect a solution exists to \\\\(Au = b\\\\).  We therefore relax the problem and aim to find a solution that is as close as possible to fitting the data. This is conventionally measured in a least-squares sense in that we solve the leastproblem\n",
    "\\\\[ \\min_u \\|Au - b \\|_2^2 \\\\]\n",
    "where\n",
    "\\\\[\\|y \\|_2^2 = \\sum_i y_i^2\\\\].\n",
    "The function that is to be minimized is called the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction is the image \\\\(u\\\\) solves the optimisation problem, i.e., that results in the lowest possible value of the objective function, in this case of the (squared) residual norm \\\\(\\|Au - b \\|_2^2 \\\\). In order to find the solution we use an iterative optimisation algorithm. Many exist, the perhaps most basic one is the gradient descent method, which is available in CIL as the GD algorithm. To set it up we need to specify the objective function in terms of a CIL Function. For least-squares this is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = LeastSquares(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In iterative algorithms we must provide an initial point from which to start, here we choose the zero image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = ig.allocate(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f1` is a CIL Function and CIL Functions can be evaluated for particular input images, for example we can evaluate it (which is the starting objective value of the optimisation problem)  for `x0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIL Functions provide a number of methods that are used by optimisation algorithms, most notably, if a function is smooth (continuously differentiable), then a CIL Function provides the `gradient` method using which the gradient of the function can be evaluated at a particular input image. For example we can evaluate the gradient at `x0` and since it contains an element for each voxel, we can display it as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(f1.gradient(x0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the gradient descent algorithm, we specify the initial point, the objective function, whether to use a fixed step size or a back-tracking line search (None), the maximal number of iterations to run, and how often to evaluate the objective function value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_LS = GD(x_init=x0, \n",
    "             objective_function=f1, \n",
    "             step_size=None, \n",
    "             max_iteration=1000, \n",
    "             update_objective_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the algorithm is set up, we can run it for a specified number of iterations and here using `verbose=1` to print out progress information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_LS.run(300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, the latest image/solution in the algorithm can be shown as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myGD_LS.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a basic least-squares example. We can specify more involved optimisation problems by combining multiple CIL Functions through addition, scalar multiplication as well as composition with CIL operators. For example, as an alternative to using CGLS to solve the Tikhonov problem with gradient operator D, i.e.,  \n",
    "\\\\[\\min_u \\|Au-b\\|_2^2 + \\alpha^2\\|Du\\|_2^2\\\\]\n",
    "as covered by a different exercise, we can set this objective function up step by step.\n",
    "\n",
    "First, we set again the least-squares data fitting term as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = LeastSquares(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the operator `D` in the regularisation term and the value of the regularisation parameter `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = GradientOperator(ig)\n",
    "alpha = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the regularisation term by composing the squares L2-norm with the operator D as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = OperatorCompositionFunction(L2NormSquared(),D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we form the full optimisation problem from the composnents defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f1 + (alpha**2)*f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can set up a gradient descent algorithm to solve this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_tikh = GD(x_init=x0, \n",
    "               objective_function=f, \n",
    "               step_size=None, \n",
    "               max_iteration=1000, \n",
    "               update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_tikh.run(200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myGD_tikh.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an exercise you may specify and solve the same Tikhonov problem using CGLS as in a previous notebook, and compare the result and performance with using the GD algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many useful reconstruction methods involve minimisation of functions that are NOT smooth and in those cases we need dedicated optimisation algorithms for non-smooth problems. In this notebook we consider optimisation problems that can be written in the form\n",
    "\\\\[\\min_u f(u) + g(u)  \\\\]\n",
    "where \\\\(f\\\\) is a smooth function as before, but \\\\(g\\\\) may now be non-smooth. \\\\(g\\\\) further needs to be \"simple\", in a certain sense, namely it should have a proximal mapping that is easy to evaluate. Proximal mapping can be thought of a generalisation of the gradient for a non-smooth function.\n",
    "\n",
    "For this problem class the algorithm FISTA (Fast iterative shrinkage thresholding algorithm) can be employed. It is also known as the accelerated proximal gradient method.\n",
    "\n",
    "We consider a couple of examples for different functions \\\\(g\\\\). First we consider again least-squares but with a non-negativity constraint on all pixels. This problem can be written \n",
    "\\\\[\\min_u \\|Au-b\\|_2^2 + I_{C}(u) \\\\]\n",
    "where $I_{\\mathbb{R}^+}(u)$ is a special convex function known as an indicator function, which takes on the value 0 in its convex domain C (which we here take to be the set of images with only nonnegative pixel values), and the (extended) value of \\\\(\\infty\\\\) outside its domain. This can be specified in CIL using an IndicatorBox function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = LeastSquares(A, b)\n",
    "G = IndicatorBox(lower=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A FISTA algorithm instance can be set up similarly to the GD algorithm but specifying the \\\\(f\\\\) and \\\\(g\\\\) functions separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTANN = FISTA(f=F, \n",
    "                  g=G, \n",
    "                  x_init=x0, \n",
    "                  max_iteration=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run it and display the resulting solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTANN.run(300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myFISTANN.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the nonnegativity constraint, as expected, prevents any values from being negative. Furthermore, this has a positive effect of removing some of the streak artifacts in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is sparsity regularization which we can achieve by choosing \\\\(g\\\\) as the L1-norm multiplied by a regularisation parameter \\\\(\\alpha\\\\) to balance the strength of fitting to the data and enforcing regularisation:\n",
    "\\\\[g(u) = \\alpha\\|u\\|_1 = \\alpha\\sum_u |u_i| \\\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = 30\n",
    "G = a1*L1Norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we set up a FISTA algorithm instance and run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTAL1 = FISTA(f=F, \n",
    "                  g=G, \n",
    "                  x_init=x0, \n",
    "                  max_iteration=1000, \n",
    "                  update_objective_interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTAL1.run(300,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the solution of L1 regularised least-squares produced by FISTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myFISTAL1.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all small values of the background, and part of the sample, have been forced to be zero by the sparsity regularisation term, keeping only the pixel values of the largest magnitude. Sparsity regularisation does not directly enforce smoothing, which is seen in the image by neighbouring pixel values being rather different in the part of the image that is not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An option that is sometimes better is to enforce sparsity of the image gradient. This is known as Total Variation (TV) regularisation and tends to enforce piecewise constant areas separated by sharp edges. Recall that for example Tikhonov regularisation may reduce noise but tends to blur edges, so TV may have an advantage if the image to be reconstructed is known to consist of relatively homogenuous areas separated by sharp edges. In CIL, TV is available as the TotalVariation function, and we can set up and solve the TV-regularised problem in the same way as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTV = 0.02*TotalVariation()\n",
    "myFISTATV = FISTA(f=F, \n",
    "                  g=GTV, \n",
    "                  x_init=x0 ,\n",
    "                  max_iteration=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the proximal mapping of TotalVariation is not simple but needs to be evaluated numerically, but this is handled by the TotalVariation function, however it does take a while to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTATV.run(200,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the slices of the TV reconstruction by FISTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myFISTATV.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We that TV-regularisation succesfully compensates for the streak artifacts caused by few projections, suppresses noise and preserves sharp edges in the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more flexible algorithm for non-smooth problems is the Primal Dual Hybrid Gradient (PDHG) algorithm, which also goes under other names such as the Chambolle-Pock algorithm. In PDHG we can split complicated functionals into simpler parts for which the proximal mapping can be evaluated. PDHG will be covered in more detail in a separate notebook, here it is demonstrated how to set up the same TV-regularized problem we just solved with FISTA. Note how BlockFunctions and BlockOperators are used to specify multiple terms/operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.02\n",
    "F = BlockFunction(L2NormSquared(b=b), \n",
    "                  alpha*MixedL21Norm())\n",
    "K = BlockOperator(A, \n",
    "                  D)\n",
    "G = ZeroFunction()\n",
    "myPDHG = PDHG(f=F, \n",
    "              g=G, \n",
    "              operator=K, \n",
    "              max_iteration=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm for a specified number of iterations with increased verbosity/amount of printing to screen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPDHG.run(5000,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the TV-regularized solution obtained by the PDHG Algorithm and note it is identical with the one from FISTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myshow(myPDHG.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIL Algorithms can record history of objective values (primal and dual for PDHG) for monitoring convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(myFISTATV.iterations[1:], myFISTATV.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.dual_objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], np.array(myPDHG.objective[1:])-np.array(myPDHG.dual_objective[1:]))\n",
    "plt.ylim((1e0,1e5))\n",
    "plt.legend(['FISTA','PDHG primal','PDHG dual','PDHG gap'])\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Objective value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the two algorithms achieve apprximately the same (primal) objective value. PDHG in addition supplies the dual objective, and the primal-dual gap (difference of primal of dual objectives) which helps for monitoring convergence, as it tends to zero as the algorithm approaches the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ciljj21dev] *",
   "language": "python",
   "name": "conda-env-ciljj21dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
