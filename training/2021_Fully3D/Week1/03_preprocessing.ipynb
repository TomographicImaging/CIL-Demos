{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52842c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Copyright 2019 Science Technology Facilities Council\n",
    "# Copyright 2019 University of Manchester\n",
    "#\n",
    "# This work is part of the Core Imaging Library developed by Science Technology\t\n",
    "# Facilities Council and University of Manchester\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0.txt\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# \n",
    "#========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-stress",
   "metadata": {},
   "source": [
    "# CT data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-ethiopia",
   "metadata": {},
   "source": [
    "Sometimes we are lucky to have a (simulated) dataset which is ready to reconstruct. However sometimes we get raw data which we need to preprocess first to get sensible reconstruction.\n",
    "CIL provides a number of useful image manipulation tools - processors. \n",
    "In this notebook we will demonstrate some of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-spectrum",
   "metadata": {},
   "source": [
    "## Learning objectives:\n",
    "- Read in and manipulate data\n",
    "- Compensate for centre-of-rotation offset\n",
    "- Slice and bin data\n",
    "- Remove hot/ dead pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-lingerie",
   "metadata": {},
   "source": [
    "We start with some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cil imports\n",
    "from cil.framework import ImageData, ImageGeometry\n",
    "from cil.framework import AcquisitionGeometry, AcquisitionData\n",
    "\n",
    "from cil.processors import CentreOfRotationCorrector, Slicer, \\\n",
    "    Binner, Masker, MaskGenerator, TransmissionAbsorptionConverter\n",
    "\n",
    "from cil.plugins.astra.processors import FBP\n",
    "\n",
    "from cil.utilities import dataexample\n",
    "from cil.utilities.display import show2D, show_geometry\n",
    "\n",
    "\n",
    "# External imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "logging.basicConfig(level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38038188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up default colour map for visualisation\n",
    "cmap = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-institute",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-desktop",
   "metadata": {},
   "source": [
    "We use the steel-wire dataset from the Diamond Light Source (DLS). The dataset is icluded in CIL for demonstration purposes and can be loaded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a reader object pointing to the Nexus data set\n",
    "data_raw = dataexample.SYNCHROTRON_PARALLEL_BEAM_DATA.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137817d5",
   "metadata": {},
   "source": [
    "We load not only data array itself but also corresponding metadata, i.e. `AcquisitionGeometry`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3243f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise data\n",
    "show2D(data_raw, slice_list=[('angle',0), ('angle', 30), ('angle',60)], \\\n",
    "        cmap=cmap, num_cols=1, size=(15,15), origin='upper-left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and show geometry\n",
    "show_geometry(data_raw.geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-research",
   "metadata": {},
   "source": [
    "## Transmission to absorption convertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-summit",
   "metadata": {},
   "source": [
    "From contrast and background values we infer that the dataset has ben already flat field corrected. However background values are < than 1. We simply rescale intensity values by taking mean of an empty slice and dividing the data array by the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = data_raw.subset(vertical=20).mean()\n",
    "data_raw /= background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-reproduction",
   "metadata": {},
   "source": [
    "To convert from transmission contrast to absorption contrast we will aply negative logarithm.\n",
    "We have implemented `TransmissionAbsorptionConverter()` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from transmission to attenuation \n",
    "data_exp = TransmissionAbsorptionConverter()(data_raw)\n",
    "# and plot again\n",
    "show2D(data_exp, slice_list=[('angle',0), ('angle', 30), ('angle',60)], \\\n",
    "        cmap=cmap, num_cols=1, size=(15,15), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-fleet",
   "metadata": {},
   "source": [
    "## Removing bad pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-hearing",
   "metadata": {},
   "source": [
    "Image contrast does not look right. The reason for this is a single dead pixel in the top left corner. Dead/ stuck/ misperforming pixels is a very common problem. Sometimes there are only a few of them and they will be effectively filtered out during reconstruction. However sometimes flat field images look like a night sky and misperforming\n",
    "pixels can significantly impair reconstructed image quality. CIL provides processors to deal with both cases. More advanced options will be discussed later in this notebook.\n",
    "\n",
    "There are several ways to remove the misperforming pixel in this case. The simplest one is to crop top slice. It's just air, so we will not loose any information.\n",
    "\n",
    "`Slicer()` is a processor used to slice the data, similar to numpy slicing. To crop the data pass the region of interest parameter `roi`. This is a dictionary where each element defines the behaviour along one dimension. \n",
    "To crop along an axis pass a tuple containing the start, end coordinates and step. `roi={vertical: (index0, index1)}` will crop the data between `index0` and `index1` along `vertical` dimension.\n",
    "`None` is a shortcut; in the fist posiiton it corresponds to the first item, in the second to the last item along the axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-production",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_crop = Slicer(roi={'vertical': (1, None)})(data_exp)\n",
    "\n",
    "show2D(data_crop, slice_list=[('angle',0), ('angle', 30), ('angle',60)], \\\n",
    "        cmap=cmap, num_cols=1, size=(15,15), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-appliance",
   "metadata": {},
   "source": [
    "Looks better, isn't it? Alternatively we can use a processor for outlier detection. It is called `MaskGenerator()`. \n",
    "`MaskGenerator()` is a powerful tool to detect outliers, which was inspired by MATLAB rmoutliers\n",
    "function. It supports a number of methods including simple threshold and quantiles along with statistical median, mean, moving median and moving mean methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-mumbai",
   "metadata": {},
   "source": [
    "In this case, a simple threshold is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-motorcycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = MaskGenerator.threshold(max_val=10)(data_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-nursing",
   "metadata": {},
   "source": [
    "Now `mask` is a binary image which contains 0 where outliers were detected and 1 for other pixels. We use `Masker()` to mask out the detected outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_masked = Masker.interpolate(mask=mask, method='nearest', axis='vertical')(data_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-effort",
   "metadata": {},
   "source": [
    "Let's visualise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(data_masked, slice_list=[('angle',0), ('angle', 30), ('angle',60)], \\\n",
    "        cmap=cmap, num_cols=1, size=(15,15), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-authority",
   "metadata": {},
   "source": [
    "Note that data_crop and data_masked will have different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_crop shape: {}'.format(data_crop.shape))\n",
    "print('data_masked shape: {}'.format(data_masked.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-heaven",
   "metadata": {},
   "source": [
    "## FBP reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-racing",
   "metadata": {},
   "source": [
    "The next step is to reconstruct the dataset. In this notebook we will use simple FBP for reconstruction. More advanced methods will be discussed in the next notebooks. \n",
    "\n",
    "CIL supports different back-ends for which data order conventions may differ. Here we use the FBP algorithm from the ASTRA toolbox. In 3D geometry the ASTRA toolbox requires the dataset in the form `['vertical','angle','horizontal']`, which doesn't match the DLS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('old dimension labels: {}'.format(data_crop.dimension_labels))\n",
    "data_crop.reorder(order='astra')\n",
    "print('new dimension labels: {}'.format(data_crop.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-reform",
   "metadata": {},
   "source": [
    "Now we are ready to run FBP reconstruction. Remember, reconstruction requires `ImageGeometry` and `AcquisitionGeometry`. `data_crop` contains the dataset itself along with all metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acquisiiton geometry\n",
    "ag = data_crop.geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-likelihood",
   "metadata": {},
   "source": [
    "Here we use a default `ImageGeometry` which matches the `AcquisitionGeometry`. Obviously one can modify the `ImageGeometry` to reconstruct on coarser/ finer grid or perform ROI reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create image geometry\n",
    "ig = ag.get_ImageGeometry()\n",
    "\n",
    "fbp_recon = FBP(ig, ag, device='gpu')(data_crop)\n",
    "\n",
    "# visualise reconstruction results\n",
    "show2D(fbp_recon, slice_list=[('vertical',80), ('vertical',100), ('horizontal_x',80)], \\\n",
    "        cmap=cmap, num_cols=1, size=(15,15), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-howard",
   "metadata": {},
   "source": [
    "## Centre of Rotation correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-senate",
   "metadata": {},
   "source": [
    "Reconstructed slices do not look good. An experienced CT reconstruction person will immediately recongnise that there is an offset in the centre of rotation.\n",
    "\n",
    "In a well aligned CT system the axis of rotation is perpendicular to the X-ray beam and with the rows of detector pixels. The centre of rotation is the projection of the axis of rotation on to the detector. The reconstruction assumes this is horizontally centred on the detector. An offset introduces blurring and artefacts in the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifty-transition",
   "metadata": {},
   "source": [
    "There are various ways how centre of rotation offset can be estimated. For parallel geometry case we can use cross-correlation between 0 and 180 degrees. CIL provides a processor which implements this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_centred = CentreOfRotationCorrector.xcorr()(data_crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-naples",
   "metadata": {},
   "source": [
    "Note, that `CentreOfRotationCorrector` doesn't modify the dataset but update corresponding geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('data_crop rotation axis position: {}'.format(data_crop.geometry.config.system.rotation_axis.position))\n",
    "print('data_centred rotation axis position: {}'.format(data_centred.geometry.config.system.rotation_axis.position))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ca433",
   "metadata": {},
   "source": [
    "We use `show_geometry` utility to illustrate `AcquisitionGeometry` before and afetr the correction. Note, after the correction the rotation axis position and the detector positon do not coincide anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_geometry(data_crop.geometry)\n",
    "show_geometry(data_centred.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "combined-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get acquisiiton geometry\n",
    "ag_centred = data_centred.geometry\n",
    "\n",
    "# FBP reconstruction\n",
    "fbp_recon_centred = FBP(ig, ag_centred, device='gpu')(data_centred)\n",
    "\n",
    "# visualize reconstruction results\n",
    "show2D([fbp_recon_centred.subset(vertical=80), \\\n",
    "        fbp_recon_centred.subset(vertical=100)], \\\n",
    "        cmap=cmap, num_cols=1, size=(10,10), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-superintendent",
   "metadata": {},
   "source": [
    "## Slicing and binning data\n",
    "\n",
    "The data contains a redundant projection at 180 degrees, which can be discarded by keeping only the 90 angles. At the same time air on both sides can be cropped off by keeping only horizontal pixels from 20 to 140 out of 160. This is done using the `Slicer` Processor and the trimmed data is printed, showing the horizontal dimension now reduced to 120. Note, `Slicer` supports negative indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_centred = Slicer(roi={'angle':(0,90), 'horizontal':(20,-20,1)})(data_centred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-lindsay",
   "metadata": {},
   "source": [
    "Quite often we want to test methods with a lower number of projections. `Slicer` can be used to skip projections. Here we will use `Slicer` to generate new datasets with a lower number of projections. To speed up reconstrucion, we will work only with a single slice. Note, dimension labels refer to different dimenions therefore we can conviniently use the `subset` method to extract a single slice along the corresponsing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_list = [1,2,3,4,5,6]\n",
    "titles = []\n",
    "results = []\n",
    "\n",
    "# get single slice\n",
    "data_slice = data_centred.subset(vertical=100)\n",
    "# and corresponding geometry\n",
    "ig_slice = data_slice.geometry.get_ImageGeometry()\n",
    "\n",
    "for step in step_list:\n",
    "   \n",
    "    #slice acquisition data\n",
    "    data_sliced = Slicer(roi={'angle': (None, None, step)})(data_slice)\n",
    "\n",
    "    #Perform a fast reconstruction of the slice using FBP\n",
    "    FBP_output = FBP(ig_slice, data_sliced.geometry, device='gpu')(data_sliced)\n",
    "\n",
    "    #save the results\n",
    "    titles.append(\"# projections {}\".format(data_sliced.shape[0]))\n",
    "    results.append(FBP_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results    \n",
    "show2D(results, titles, fix_range=True, cmap=cmap, origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-accuracy",
   "metadata": {},
   "source": [
    "Or we might want to bin data. CIL provides a `Binner` processor as well. THe `Binner` operates very similar to the `Slicer` but instead of skipping elements, it calculates their average. For instance, `Binner(roi={horizontal: (None, None,2)})` will calculate average of every 2 elements along horizontal axis.\n",
    "Note, in the example below we bin `AcquisitionData` and keep `ImageData` with original 120x120 resolution. Therefore, the reconstructed slice will contain the same number of pixles but resolution will degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_list = [1,2,3,4]\n",
    "titles = []\n",
    "results = []\n",
    "\n",
    "for bin in bin_list:\n",
    "   \n",
    "    #slice acquisition data\n",
    "    data_binned = Binner(roi={'horizontal': (None, None, bin)})(data_slice)\n",
    "\n",
    "    #Perform a fast reconstruction of the slice using FBP\n",
    "    FBP_output = FBP(ig_slice, data_binned.geometry, device='gpu')(data_binned)\n",
    "\n",
    "    #save the results\n",
    "    titles.append(\"# pixels {}\".format(data_binned.shape[1]))\n",
    "    results.append(FBP_output)\n",
    "\n",
    "#plot the results    \n",
    "show2D(results, titles, fix_range=True, cmap=cmap, origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb6a28",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook you learned basic processors provided by CIL. Theese processors support basic image manipulations and allow quick design of benchmark studies without manual modification of `AcquisitionGeometry`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-camel",
   "metadata": {},
   "source": [
    "## Advanced: working with bad pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-zambia",
   "metadata": {},
   "source": [
    "Normally we get calibrated detector data with bad pixels removed one way or another. In case we do not or calibration is not sufficient to remove bad pixels, we developed some CIL processors to treat such data. Note, we do not advocate for a solution below as a cure-all remedy as proper detetor calibration is still preferable over image processing type of compensation. We developed this use case for demonstration of CIL tools only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-recording",
   "metadata": {},
   "source": [
    "FIrst we define a small helper function which will add `number_of_colums` corrupted collumns and `number_of_hot_pix` bad pixels to `data_slice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bad_pixels(data, number_of_colums, number_of_hot_pix, seed):\n",
    "\n",
    "    data_corrupted = data.copy()\n",
    "\n",
    "    # get intensity range\n",
    "    low = np.amin(data.as_array())\n",
    "    high = np.amax(data.as_array())\n",
    "\n",
    "    # we seed random number generator for repeaability\n",
    "    rng = np.random.RandomState(seed=seed) \n",
    "    # indices of bad columns\n",
    "    columns = rng.randint(0, data.shape[1], size=number_of_colums)\n",
    "    # indices of hot pixels\n",
    "    pix_row = rng.randint(0, data.shape[0], size=number_of_hot_pix)\n",
    "    pix_col = rng.randint(0, data.shape[1], size=number_of_hot_pix)\n",
    "    # values in hot pixels\n",
    "    pixel_values = rng.uniform(low=low, high=high, size=number_of_hot_pix)\n",
    "\n",
    "    for i in range(number_of_colums):\n",
    "        col_pattern = rng.uniform(low=low, high=high, size=data.shape[0])\n",
    "        data_corrupted.as_array()[:, columns[i]] = data.as_array()[:, columns[i]]+col_pattern\n",
    "\n",
    "    for i in range(number_of_hot_pix):\n",
    "        data_corrupted.as_array()[pix_row[i], pix_col[i]] = pixel_values[i]\n",
    "\n",
    "    return data_corrupted    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f81ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of 'bad' columns\n",
    "number_of_colums = 5\n",
    "# number of randomly located hot pixels\n",
    "number_of_hot_pix = 200\n",
    "# we seed random number generator for repeaability\n",
    "seed = 8392\n",
    "\n",
    "data_corrupted = add_bad_pixels(data_slice, number_of_colums, number_of_hot_pix, seed)\n",
    "\n",
    "show2D([data_slice, data_corrupted], \\\n",
    "        ['clean data', 'corrupted data'], \\\n",
    "        cmap=cmap, num_cols=2, size=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165996fd",
   "metadata": {},
   "source": [
    "As expected, FBP reconstruction of `corrupted_data` will show severe artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbp = FBP(ig_slice, data_slice.geometry, device='gpu')\n",
    "fbp.set_input(data_slice)\n",
    "fbp_recon_clean = fbp.get_output()  \n",
    "\n",
    "fbp.set_input(data_corrupted)\n",
    "fbp_recon_corrupted = fbp.get_output()  \n",
    "\n",
    "show2D([fbp_recon_clean, fbp_recon_corrupted], \\\n",
    "        ['clean data', 'corrupted data'], \\\n",
    "        cmap=cmap, num_cols=2, size=(15,10), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59775ee5",
   "metadata": {},
   "source": [
    "In this case, simple thresholding will not detect all bad pixels. We use `MaskGenerator` with `movmedian` method to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = MaskGenerator.median(threshold_factor=3, window=7)(data_corrupted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba9dd5a",
   "metadata": {},
   "source": [
    "Now `mask` is a binary image which contains 0 where outliers were detected and 1 for other pixels. We use `Masker` to mask out the detected outliers. We use linear interpolation in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_masked = Masker.interpolate(mask=mask, method='linear', axis='horizontal')(data_corrupted)\n",
    "\n",
    "# visualise corrected sinogram\n",
    "show2D([data_slice, data_masked, (data_slice-data_masked).abs()], \\\n",
    "        ['clean data', 'masked data', 'difference'], \\\n",
    "        cmap=cmap, num_cols=1, size=(15,15), origin='upper-left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-sleep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct corrected data\n",
    "fbp.set_input(data_masked)\n",
    "fbp_recon_masked = fbp.get_output()  \n",
    "\n",
    "# visualise reconstructions\n",
    "show2D([fbp_recon_clean, fbp_recon_masked], \\\n",
    "        ['clean data', 'masked data'], \\\n",
    "        cmap=cmap, num_cols=2, size=(15,10), origin='upper-left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bf30dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0efc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
