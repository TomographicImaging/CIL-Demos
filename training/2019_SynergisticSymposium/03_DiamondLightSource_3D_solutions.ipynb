{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Copyright 2019 Science Technology Facilities Council\n",
    "# Copyright 2019 University of Manchester\n",
    "#\n",
    "# This work is part of the Core Imaging Library developed by Science Technology\t\n",
    "# Facilities Council and University of Manchester\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0.txt\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# \n",
    "#========================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing a dataset from the Diamond Light Source facility\n",
    "\n",
    "Diamond Light Source (DLS) is the UK‚Äôs national synchrotron science facility, located at the Harwell Science and Innovation Campus in Oxfordshire.\n",
    "\n",
    "This exercise will walk you through the reconstruction of a parallel beam 3D data set acquired by DLS. In this set-up parallel beams of X-rays are emitted on to a 2D detector array which allows us to reconstruct a 3D volume. This common geometry for data from a synchrotron source is shown below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/parallel3d.png\" width=600 height=600 align=\"centre\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning objectives:**\n",
    "1. You will be able to read in a data set and manipulate it in to the form required for the ASTRA projectors\n",
    "2. Use CIL processors CenterOfRotation() and Resizer() to pre-process the data\n",
    "3. Apply the same reconstruction alorithms to real data that we previously have to simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, all required imports are carried out. As before this includes tools from the ccpi.framework and ccpi.optimisation modules, but now we also use tools from the ccpi.processors and ccpi.io modules.\n",
    "\n",
    "The ASTRA projectors are imported from ccpi.astra.oprators and the ccpi.astra.processors modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from ccpi.framework import ImageData, ImageGeometry\n",
    "from ccpi.framework import AcquisitionGeometry, AcquisitionData\n",
    "from ccpi.framework import BlockDataContainer\n",
    "\n",
    "from ccpi.framework.TestData import data_dir\n",
    "\n",
    "from ccpi.optimisation.algorithms import CGLS\n",
    "from ccpi.optimisation.operators import BlockOperator, Gradient\n",
    "\n",
    "from ccpi.processors import Resizer, CenterOfRotationFinder\n",
    "\n",
    "from ccpi.io import NEXUSDataReader\n",
    "\n",
    "from ccpi.astra.operators import AstraProjectorSimple , AstraProjector3DSimple\n",
    "from ccpi.astra.processors import FBP\n",
    "\n",
    "from utilities import islicer, link_islicer\n",
    "from utilities import plotter2D\n",
    "\n",
    "# All external imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the NEXUS data reader to read in a dataset from the Diamond Light Source. The data reader creates the AquisitionData object for you with the geometry specified in the file.\n",
    "\n",
    "CIL also provides a reader for Nikon datasets `NikonDataReader()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up a reader object pointing to the Nexus data set\n",
    "path = os.path.join(data_dir,'24737_fd_normalised.nxs')\n",
    "myreader = NEXUSDataReader(nexus_file=path)\n",
    "data_raw = myreader.load_data()\n",
    "\n",
    "#Convert the data from intensity to attenuation by taking the negative log\n",
    "data_raw.log(out=data_raw)\n",
    "data_raw *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NEXUSDataReader output is either an ImageData object or an AcquisitonData Object. This is decided by the fields present in the dataset.\n",
    "\n",
    "We have created an AcquisitionData object from the input file. We can see the raw data has 3-axes where 'vertical' and 'horizontal' describe the detector axes and 'angle' giving the rotation of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data_raw))\n",
    "print(data_raw)\n",
    "\n",
    "islicer(data_raw, direction='angle', minmax=(0,3), size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the data ready for ASTRA\n",
    "\n",
    "In order to use the ASTRA projectors we need to manipulate the data in to the form the ASTRA projectors expect.\n",
    "\n",
    "In 3D geometry we need the data in the form `['vertical','angle','horizontal']`, which doesn't match the DLS dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_raw.dimension_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `AcquisitionData.subset()` to return an ordered subset of the AcquisitionData and regenerate the geometry.\n",
    "\n",
    "This can be used to slice along the dataset in any dimension. For example `data.subset(vertical=n)` will create a new dataset containing the horizontal and angle data at vertical=n\n",
    "\n",
    "On the other hand, `AcquisitionData.subset(dimensions=['horizontal','vertical','angle'])` will return the same size array with the axes transposed to match the specified order.\n",
    "\n",
    "<span style=\"color:red;font-size:larger\">**Exercise 1:**</span> Reorder the data axes to prepare the data for the ASTRA operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_raw.subset(dimensions=['vertical','angle','horizontal'])\n",
    "print(data_raw)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ASTRA also requires the of projection angles to be in radians. DLS strores their angular data in degrees so we need to convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the angles to radians\n",
    "if data.geometry.angle_unit == 'degree':\n",
    "    data.geometry.angle_unit = 'radian'\n",
    "    data.geometry.angles = data.geometry.angles * np.pi /180."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the acquisition data\n",
    "islicer(data, direction='angle', minmax=(0,3), size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use processors to pre-proccess the data\n",
    "\n",
    "CIL gives you access to some commonly needed data processors including:\n",
    "- `Normalizer()` normalises AcquisitionData based on the instrument reading with and without incident photons or neutrons\n",
    "- `Resizer()` allows you to crop or bin the data in any dimension\n",
    "- `CenterOfRotationFinder()` finds the center of rotation in a parallel beam dataset (credit: Nghia Vo)\n",
    "\n",
    "The processors are called in the following way:<br>\n",
    ">processor_instance = Processor(set_up_parameters)<br>\n",
    ">processor_instance.set_input(data_in)<br>\n",
    ">data_out = processor_instance.get_output()<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section_resizer\"></a>\n",
    "#### Resizer()\n",
    "`Resizer(roi, binning)`\n",
    "\n",
    "Resizer() is a processor used to crop or bin the data.\n",
    "\n",
    "To crop the data pass the optional region of interest parameter `roi`. This is a list where each element defines the behaviour along one dimension. To crop along an axis pass a tuple containing the start and end coordinates of the crop `roi=[-1,-1,(index0, index1)]` will crop the data between index0 and index1 in dimension 2.\n",
    "\n",
    "To bin the data in any dimension pass an optional paramer `binning`. This is a list with the number of pixels to bin in each dimension `binning = [1, 1, 2]` will bin the data in blocks of 2 in dimension 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<span style=\"color:red;font-size:larger\">**Exercise 2:**</span> Have you noticed the bad pixel in the top left of each angular projection. Use `Resizer()` to remove the first row of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the region of interest here\n",
    "roi_crop = [(1, data.shape[0]),-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise the processsor\n",
    "resizer = Resizer(roi=roi_crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the input data\n",
    "resizer.set_input(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the output data\n",
    "data_reduced = resizer.get_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the acquistion geometry has been generated with the new dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)\n",
    "print(data_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicer1 = islicer(data, direction='angle', minmax=(0,3), size=8)\n",
    "slicer2 = islicer(data_reduced, direction='angle', minmax=(0,3), size=8)\n",
    "link_islicer(slicer1,slicer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centre of Rotation\n",
    "In a well aligned CT system the axis of rotation is perpendicular to the X-ray beam and with the rows of detector pixels.\n",
    "\n",
    "The centre of rotation is the projection of the axis of rotation on to the detector. The reconstruction assumes this is horizontally centred on the detector. An offset introduces blurring and artefacts in the reconstruction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A top-down view of a centre of rotation offset during acquisition:\n",
    "<img src=\"figures/CofR1.png\" width=600 height=600 align=\"centre\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection of the rotation axis on to a 2D detector:\n",
    "<img src=\"figures/CofR2.png\" width=400 height=400 align=\"centre\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to re-proccess the acquisition data to correct it for this offset.\n",
    "\n",
    "The code below reconstucts one slice of the data. By shifting the acquisition data and looking at the reconstructed slice we can get a feel for what a centre of rotation offset looks like.\n",
    "\n",
    "<span style=\"color:red;font-size:larger\">**Exercise 3:**</span> Change the value of centre of rotation offset to find the best reconstruction of this slice. What happens if you change the slice number to 80? How about 20?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create empty lists containers for the output\n",
    "title = []\n",
    "results = []\n",
    "\n",
    "#pick a slice to reconstruct\n",
    "slice_num = 67\n",
    "\n",
    "#create a new dataset that is just single slice of the data\n",
    "data_slice =  data_reduced.subset(vertical=slice_num)\n",
    "\n",
    "# Use the acquisition geometry from subset()\n",
    "ag = data_slice.geometry\n",
    "\n",
    "# Create image geometry\n",
    "ig = ImageGeometry(voxel_num_x=ag.pixel_num_h, voxel_num_y=ag.pixel_num_h )\n",
    "\n",
    "#pick some values of Centre of rotation offset to compare\n",
    "offset_list = [0,6,6.5,7]\n",
    "\n",
    "for shift in offset_list:\n",
    "   \n",
    "    #translate the acquisition data\n",
    "    data_shifted = ag.allocate()  \n",
    "    scipy.ndimage.interpolation.shift(data_slice.as_array(), (0,-shift), output = data_shifted.as_array(), order=1,mode='nearest')\n",
    "    \n",
    "    #Perform a fast reconstruction of the slice using FBP\n",
    "    fbp = FBP(ig, ag, device='gpu')\n",
    "    fbp.set_input(data_shifted)\n",
    "    FBP_output = fbp.get_output()  \n",
    "\n",
    "    #save the results\n",
    "    title.append(\"CoR = %s pixels\" % shift)\n",
    "    results.append(FBP_output)\n",
    "\n",
    "#plot the results    \n",
    "plotter2D(results,title,fix_range=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use CenterOfRotationFinder()\n",
    "\n",
    "We can use the processor `CenterOfRotationFinder()` to locate the centre of rotation in a parallel beam dataset. This processor is based on Nghia Vo's method. https://doi.org/10.1364/OE.22.019078\n",
    "    \n",
    "This processor can be applied to 2D or 3D parallel beam geometries. It will return the centre of rotation of the centre slice in pixels. A different slice can be specified by passing the slice index or 'centre' to `set_slice()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the processsor\n",
    "cor = CenterOfRotationFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input data\n",
    "cor.set_input(data_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output data\n",
    "center_of_rotation = cor.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Centre of rotation at x = \", center_of_rotation)\n",
    "shift = (center_of_rotation - data.shape[2]/2)\n",
    "print(\"Centre of rotation - detector centre = \", shift, \" pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this agree with what you found in Excercise 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can correct the acquisition data for the centre of rotation offset above. We do this using a scipy function that shifts and interpolates the data. You could also crop or pad the data to correct for the offset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#allocate the memory\n",
    "data_centred = data_reduced.geometry.allocate()\n",
    "#use scipy to do a translation and interpolation of each projection image\n",
    "shifted = scipy.ndimage.interpolation.shift(data_reduced.as_array(), (0,0,-shift), order=3,mode='nearest')\n",
    "data_centred.fill(shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view the data set\n",
    "islicer(data_centred, direction='angle', size=8,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-size:larger\">**Exercise 4:**</span> Process the corrected data `data_centred` with  CenterOfRotationFinder() and convince yourself it's now close to the centre of the detector.\n",
    "\n",
    "Remember processors are used as:\n",
    ">processor_instance = Processor(set_up_parameters)<br>\n",
    ">processor_instance.set_input(data_in)<br>\n",
    ">data_out = processor_instance.get_output()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the processsor\n",
    "cor = CenterOfRotationFinder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input data\n",
    "cor.set_input(data_centred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional: set the slice to run over using set_slice()\n",
    "cor.set_slice('centre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output data\n",
    "center_of_rotation = cor.get_output()\n",
    "\n",
    "print(\"Centre of rotation at x = \", center_of_rotation)\n",
    "shift = (center_of_rotation - data_centred.shape[2]/2)\n",
    "print(\"Centre of rotation - detector centre = \", shift, \" pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the 3D reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the 3D Acquistion geometry\n",
    "In the 2D example we used:<br>\n",
    "`ag = AcquisitionGeometry(geom_type='parallel', angles=angles, pixel_num_h=number_pixels_x)`<br>\n",
    "\n",
    "For 3D we need to change the dimension description to pass the number of vertical pixels as `pixel_num_v`<br>\n",
    "\n",
    "However we've been using the acquistion geometry throughout this notebook so we don't need to redefine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Acquisition Geometry\n",
    "ag = data_centred.geometry.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the 3D Image geometry\n",
    "In the 2D example we used:<br>\n",
    "`ig = ImageGeometry(voxel_num_x = num_voxels_xy, voxel_num_y = num_voxels_xy)`\n",
    "\n",
    "For a 3D reconstruction we also need to pass the number of voxels we want in the $z$-direction as `voxel_num_z`. We can also set the voxel size to be equal to the detector pixel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Image Geometry\n",
    "ig = ImageGeometry(voxel_num_x=ag.pixel_num_h,\n",
    "                   voxel_num_y=ag.pixel_num_h, \n",
    "                   voxel_num_z=ag.pixel_num_v,\n",
    "                   voxel_size_x=ag.pixel_size_h,\n",
    "                   voxel_size_y=ag.pixel_size_h,\n",
    "                   voxel_size_z=ag.pixel_size_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up the projector\n",
    "\n",
    "In the 2D example we used the ASTRA projector:<br>\n",
    "`'AstraProjectorSimple(volume_geometry, sinogram_geometry, device)`\n",
    "\n",
    "Now we need to use ASTRA's 3D projector (note this projector is GPU only)<br>\n",
    "`AstraProjector3DSimple(volume_geometry, sinogram_geometry)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = AstraProjector3DSimple(ig, ag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct using Filtered Back Projection\n",
    "\n",
    "Reconstruct the data set using the FBP processor from ASTRA\n",
    "\n",
    "`from ccpi.astra.processors import FBP`\n",
    "\n",
    "We Run this in the same way as the processors introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the processsor\n",
    "fbp = FBP(ig, ag, device='gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the input data\n",
    "fbp.set_input(data_centred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the output data\n",
    "FBP_output = fbp.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the results\n",
    "islicer(FBP_output, direction='vertical', size=10, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct using Tikhonov with gradient regularisation\n",
    "\n",
    "Now we have the acquisition data in good shape it's time to set up the reconstruction. We'll once again work through the Tikhonov example, but as we go we'll modify it to work with the 3D dataset.\n",
    "\n",
    "Recall we are solving:\n",
    "$$\\underset{u}{\\mathrm{argmin}}\\begin{Vmatrix}\\tilde{A} u - \\tilde{b}\\end{Vmatrix}^2_2$$\n",
    "\n",
    "\n",
    "\n",
    "with, $\\tilde{A} = \\binom{A}{\\alpha L}$ and, $\\tilde{b} = \\binom{b}{0}$\n",
    "\n",
    "where,\n",
    "- $u$ is the unknown image to be solved for\n",
    "\n",
    "- $A$ is the projection operator\n",
    "\n",
    "- $\\alpha$ is the regularisation parameter\n",
    "\n",
    "- $L$ is a regularisation operator\n",
    "\n",
    "- $b$ is the acquired data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up the block operator $\\tilde{b}$\n",
    "\n",
    "We'll again use the `Gradient()` operator. Its domain is specified by the image geometry so the code doesn't need changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = Gradient(ig)\n",
    "\n",
    "alpha = 10\n",
    "operator_block = BlockOperator(A, alpha * L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the block data container, $\\tilde{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_data = L.range_geometry().allocate(0)\n",
    "data_block = BlockDataContainer(data_centred,zero_data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CGLS as before, passing the BlockOperator and BlockDataContainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup CGLS with the Block Operator and Block DataContainer\n",
    "x_init = ig.allocate(0)      \n",
    "cgls_tikhonov = CGLS(x_init=x_init, operator=operator_block, data=data_block, update_objective_interval = 10)\n",
    "cgls_tikhonov.max_iteration = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the algorithm\n",
    "cgls_tikhonov.run(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results as a stack of 2D slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CGLS_tikhonov_output = cgls_tikhonov.get_output()\n",
    "\n",
    "islicer(CGLS_tikhonov_output, direction='vertical', size=10, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-size:larger\">**Exercise 5:**</span> Try a range of  ùõº  values ranging from very small to very large, visualise the resulting image and central line profiles, and describe the effect of the regularisation parameter choice. Find the $\\alpha$  that (visually) gives you the best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To adapt the optimiser to run over the 3D data we only had to update the `ImageGeomerty`, the `AcquisitionGeometry` and the ASTRA projector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the FBP and Tikhonov reconstruction of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the outputs\n",
    "clim_range=(0,0.11)\n",
    "slicer1=islicer(CGLS_tikhonov_output, direction=0,minmax=clim_range,title='CGLS', size=10,cmap='viridis')\n",
    "slicer2=islicer(FBP_output, direction=0,minmax=clim_range,title='FBP', size=10, cmap='viridis')\n",
    "\n",
    "link_islicer(slicer1,slicer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a line profile of Tikhonov and FBP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertical_ind = 67\n",
    "horizontal_y_ind = 90\n",
    "\n",
    "plt.plot(CGLS_tikhonov_output.subset(vertical=vertical_ind,horizontal_y=horizontal_y_ind).as_array(),label='CGLS')\n",
    "plt.plot(FBP_output.subset(vertical=vertical_ind,horizontal_y=horizontal_y_ind).as_array(),label='FBP')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from images and line profiles, Tikhonov with Gradient regularisation allows us to reduce the noise in the reconstruction substantially. However, we may pay a price in terms of blurring the edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning objectives:**\n",
    "\n",
    "After having worked through this notebook, we have now seen how to:\n",
    "\n",
    "1. Read in a data set and manipulate it in to the form required for the ASTRA projectors\n",
    "2. Use CIL processors CenterOfRotation() and Resizer() to pre-process the data\n",
    "3. Apply the same reconstruction alorithms to real data that we previously have to simulated data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-cil_19.07] *",
   "language": "python",
   "name": "conda-env-.conda-cil_19.07-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
