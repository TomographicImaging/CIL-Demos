{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Primal Dual Hybrid Gradient Algorithm: Tomography </center></h1>\n",
    "\n",
    " In this demo, we learn how to use the Primal Dual hubrid algorithm introduced by [Champolle & Pock](https://hal.archives-ouvertes.fr/hal-00490826/document). We focus on Tomography Reconstruction under an edge-preserving prior, i.e., the __Total variation regularisation__, see [ROF](https://en.wikipedia.org/wiki/Total_variation_denoising). \n",
    "\n",
    "The minimisation problem for Total variation Tomography reconstuction is \n",
    "\n",
    "<a id='TomoTV'></a>\n",
    "$$ \\underset{u}{\\operatorname{argmin}} \\frac{1}{2} \\| \\mathcal{A} u - g\\|^{2} + \\alpha\\,\\mathrm{TV}(u) +  \\mathbb{I}_{\\{u>0\\}}(u) $$\n",
    "\n",
    "where,\n",
    "1. The total variation is defined as $$\\mathrm{TV}(u) = \\|\\nabla u \\|_{2,1} = \\sum \\sqrt{ (\\partial_{y}u)^{2} + (\\partial_{x}u)^{2} }$$\n",
    "1. g is the Acqusisition data obtained from the detector.\n",
    "1. $\\mathcal{A}$ is the projection operator ( _Radon transform_ ) that maps from an image-space to an acquisition space, i.e., $\\mathcal{A} : X \\rightarrow Y, $ where X is an __ImageGeometry__ and Y is an __AcquisitionGeometry__.\n",
    "1. $\\alpha$: regularising parameter that measures a trade-off between the fidelity and the regulariser terms.\n",
    "1. $\\mathbb{I}_{\\{u>0\\}}(u) : = \n",
    "\\begin{cases}\n",
    "0, & \\mbox{ if } u>0\\\\\n",
    "\\infty , & \\mbox{ otherwise}\n",
    "\\quad\n",
    "\\end{cases}\n",
    "$, $\\quad$ a positivity constraint for the minimiser $u$.\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "1. Brief intro for non-smooth minimisation problem using PDHG algorithm.\n",
    "1. Setup and run PDHG with (__non-smooth__) $L^{1}$ norm regulariser. __(No BlockFramework)__\n",
    "1. Use __BlockFunction__ and setup PDHG within the __Block Framework__.\n",
    "1. Run Total variation reconstruction with different regularising parameters and compared with FBP and SIRT reconstructions.\n",
    "\n",
    "At the end of this demo, we will be able to reproduce all the reconstructions presented in the figure below. One can observe that the __Tikhonov regularisation__ with $L = \\nabla$ was able to remove the noise but could not preserve the edges. However, this can be achieved with the the total variation reconstruction.\n",
    "\n",
    "<img src=\"images/recon_all_tomo.jpeg\"  width=\"1500\"/>\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- AcquisitionData, AcquisitionGeometry, AstraProjectorSimple.\n",
    "- BlockOperator, Gradient.\n",
    "- FBP, SIRT, CGLS, Tikhonov.\n",
    "\n",
    "\n",
    "In order to use the PDHG algorithm for the problem above, we need to express our minimisation problem into the following form:\n",
    "\n",
    "<a id='PDHG_form'></a>\n",
    "$$\\min_{u} \\mathcal{F}(K u) + \\mathcal{G}(u)$$\n",
    "\n",
    "where we assume that:\n",
    "\n",
    "1. $\\mathcal{F}$, $\\mathcal{G}$ are __convex__ functionals\n",
    "    \n",
    "    - $\\mathcal{F}: Y \\rightarrow \\mathbb{R}$ \n",
    "    \n",
    "    - $\\mathcal{G}: X \\rightarrow \\mathbb{R}$\n",
    "    \n",
    "    \n",
    "2. $K$ is a continuous linear operator acting from a space X to another space Y :\n",
    "\n",
    "$$K : X \\rightarrow Y \\quad $$ \n",
    "\n",
    "with operator norm  defined as $$\\| K \\| = \\max\\{ \\|K x\\|_{Y} : \\|x\\|_{X}\\leq 1 \\}.$$  \n",
    "\n",
    "**Note**: The Gradient operator  has  $\\|\\nabla\\| = \\sqrt{8} $ and for the projection operator we use the [Power Method](https://en.wikipedia.org/wiki/Power_iteration) to approximate the greatest eigenvalue of $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from ccpi.framework import ImageData, TestData, ImageGeometry, AcquisitionGeometry, AcquisitionData, BlockDataContainer\n",
    "\n",
    "from ccpi.optimisation.functions import L2NormSquared, ZeroFunction, L1Norm, BlockFunction, MixedL21Norm, IndicatorBox, FunctionOperatorComposition\n",
    "from ccpi.optimisation.operators import Gradient, BlockOperator\n",
    "from ccpi.optimisation.algorithms import PDHG, SIRT, CGLS\n",
    "\n",
    "from ccpi.astra.operators import AstraProjectorSimple, AstraProjector3DSimple\n",
    "from ccpi.astra.processors import FBP\n",
    "\n",
    "import tomophantom\n",
    "from tomophantom import TomoP2D\n",
    "import os, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utilities import islicer, link_islicer, psnr, plotter2D\n",
    "from utilities.show_utilities import show\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PDHG?\n",
    "\n",
    "In the previous demo, we presented the __Tikhonov regularisation__ for tomography reconstruction, i.e.,\n",
    "\n",
    "<a id='Tikhonov'></a>\n",
    "$$ \\underset{u}{\\operatorname{argmin}} \\|\\mathcal{A} u - g\\|^{2}_{2} + \\alpha^{2}\\|L u\\|^{2}_{2} $$\n",
    "\n",
    "where we set $L = \\nabla $ or $L = \\mathbb{I}$. Due to the $\\|\\cdot\\|^{2}_{2}$ terms, one can observe that the above objective function is differentiable. However, this is not always the case. Consider for example an $L^{1}$ norm for the fidelity, i.e., $\\|\\mathcal{A} u - g\\|_{1}$ or an $L^{1}$ norm of the regulariser i.e., $\\|u\\|_{1}$.\n",
    "\n",
    "Using the __Primal-Dual hybrid gradient algorithm__, we can solve minimisation problems where the objective is not differentiable, and only a convexity is required. \n",
    "\n",
    "The algorithm is described in the [Appendix](#Appendix) and for every iteration, we solve two subproblems, i.e., __primal & dual problems__ where the __[proximal operators](#Proximal)__ have a closed form solution or can be solved efficiently using an iterative solver. \n",
    "\n",
    "### L$^{1}$ regularisation\n",
    "\n",
    "Let $L=\\mathbb{I}$ in [Tikhonov regularisation](#Tikhonov) and replace the\n",
    "\n",
    "$$\\alpha^{2}\\|L u\\|^{2}_{2}\\mbox{  with  } \\alpha\\|u\\|_{1}, $$ \n",
    "\n",
    "which results to a non-differentiable objective function. Hence, we have \n",
    "\n",
    "<a id='Lasso'></a>\n",
    "$$ \\underset{u}{\\operatorname{argmin}} \\|\\mathcal{A} u - g\\|^{2}_{2} + \\alpha\\|u\\|_{1}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to setup and run PDHG algorithm for the above minimisation problem. \n",
    "\n",
    "First, we load a phantom, from the [Tomophantom](https://github.com/dkazanc/TomoPhantom) package. We can\n",
    "choose different 2D,3D & 4D phantoms from this [library](https://github.com/dkazanc/TomoPhantom/tree/master/PhantomLibrary/models). For the well-known Shepp-Logan phantom, use **_model = 1_** in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 12 # select a model number from the library\n",
    "N = 256 # set dimension of the phantom\n",
    "path = os.path.dirname(tomophantom.__file__)\n",
    "path_library2D = os.path.join(path, \"Phantom2DLibrary.dat\")\n",
    "\n",
    "phantom = TomoP2D.Model(model, N, path_library2D) \n",
    "\n",
    "# Define image geometry.\n",
    "ig = ImageGeometry(voxel_num_x = N, voxel_num_y = N, \n",
    "                   voxel_size_x = 0.1,\n",
    "                   voxel_size_y = 0.1)\n",
    "im_data = ig.allocate()\n",
    "im_data.fill(phantom)\n",
    "\n",
    "show(im_data, title = 'TomoPhantom', cmap = 'inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='noise'></a>\n",
    "The next step is to create an __AcquisitionGeometry__ and __AcquisitionData__ using a __Projection Operator__ $\\mathcal{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AcquisitionGeometry and AcquisitionData \n",
    "detectors = N\n",
    "angles = np.linspace(0, np.pi, 180, dtype='float32')\n",
    "ag = AcquisitionGeometry('parallel','2D', angles, detectors,\n",
    "                        pixel_size_h = 0.1)\n",
    "\n",
    "# Create projection operator using Astra-Toolbox. Available CPU/CPU\n",
    "A = AstraProjectorSimple(ig, ag, device = 'gpu')\n",
    "\n",
    "# Create an acqusition data (numerically)\n",
    "sino_num = A.direct(im_data)\n",
    "\n",
    "# Show numerical sinogram\n",
    "show(sino_num, title = 'Numerical Sinogram', labels = ['Detectors','Angle'], cmap = 'inferno')\n",
    "\n",
    "# Simulate Gaussian noise for the sinogram\n",
    "gaussian_var = 0.2\n",
    "gaussian_mean = 0 # or np.mean(sino_num.as_array())\n",
    "n1 = np.random.normal(gaussian_mean, gaussian_var, size = ag.shape)\n",
    "                      \n",
    "sino_noisy = ag.allocate()\n",
    "sino_noisy.fill(n1 + sino_num.as_array())\n",
    "show(sino_noisy, title = 'Noisy Sinogram', labels = ['Detectors','Angle'], cmap = 'inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBP and SIRT reconstuctions\n",
    "\n",
    "Let's start with a Filtered Back Projection reconstruction and Simultaneous Iterative Reconstruction Technique.\n",
    "Recall, for FBP we type\n",
    "\n",
    "` fbp = FBP(ig, ag, filter_type = 'ram-lak', device = 'gpu') `\n",
    "\n",
    "` fbp.set_input(sino_noisy)`\n",
    "\n",
    "` fbp_recon = fbp.get_output()` \n",
    "\n",
    "and the available filters from Astra are:\n",
    "\n",
    "`\n",
    "‘ram-lak’, ‘shepp-logan’, ‘cosine’, ‘hamming’, ‘hann’, ‘none’, ‘tukey’, ‘lanczos’, ‘triangular’, ‘gaussian’, ‘barlett-hann’, ‘blackman’, ‘nuttall’, ‘blackman-harris’, ‘blackman-nuttall’, ‘flat-top’, ‘kaiser’, ‘parzen’, ‘projection’, ‘sinogram’, ‘rprojection’, ‘rsinogram’.\n",
    "`\n",
    "\n",
    "For SIRT, we type\n",
    "\n",
    "` x_init = ig.allocate()` \n",
    "\n",
    "` sirt = SIRT(x_init = x_init, operator = A, data=sino_noisy, constraint = IndicatorBox(lower=0),max_iteration = 50)`\n",
    "\n",
    "` sirt.run(verbose=False) `\n",
    "\n",
    "` sirt_recon = sirt.get_output() `\n",
    "\n",
    " __(Reminder)__ : If $\\mathcal{A} u = g$, then:  $$u^{k+1} = \\mathcal{P}_{C}(u^{k} + D \\mathcal{A}^{T} M ( g - \\mathcal{A} u^{k}))$$ where,\n",
    " $$\n",
    "    \\begin{cases}\n",
    "    M = \\frac{\\mathbb{1}}{\\mathcal{A}\\mathbb{1}}, \\quad m_{ii} = \\frac{1}{\\sum_{j} a_{ij}}, \\quad\\mbox{ sum over columns }\\\\\n",
    "    D = \\frac{\\mathbb{1}}{\\mathcal{A}^{T}\\mathbb{1}}, \\quad d_{jj} = \\frac{1}{\\sum_{i} a_{ij}}\\quad\\mbox{ sum over rows }\\\\\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "and $\\mathcal{P}_{C}$ is a projection onto a convex set C. For this demo, we will use a positivity constraint.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run the FBP algorithm\n",
    "\n",
    "fbp = FBP(ig, ag, filter_type = 'hann', device = 'gpu')\n",
    "fbp.set_input(sino_noisy)\n",
    "fbp_recon = fbp.get_output()\n",
    "show(fbp_recon, title = 'FBP reconstruction', cmap = 'inferno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run the SIRT algorithm, with positivity constraint\n",
    "\n",
    "x_init = ig.allocate()  \n",
    "\n",
    "sirt = SIRT(x_init = x_init, operator = A, data=sino_noisy, constraint = IndicatorBox(lower=0),\n",
    "           max_iteration = 50)\n",
    "sirt.run(verbose=False)\n",
    "\n",
    "sirt_recon = sirt.get_output()\n",
    "show(sirt_recon, title = 'SIRT reconstruction', cmap = 'inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to setup and run PDHG\n",
    "\n",
    "In order to setup and run PDHG, we need to define the following:\n",
    "\n",
    "- The operator K.\n",
    "- The function $\\mathcal{F}$ and $\\mathcal{G}$.\n",
    "- Step-sizes $\\sigma$ and $\\tau$ such that $\\sigma\\tau\\|K\\|^{2}<1$, see [Appendix](#Appendix).\n",
    "\n",
    "The setup and run of PDHG:\n",
    "\n",
    "` pdhg = PDHG(f = F, g = G, operator = K, tau = tau, sigma = sigma, max_iterations = maxiter)`\n",
    "\n",
    "` pdhg.run()`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express our [problem](#Lasso) into this [form](#PDHG_form), if we let\n",
    "\n",
    "1. $K = \\mathcal{A} \\quad \\Longleftrightarrow \\quad $ `K = A`  \n",
    "\n",
    "1. $\\mathcal{F}: Y \\rightarrow \\mathbb{R}, \\mbox{ with } \\mathcal{F}(z) := \\frac{1}{2}\\| z - g \\|^{2}, \\quad \\Longleftrightarrow \\quad$ ` F = 0.5 * L2NormSquared(sino_noisy)`\n",
    "\n",
    "1. $\\mathcal{G}: X \\rightarrow \\mathbb{R}, \\mbox{ with } \\mathcal{G}(z) := \\alpha\\|z\\|_{1}, \\quad \\Longleftrightarrow \\quad$ ` G = alpha * L1Norm()`.\n",
    "\n",
    "**Note**: In order to use the PDHG algorithm, we do not need to setup the objective function, but rather the functions $\\mathcal{F}, \\mathcal{G}$ and operator $K$ that form the objective.\n",
    "\n",
    "Hence, we can verify that with the above setting we end up with this [form](#PDHG_form) for $x=u$,  $$\\underset{u}{\\operatorname{argmin}} \\frac{1}{2}\\|\\mathcal{A} u - g\\|^{2}_{2} + \\alpha\\|u\\|_{1} = \n",
    "\\underset{u}{\\operatorname{argmin}} \\mathcal{F}(\\mathcal{A}u) + \\mathcal{G}(u) = \\underset{x}{\\operatorname{argmin}} \\mathcal{F}(Kx) + \\mathcal{G}(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sigma_tau'></a>\n",
    "### Define operator K, functions $\\mathcal{F}$  and $\\mathcal{G}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = A\n",
    "F = 0.5 * L2NormSquared(b=sino_noisy)\n",
    "alpha = 0.5\n",
    "G = alpha * L1Norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Compute operator norm and step-sizes $\\sigma, \\tau$, such that $\\sigma\\tau\\|K\\|^{2}<1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normK = K.norm()\n",
    "sigma = 1/normK\n",
    "tau = 1/normK  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and run PDHG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and run PDHG\n",
    "pdhg = PDHG(f = F, g = G, operator = K, sigma = sigma, tau = tau, \n",
    "            max_iteration = 500,\n",
    "            update_objective_interval = 100)\n",
    "pdhg.run()\n",
    "\n",
    "pdhg_l1_recon = pdhg.get_output()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show reconstruction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "PSNR = psnr(im_data.as_array(), pdhg_l1_recon.as_array())\n",
    "\n",
    "show(pdhg_l1_recon, title = 'Reconstruction: L1 regularisation: PSNR = {0:.2f}'.format(PSNR), cmap = 'inferno')\n",
    "show(im_data, title = 'Ground Truth', cmap = 'inferno')\n",
    "\n",
    "# Plot middle line profile\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams.update({'lines.linewidth': 5})\n",
    "plt.plot(np.linspace(1,N,N),im_data.subset(horizontal_y = 128).as_array(), label = 'Ground Truth')\n",
    "plt.plot(np.linspace(1,N,N),pdhg_l1_recon.subset(horizontal_y = 128).as_array(), label = 'L1 Reconstruction')\n",
    "plt.legend()\n",
    "plt.title('Middle Line Profiles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Regularising parameter\n",
    "\n",
    "Run the code above, using different regularisation parameters:\n",
    "\n",
    "1. $\\alpha = 1e-6$, $\\alpha = 1e6$. What do you observe for these asympotic values of $\\alpha$.\n",
    "1. Add more [noise](#noise), e.g., __gaussian_var = 5__ and try to find a _good_ regularisation parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: PDHG Convergence \n",
    "\n",
    "We say that the PDHG converges if the __Primal-Dual Gap = Primal objective + Dual objective__ $\\rightarrow 0 $,  see  [here](#Appendix) . \n",
    "\n",
    "Moreover, the speed of convergence of the PDHG algorithm depends heavily on the choise of step-sizes $\\sigma$ and $\\tau$. Run the [code](#sigma_tau) above with $\\sigma=1$, $\\tau = \\frac{1}{\\sigma \\|K\\|^2}$ and 500 iterations. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Variation Regularisation\n",
    "\n",
    "Now, we can continue with the setup of the PDHG algorithm using the Total variation regulariser defined [here](#TomoTV). Compared to the [Tikhonov](#Tikhonov) problem, we use $L = \\nabla$ but with a non-smooth term which is the __MixedL21Norm__ define [here](#TomoTV). In addition, we enforce a positivity constraint for our solution using $\\mathbb{I}_{\\{u>0\\}}(u)$.\n",
    "\n",
    "## BlockFunction\n",
    "\n",
    "In order to setup the above [problem](#TomoTV) we need to use the __BlockFramework__ and in particular the concepts of __BlockOperator__, __BlockDataContainer__ and __BlockFunction__. In the previous demos, we have already discussed BlockOperator and BlockDataContainer. \n",
    "\n",
    "BlockFunction behaves similarly to a BlockOperator/BlockDataContainer but instead of _stacking_ operators or DataContainers we _append_ functions as below:\n",
    "\n",
    "$$ F = [f_{1}, f_{2}]$$\n",
    "\n",
    "For example, let \n",
    "\n",
    "$$\\begin{align}\n",
    "f_{1}: Y \\rightarrow \\mathbb{R}, \\quad f_{1}(z_{1}) = \\alpha\\,\\|z_{1}\\|_{2,1}, \\mbox{ ( the TV term ) }\\\\\n",
    "f_{2}: X \\rightarrow \\mathbb{R}, \\quad f_{2}(z_{2}) = \\frac{1}{2}\\|z_{2} - g\\|_{2}^{2}, \\mbox{ ( the data-fitting term ) }\n",
    "\\end{align}$$\n",
    "\n",
    "and consider $z = (z_{1}, z_{2})\\in Y\\times X$, then \n",
    "<a id='BlockFunction'></a>\n",
    "$$F(z) : = F((z_{1},z_{2}) = f_{1}(z_{1}) + f_{2}(z_{2})$$\n",
    "\n",
    "\n",
    "## Why BlockFunction?\n",
    "\n",
    "With the above form, $F(z)$ is a __separable sum__ of decoupled functions. __So, why do we need to write in this form__? It turns out, that the __proximal operator of a BlockFunction__ can be separated, i.e., \n",
    "\n",
    "$$\\mathrm{prox}_{\\tau F}(z) = \n",
    "\\begin{bmatrix}\n",
    "\\mathrm{prox}_{\\tau f_1}(z_1)\\\\\n",
    "\\mathrm{prox}_{\\tau f_2}(z_2)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "\\underset{w_{1}\\in Y}{\\operatorname{argmin}} \\tau f_{1}(w_{1}) + \\frac{1}{2}\\|w_{1} - z_{1}\\|^{2}\\\\\n",
    "\\underset{w_{2}\\in X}{\\operatorname{argmin}} \\tau f_{2}(w_{2}) + \\frac{1}{2}\\|w_{2} - z_{2}\\|^{2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Hence, in order to solve the __dual problem__ of the [PDHG](#Appendix) algorithm, we need to solve the two decoupled problems as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDHG for TV regularisation\n",
    "\n",
    "At this stage, we have all the necessary ingredients in order to setup our PDHG algorithm for Total variation tomography reconstruction. We define K as a BlockOperator, with the Gradient and Projection operator:\n",
    "\n",
    "$$ K = \n",
    "\\begin{bmatrix}\n",
    "\\nabla\\\\\n",
    "A\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "` K = BlockOperator(Grad, A)`\n",
    "\n",
    "The function $\\mathcal{F}$, is a [BlockFunction](#BlockFunction) with \n",
    "\n",
    "1. a function $\\alpha\\|\\cdot\\|_{2,1}\\quad\\Longleftrightarrow\\quad$ ` alpha * MixedL21Norm() ` term that represents the Total variation regularisation ,\n",
    "\n",
    "1. a function $\\frac{1}{2}\\|\\cdot -g \\|_{2}^{2}\\quad\\Longleftrightarrow\\quad$ `0.5 * L2NormSquared(sino_noisy)` term that represents the data fitting.\n",
    "\n",
    "Hence, $F = [f1, f2] \\quad\\Longleftrightarrow\\quad $ ` F = BlockFunction( MixedL21Norm(), L2NormSquared(sino_noisy))`\n",
    "\n",
    "Finally, we have the function $\\mathcal{G} = \\mathbb{I}_{\\{u>0\\}}(u) \\quad\\Longleftrightarrow\\quad$ ` G = IndicatorBox(lower=0)`\n",
    "\n",
    "Again, we can verify that with the above setting we can express our problem into this [form](#PDHG_form), for $x=u$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{u}{\\operatorname{argmin}}\\alpha\\|\\nabla u\\|_{2,1} + \\frac{1}{2}\\|\\mathcal{A} u - g\\|^{2}_{2} + \\mathbb{I}_{\\{u>0\\}}(u) =  \\underset{u}{\\operatorname{argmin}} f_{1}(\\nabla u) + f_{2}(\\mathcal{A}u) + \\mathbb{I}_{\\{u>0\\}}(u) \\\\ = \\underset{u}{\\operatorname{argmin}} F(\n",
    "\\begin{bmatrix}\n",
    "\\nabla \\\\\n",
    "\\mathcal{A}\n",
    "\\end{bmatrix}u) + \\mathbb{I}_{\\{u>0\\}}(u) = \n",
    "\\underset{u}{\\operatorname{argmin}} \\mathcal{F}(Ku) + \\mathcal{G}(u) = \\underset{x}{\\operatorname{argmin}} \\mathcal{F}(Kx) + \\mathcal{G}(x) \n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Setup and run PDHG algorithm with the Total variation regulariser\n",
    "\n",
    "Follow the steps below:\n",
    "\n",
    "1. Define Gradient operator and BlockOperator K. The ImageGeometry, AcquisitionGeometry and Projection Operator and are already defined.\n",
    "1. Define BlockFunction F ( do not forget the regularisation parameter ).\n",
    "1. Define Function G.\n",
    "1. Computer operator norm of K, sigma and tau.\n",
    "1. Setup and run PDHG.\n",
    "1. Show reconstruction result.\n",
    "\n",
    "The reconstruction will look like the figure below after running PDHG for 1000 iterations, with $\\sigma=1$, $\\tau = \\frac{1}{\\sigma\\|K\\|^{2}}$ and $\\alpha=2$.\n",
    "\n",
    "<img src=\"images/tv_recon_1000iter_alpha_2.png\"  width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradient operator and BlockOperator K\n",
    "\n",
    "### START CODE HERE (2 lines) ### \n",
    "    \n",
    "    # Grad = Gradient(...)\n",
    "    # K = BlockOperator(... , ...  )\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Define BlockFunction F using the MixedL21Norm() and the L2NormSquared() \n",
    "\n",
    "### START CODE HERE (4 lines) ### \n",
    "    \n",
    "    # alpha = 2\n",
    "    # f1 = MixedL21Norm(...)\n",
    "    # f2 = L2NormSquared(...)\n",
    "    # F = BlockFunction(...  , ... )\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Define BlockFunction G, as a positivity constraint\n",
    "\n",
    "### START CODE HERE (1 lines) ### \n",
    "    \n",
    "    # G = IndicatorBox(...)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1 (3 lines)  \n",
    "\n",
    "### START CODE HERE (3 lines) ### \n",
    "\n",
    "    # normK = ...\n",
    "    # sigma = ...\n",
    "    # tau = ...\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Setup and run PDHG\n",
    "\n",
    "### START CODE HERE (1 lines) ### \n",
    "\n",
    "    # pdhg = ..\n",
    "    # pdhg.run()    \n",
    "    # pdhg_tv_recon = pdhg.get_output()\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "# Show reconstruction, compared middle line profiles\n",
    "\n",
    "# show(pdhg_tv_recon, title = 'TV reconstruction: PSNR = {0:.2f}'.format(PSNR), cmap = 'inferno')\n",
    "# show(im_data, title = 'Ground Truth', cmap = 'inferno')\n",
    "\n",
    "# # Plot middle line profiles\n",
    "# plt.figure(figsize=(30,15))\n",
    "# plt.rcParams.update({'font.size': 22})\n",
    "# plt.rcParams.update({'lines.linewidth': 5})\n",
    "# plt.plot(np.linspace(1,N,N),im_data.subset(horizontal_y = 128).as_array(), label = 'Ground Truth')\n",
    "# plt.plot(np.linspace(1,N,N),pdhg_tv_recon.subset(horizontal_y = 128).as_array(), label = 'TV Reconstruction')\n",
    "# # plt.plot(np.linspace(1,N,N),pdhg_l1_recon.subset(horizontal_y = 128).as_array(), label = 'L1 Reconstruction')\n",
    "# plt.legend()\n",
    "# plt.title('Middle Line Profiles')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradient Operator and BlockOperator \n",
    "Grad = Gradient(ig)\n",
    "K = BlockOperator(Grad,A)\n",
    "\n",
    "# Define BlockFunction F using the MixedL21Norm() and the L2NormSquared()\n",
    "alpha = 2\n",
    "f1 =  alpha * MixedL21Norm()\n",
    "f2 = 0.5 * L2NormSquared(b=sino_noisy)\n",
    "F = BlockFunction(f1,f2)\n",
    "\n",
    "# Define BlockFunction G, as a positivity constraint\n",
    "G = IndicatorBox(lower=0)\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1\n",
    "normK =  K.norm()\n",
    "sigma = 1\n",
    "tau = 1/(sigma*normK**2)\n",
    "\n",
    "pdhg = PDHG(f = F, g = G, operator = K, tau = tau, sigma = sigma, \n",
    "            max_iteration = 1000, update_objective_interval = 500)\n",
    "pdhg.run(verbose = True)\n",
    "\n",
    "pdhg_tv_recon = pdhg.get_output()\n",
    "\n",
    "# Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "PSNR = psnr(im_data.as_array(), pdhg_tv_recon.as_array(), data_range=np.max(im_data.as_array()))\n",
    "\n",
    "show(pdhg_tv_recon, title = 'TV reconstruction: PSNR = {0:.2f}'.format(PSNR), cmap = 'inferno')\n",
    "show(im_data, title = 'Ground Truth', cmap = 'inferno')\n",
    "\n",
    "# Plot middle line profiles\n",
    "plt.figure(figsize=(30,15))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams.update({'lines.linewidth': 5})\n",
    "plt.plot(np.linspace(1,N,N),im_data.subset(horizontal_y = 128).as_array(), label = 'Ground Truth')\n",
    "plt.plot(np.linspace(1,N,N),pdhg_tv_recon.subset(horizontal_y = 128).as_array(), label = 'TV Reconstruction')\n",
    "# plt.plot(np.linspace(1,N,N),pdhg_l1_recon.subset(horizontal_y = 128).as_array(), label = 'L1 Reconstruction')\n",
    "plt.legend()\n",
    "plt.title('Middle Line Profiles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Find the best regulariser for TV reconstruction and compare with FBP and SIRT algorithms\n",
    "\n",
    "For this exercise, we ask the user to\n",
    "\n",
    "1. Run TV reconstruction with 5 different regularising parameters, e.g., $\\alpha = [0.05, 0.5, 2, 5, 10]$.\n",
    "1. Choose the best reconstruction according to the highest PSNR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of possible regularisation parameters\n",
    "\n",
    "### START CODE HERE (1 line) ### \n",
    "    \n",
    "    # alpha = [ , , , , , ]\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1\n",
    "\n",
    "### START CODE HERE (3 lines) ### \n",
    "    \n",
    "    # normK = ...\n",
    "    # sigma = ...\n",
    "    # tau = ...\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Allocate space for reconstruction results\n",
    "    # recon_all_geometry = ImageGeometry(channels=len(alpha), voxel_num_x = N, voxel_num_y = N)\n",
    "    # recon_all = recon_all_geometry.allocate()\n",
    "    # PSNR = np.zeros(len(alpha))\n",
    "    \n",
    "# Define function G, and data-fitting term that is an function-element of the BlockFunction\n",
    "\n",
    "### START CODE HERE (2 lines) ### \n",
    "    \n",
    "    # G = ...\n",
    "    # f2 = ...\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Run PDHG for different regularising parameters\n",
    "\n",
    "### START CODE HERE (3 lines) ### \n",
    "\n",
    "    #for i in range(len(alpha)):\n",
    "\n",
    "    #    f1 = ...\n",
    "    #    F = BlockFunction(..., ...)\n",
    "\n",
    "    #    pdhg = PDHG(f = ..., g = ..., operator = ..., tau = ..., sigma = ..., \n",
    "    #            max_iteration = 1000, update_objective_interval = 500)\n",
    "    #    pdhg.run(verbose = True)\n",
    "\n",
    "    #    recon_all[i] = pdhg.get_output().as_array()\n",
    "\n",
    "        # Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "    #    PSNR[i] =  compare_psnr(im_data.as_array(), pdhg.get_output().as_array(), data_range = 1 )\n",
    "\n",
    "    #    show(pdhg.get_output(), title = 'TV reconstruction: alpha = {0:.2f}'.format(alpha[i]))\n",
    "    #    plt.pause(0.5)\n",
    "    \n",
    "### END CODE HERE ###\n",
    "\n",
    "# Print Parameters and PSNR values\n",
    "\n",
    "    # print('The parameters are {}  '.format(alpha))\n",
    "    # print('The PSNR values {}  '.format(PSNR))\n",
    "\n",
    "# Use islicer to scroll over the different reconstructions\n",
    "    \n",
    "# from utilities import islicer\n",
    "# stack_recon = ImageData(np.array([fbp_recon.as_array(), \n",
    "#                                   sirt_recon.as_array(),\n",
    "#                                   pdhg_l1_recon.as_array(),\n",
    "#                                   recon_all.subset(channel=0).as_array(),\n",
    "#                                   recon_all.subset(channel=1).as_array(),\n",
    "#                                   recon_all.subset(channel=2).as_array(),\n",
    "#                                   recon_all.subset(channel=3).as_array(),\n",
    "#                                   recon_all.subset(channel=4).as_array(),\n",
    "#                                   im_data.as_array()]))\n",
    "# # from ccpi.framework import BlockDataContainer\n",
    "# # stack_recon = BlockDataContainer(fbp_recon, sirt_recon)\n",
    "# islicer(stack_recon, 0, title = ['FBP','SIRT','L1 reconstruction',\n",
    "#                                  'TV a = {}'.format(alpha[0]),\n",
    "#                                  'TV a = {}'.format(alpha[1]),\n",
    "#                                  'TV a = {}'.format(alpha[2]),\n",
    "#                                  'TV a = {}'.format(alpha[3]),\n",
    "#                                  'TV a = {}'.format(alpha[4]),                                 \n",
    "#                                  'Ground Truth'], \n",
    "#         minmax=(0,1), size=(10,10), slice_number = 0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a list of regularising parameters\n",
    "\n",
    "alpha = [0.05, 0.5, 2, 5, 10]\n",
    "\n",
    "# Compute operator norm and choose step-size sigma and tau such that sigma*tau||K||^{2}<1\n",
    "K = BlockOperator(Grad, A)\n",
    "\n",
    "normK = K.norm()\n",
    "sigma = 1\n",
    "tau = 1/(sigma*normK**2)\n",
    "\n",
    "# sigma = 1/normK\n",
    "# tau = 1/normK \n",
    "\n",
    "# Allocate space for reconstruction results\n",
    "recon_all_geometry = ImageGeometry(channels=len(alpha), voxel_num_x = N, voxel_num_y = N)\n",
    "recon_all = recon_all_geometry.allocate()\n",
    "PSNR = np.zeros(len(alpha))\n",
    "\n",
    "# Define BlockFunction G, as a positivity constraint\n",
    "G = IndicatorBox(lower=0)\n",
    "f2 = 0.5 * L2NormSquared(b=sino_noisy)\n",
    "\n",
    "for i in range(len(alpha)):\n",
    "      \n",
    "    f1 =  alpha[i]* MixedL21Norm()\n",
    "    F = BlockFunction(f1, f2)\n",
    "\n",
    "    pdhg = PDHG(f = F, g = G, operator = K, tau = tau, sigma = sigma, \n",
    "            max_iteration = 1000, update_objective_interval = 500)\n",
    "    pdhg.run(verbose = True)\n",
    "\n",
    "    np.copyto(recon_all.as_array()[i],pdhg.get_output().as_array())\n",
    "\n",
    "    # Compute PSNR quality measure, and show reconstuction and ground truth\n",
    "    PSNR[i] =  psnr(im_data.as_array(), pdhg.get_output().as_array(), data_range=1)\n",
    "    \n",
    "    show(pdhg.get_output(), title = 'TV reconstruction: alpha = {0:.2f}'.format(alpha[i]), cmap = 'inferno')\n",
    "    plt.pause(0.5)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The parameters are {}  '.format(alpha))\n",
    "print('The PSNR values {}  '.format(PSNR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import islicer\n",
    "stack_recon = ImageData(np.array([fbp_recon.as_array(), \n",
    "                                  sirt_recon.as_array(),\n",
    "                                  pdhg_l1_recon.as_array(),\n",
    "                                  recon_all.subset(channel=0).as_array(),\n",
    "                                  recon_all.subset(channel=1).as_array(),\n",
    "                                  recon_all.subset(channel=2).as_array(),\n",
    "                                  recon_all.subset(channel=3).as_array(),\n",
    "                                  recon_all.subset(channel=4).as_array(),\n",
    "                                  im_data.as_array()]))\n",
    "# from ccpi.framework import BlockDataContainer\n",
    "# stack_recon = BlockDataContainer(fbp_recon, sirt_recon)\n",
    "islicer(stack_recon, 0, title = ['FBP','SIRT','L1 reconstruction',\n",
    "                                 'TV a = {}'.format(alpha[0]),\n",
    "                                 'TV a = {}'.format(alpha[1]),\n",
    "                                 'TV a = {}'.format(alpha[2]),\n",
    "                                 'TV a = {}'.format(alpha[3]),\n",
    "                                 'TV a = {}'.format(alpha[4]),                                 \n",
    "                                 'Ground Truth'], \n",
    "        minmax=(0,1), size=(10,10), slice_number = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "plt.rcParams.update({'lines.linewidth': 5})\n",
    "plt.plot(np.linspace(1,N,N),fbp_recon.subset(horizontal_y = 128).as_array(), label = 'FBP', color = 'brown')\n",
    "plt.plot(np.linspace(1,N,N),fbp_recon.subset(horizontal_y = 128).as_array(), label = 'SIRT', color = 'orange')\n",
    "plt.plot(np.linspace(1,N,N),pdhg_l1_recon.subset(horizontal_y = 128).as_array(), label = 'L1 Reconstruction', color = 'blue')\n",
    "plt.plot(np.linspace(1,N,N),recon_all.subset(channel=1, horizontal_y=128).as_array(), label = 'TV Reconstruction', color = 'red')\n",
    "plt.plot(np.linspace(1,N,N),im_data.subset(horizontal_y = 128).as_array(), label = 'Ground Truth', color = 'black')\n",
    "plt.legend()\n",
    "plt.title('Middle Line Profiles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Summary\n",
    "\n",
    "1. Brief intro for non-smooth minimisation problem using PDHG algorithm.\n",
    "1. Setup and run PDHG with (__non-smooth__) $L^{1}$ norm regulariser. __(No BlockFramework)__\n",
    "1. Use __BlockFunction__ and setup PDHG within the __Block Framework__.\n",
    "1. Run Total variation reconstruction with different regularising parameters and compared with FBP and SIRT reconstructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix'></a>\n",
    "# Appendix\n",
    "\n",
    "The PDHG algorithm is described below :\n",
    "    \n",
    "<center> Choose step-sizes $\\tau$, $\\sigma$, such that $\\tau\\sigma\\|K\\|^{2}<1$ </center> \n",
    "<center> Choose $\\theta\\in[0,1]$ </center>\n",
    "<center> Let $(x^{0}, y^{0})\\in X\\times Y$ and $\\overline{x}^{0} = x^{0}$ </center>\n",
    "<br>\n",
    "\\begin{align}\n",
    "    y^{n+1}  & = \\mathrm{prox}_{\\sigma\\mathcal{F}^{*}}( y^{n} + \\sigma K \\overline{x}^{n} )\\quad \\mbox{ (Dual Problem)}\\tag{1}\\\\[10pt]\n",
    "    x^{n+1}  & = \\mathrm{prox}_{\\tau\\mathcal{G}} ( x^{n} - \\tau K^{*} \\overline{x}^{n} )\\quad \\mbox{ (Primal Problem) }\\tag{2}\\\\[10pt]\n",
    "    \\overline{x}^{n+1} & = x^{n+1} + \\theta ( x^{n+1} - x^{n} )\\quad\\mbox{ (Over-relaxation step) }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Proximal'></a>\n",
    "__**Proximal Operator**__: Let $\\lambda>0$ and $f$ a convex function. Then\n",
    "\n",
    "$$\n",
    "z : = \\mbox{prox}_{\\lambda f}(x) = \\underset{z}{\\operatorname{argmin}} \\frac{1}{2}\\|z - x \\|^{2} + \\lambda f(z) \n",
    "$$\n",
    " \n",
    "__**Note**__: We assume that proximal operators have a closed form solution or can be solved efficiently using an iterative algorithm.\n",
    "\n",
    "__**Primal-Dual Gap (PD)**__:\n",
    "\n",
    "$$\\mbox{PD} = \\mathcal{F}(Kx) + \\mathcal{G}(x) + \\mathcal{F}^{*}(y) + \\mathcal{G}(-K^{T}y) $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
