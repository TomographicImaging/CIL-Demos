{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========================================================================\n",
    "# Copyright 2019 Science Technology Facilities Council\n",
    "# Copyright 2019 University of Manchester\n",
    "#\n",
    "# This work is part of the Core Imaging Library developed by Science Technology\t\n",
    "# Facilities Council and University of Manchester\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0.txt\n",
    "# \n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# \n",
    "#========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please, ignore this cell for now\n",
    "# here we import and set-up some utilities for this notebook\n",
    "\n",
    "# some imports\n",
    "# to fix compatibility issues \n",
    "# between different versions of Python\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# import utilities\n",
    "# to minimise amount of code one needs to write to plot data,\n",
    "# we implemented some plotting utilities\n",
    "from utilities import plotter2D\n",
    "\n",
    "# a small function to generate a sinogram\n",
    "# imports\n",
    "from ccpi.framework import ImageData, ImageGeometry\n",
    "from ccpi.framework import AcquisitionData, AcquisitionGeometry\n",
    "from ccpi.astra.operators import AstraProjectorSimple \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# simulate 'ideal', i.e. noise-free, sino\n",
    "def get_ideal_sino(data, N, n_angles):\n",
    "    \n",
    "    # get ImageGeometry\n",
    "    ig = data.geometry\n",
    "\n",
    "    # Create AcquisitionGeometry\n",
    "    angles = np.linspace(0, np.pi, n_angles, dtype=np.float32)\n",
    "\n",
    "    ag = AcquisitionGeometry(geom_type=\"parallel\",\n",
    "                             dimension=\"2D\", \n",
    "                             angles=angles,\n",
    "                             pixel_num_h=N)\n",
    "\n",
    "    dev = \"cpu\"\n",
    "\n",
    "    Aop = AstraProjectorSimple(ig, ag, dev)    \n",
    "    sino = Aop.direct(data)\n",
    "    return sino.as_array()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework basics\n",
    "\n",
    "The goal of this notebook is to cover the Framework building blocks. We do not expect participants to step through all the cells. We suggest to use this notebook as a reference for other exercises or for independent trials in future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT geometry\n",
    "\n",
    "In conventional CT systems, an object is placed between a source emitting X-rays and a detector array measuring the  X-ray transmission images of the incident X-rays. Typically, either the object is placed on a rotating sample stage and rotates with respect to the source-detector assembly, or the source-detector gantry rotates with respect to the stationary object. This arrangement results in so-called *circular scanning trajectory*. Depending on source and detector types, there are three conventional data acquisition geometries:\n",
    " - parallel geometry (2D or 3D),\n",
    " - fan-beam geometry, and\n",
    " - cone-beam geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Parallel geometry*\n",
    "\n",
    "Parallel beams of X-rays are emitted onto 1D (single pixel row) or 2D detector array. This geometry is common for synchrotron sources. 2D parrallel geometry is illustrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/parallel.png\" width=500 height=500 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D parallel geometry is a stack of 2D slices each having 2D parallel geometry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/parallel3d.png\" width=600 height=600 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fan-beam geometry*\n",
    "\n",
    "A single point-like X-ray source emits a cone beam onto 1D detector pixel row. Cone-beam is typically collimated to imaging field of view. Collimation allows greatly reduce amount of scatter radiation reaching the detector. Fan-beam geometry is used when scattering has significant influence on image quality or single-slice reconstruction is sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fan.png\" width=500 height=500 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Cone-beam geometry*\n",
    "\n",
    "A single point-like X-ray source emits a cone beam onto 2D detector array. Cone-beam geometry is mainly used in lab-based CT instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cone.png\" width=600 height=600 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel geometry\n",
    "#### AcquisitionGeometry and AcquisitionData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Framework, we implemented `AcquisitionGeometry` class to hold acquisition parameters and `ImageGeometry` to hold geometry of a reconstructed volume. Corresponding data arrays are wrapped as `AcquisitionData` and `ImageData` classes, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest (of course from image processing point of view, not from physical implementation) geometry is the parallel geometry. Geometrical parameters for parallel geometry are depicted below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2d_parallel_geometry'></a>\n",
    "<img src=\"figures/parallel_geometry.png\" width=600 height=600 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Framework, we define `AcquisitionGeometry` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ccpi.framework import AcquisitionGeometry\n",
    "import numpy as np\n",
    "\n",
    "# acquisition angles\n",
    "n_angles = 90\n",
    "angles = np.linspace(0, np.pi, n_angles, dtype=np.float32)\n",
    "\n",
    "# number of pixels in detector row\n",
    "N = 256\n",
    "\n",
    "# pixel size\n",
    "pixel_size_h = 1\n",
    "\n",
    "# # create AcquisitionGeometry\n",
    "ag_par = AcquisitionGeometry(geom_type='parallel',\n",
    "                             dimension='2D',\n",
    "                             angles=angles,\n",
    "                             pixel_num_h=N,\n",
    "                             pixel_size_h=pixel_size_h)\n",
    "\n",
    "print('Acquisition geometry:\\n{}'.format(ag_par))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AcquisitionGeometry` contains only metadata, the actual data is wrapped in `AcquisitionData` class.`AcquisiitonGeometry` class also holds information about arrangement of the actual acquisition data array. We use attribute `dimension_labels` to label axis. The expected dimension labels are shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2d_parallel_labels'></a>\n",
    "<img src=\"figures/parallel_data.png\" width=300 height=300 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default order of dimensions for `AcquisitionData` is `[angle, horizontal]`, meaning that the number of elements along 0 and 1 axes in the acquisition data array is expected to be `n_angles` and `N`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimension labels:\\n{}'.format(ag_par.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='acquisition_data'></a>\n",
    "To have consistent `AcquisitionData` and `AcquisitionGeometry`, we recommend to allocate `AcquisitionData` using `allocate` method of `AcquisitionGeometry` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ccpi.framework import AcquisitionData\n",
    "\n",
    "# allocate AcquisitionData\n",
    "ad_par = ag_par.allocate()\n",
    "\n",
    "print('Dimensions and Labels = {}, {}'.format(ad_par.shape, ad_par.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pass actual sinogram to `AcquisitionData`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ccpi.framework import TestData\n",
    "import os, sys\n",
    "\n",
    "# load test image\n",
    "# initialise loader\n",
    "loader = TestData(data_dir=os.path.join(sys.prefix, \"share\", \"ccpi\"))\n",
    "# load data\n",
    "data = loader.load(TestData.SIMPLE_PHANTOM_2D, size = (N, N))\n",
    "# scale data\n",
    "data *= 2.5 / N\n",
    "ad_par.fill(get_ideal_sino(data, N, n_angles))\n",
    "\n",
    "# show sinogram\n",
    "plotter2D([data, ad_par],\n",
    "          [\"Image\", \"Sinogram\"],\n",
    "          fix_range=False,\n",
    "          stretch_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ImageGeometry and ImageData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store reconstruction results, we implemented two classes:`ImageGeometry` and `ImageData` class. Similar to `AcquisitionData` and `AcquisitionGeometry`, we first define 2D [`ImageGeometry`](#2d_parallel_geometry) and then allocate [`ImageData`](#2d_parallel_labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ccpi.framework import ImageData, ImageGeometry\n",
    "\n",
    "# define 2D ImageGeometry\n",
    "# given AcquisitionGeometry ag_par, default parameters for corresponding ImageData\n",
    "ig_par = ImageGeometry(voxel_num_y=ag_par.pixel_num_h, \n",
    "                       voxel_size_x=ag_par.pixel_size_h,\n",
    "                       voxel_num_x=ag_par.pixel_num_h,\n",
    "                       voxel_size_y=ag_par.pixel_size_h)\n",
    "\n",
    "# allocate ImageData filled with 0 values with the specific geometry\n",
    "im_data1 = ig_par.allocate()\n",
    "# allocate ImageData filled with random values with the specific geometry\n",
    "im_data2 = ig_par.allocate('random', seed=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Allocate with zeros \\n{}\\n'.format(im_data1.as_array()))\n",
    "print('Allocate with random numbers in [0,1] \\n{}\\n'.format(im_data2.as_array()))\n",
    "print('Dimensions and Labels = {} , {}'.format(im_data1.shape, im_data1.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default parameters are recommended to fully exploit detector resolution but they can be chosen based on other considerations. For instance, to reconstruct on coarser grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_par1 = ImageGeometry(voxel_num_y=ag_par.pixel_num_h // 2, \n",
    "                        voxel_size_x=ag_par.pixel_size_h * 2,\n",
    "                        voxel_num_x=ag_par.pixel_num_h // 2,\n",
    "                        voxel_size_y=ag_par.pixel_size_h * 2)\n",
    "\n",
    "im_data3 = ig_par1.allocate('random', seed=5)\n",
    "\n",
    "print('Dimensions and Labels = {} , {}'.format(im_data3.shape, im_data3.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D parallel, fan-beam and cone-beam geometries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fan-beam, cone-beam and 3D (multi-slice) parallel geometry can be set-up similar to 2D parallel geometry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3D parallel geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrical parameters and dimension labels for 3D parallel beam geometry:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/parallel3d_geometry.png\" width=700 height=700 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/parallel3d_data.png\" width=600 height=600 align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up 3D parallel beam AcquisitionGeometry\n",
    "# physical pixel size\n",
    "pixel_size_h = 1\n",
    "ag_par_3d = AcquisitionGeometry(geom_type='parallel', \n",
    "                                dimension='3D', \n",
    "                                angles=angles, \n",
    "                                pixel_num_h=N, \n",
    "                                pixel_size_h=pixel_size_h, \n",
    "                                pixel_num_v=N, \n",
    "                                pixel_size_v=pixel_size_h)\n",
    "\n",
    "print('Fan-beam acquisition geometry:\\n{}'.format(ag_par_3d))\n",
    "print('Dimension labels:\\n{}'.format(ag_par_3d.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `ag_par_3d` acquisition geometry, default `ImageGeometry` parameters can be set up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up 3D parallel beam ImageGeometry\n",
    "ig_par_3d = ImageGeometry(voxel_num_x=ag_par_3d.pixel_num_h,\n",
    "                          voxel_size_x=ag_par_3d.pixel_size_h,\n",
    "                          voxel_num_y=ag_par_3d.pixel_num_h,\n",
    "                          voxel_size_y=ag_par_3d.pixel_size_h,\n",
    "                          voxel_num_z=ag_par_3d.pixel_num_v,\n",
    "                          voxel_size_z=ag_par_3d.pixel_size_v)\n",
    "\n",
    "print('Fan-beam image geometry:\\n{}'.format(ig_par_3d))\n",
    "print('Dimension labels:\\n{}'.format(ig_par_3d.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fan_beam_geometry'></a>\n",
    "#### Fan-beam geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrical parameters and dimension labels for fan-beam geometry:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fan_geometry.png\" width=700 height=700 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/fan_data.png\" width=450 height=450 align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up fan-beam AcquisitionGeometry\n",
    "# distance from source to center of rotation\n",
    "dist_source_center = 200.0\n",
    "# distance from center of rotation to detector\n",
    "dist_center_detector = 300.0\n",
    "# physical pixel size\n",
    "pixel_size_h = 2\n",
    "ag_fan = AcquisitionGeometry(geom_type='cone', \n",
    "                             dimension='2D', \n",
    "                             angles=angles, \n",
    "                             pixel_num_h=N, \n",
    "                             pixel_size_h=pixel_size_h, \n",
    "                             dist_source_center=dist_source_center, \n",
    "                             dist_center_detector=dist_center_detector)\n",
    "\n",
    "print('Fan-beam acquisition geometry:\\n{}'.format(ag_fan))\n",
    "print('Dimension labels:\\n{}'.format(ag_fan.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given `ag_fan` acquisition geometry, default `ImageGeometry` parameters can be set up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up fan-beam ImageGeometry\n",
    "\n",
    "# calculate geometrical magnification\n",
    "mag = (ag_fan.dist_source_center + ag_fan.dist_center_detector) / ag_fan.dist_source_center\n",
    "\n",
    "ig_fan = ImageGeometry(voxel_num_x=ag_fan.pixel_num_h,\n",
    "                       voxel_size_x=ag_fan.pixel_size_h / mag,\n",
    "                       voxel_num_y=ag_fan.pixel_num_h,\n",
    "                       voxel_size_y=ag_fan.pixel_size_h / mag)\n",
    "\n",
    "print('Fan-beam image geometry:\\n{}'.format(ig_fan))\n",
    "print('Dimension labels:\\n{}'.format(ig_fan.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cone_beam_geometry'></a>\n",
    "#### Cone-beam geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrical parameters and dimension labels for cone-beam geometry:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cone_data.png\" width=650 height=650 align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cone_geometry.png\" width=800 height=800 align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up cone-beam geometry\n",
    "# distance from source to center of rotation\n",
    "dist_source_center = 200.0\n",
    "# distance from center of rotation to detector\n",
    "dist_center_detector = 300.0\n",
    "# physical pixel size\n",
    "pixel_size_h = 2\n",
    "\n",
    "ag_cone = AcquisitionGeometry(geom_type='cone', \n",
    "                             dimension='3D', \n",
    "                             angles=angles, \n",
    "                             pixel_num_h=N, \n",
    "                             pixel_size_h=pixel_size_h, \n",
    "                             pixel_num_v=N, \n",
    "                             pixel_size_v=pixel_size_h, \n",
    "                             dist_source_center=dist_source_center, \n",
    "                             dist_center_detector=dist_center_detector)\n",
    "\n",
    "print('Cone-beam acquisition geometry:\\n{}'.format(ag_cone))\n",
    "print('Dimension labels:\\n{}'.format(ag_cone.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conventionally, the voxel size is related to the detector pixel size through a geometrical magniﬁcation:\n",
    "\n",
    "$$F = \\frac{r_1 + r_2}{r_1}$$\n",
    "\n",
    "where $r_1$ and $r_2$ are `dist_source_center` and `dist_center_detector`, respectively.\n",
    "\n",
    "Given `ag_cone` acquisition geometry, default `ImageGeometry` parameters can be calculated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cone-beam ImageGeometry\n",
    "# calculate geometrical magnification\n",
    "mag = (ag_cone.dist_source_center + ag_cone.dist_center_detector) / ag_cone.dist_source_center\n",
    "\n",
    "ig_cone = ImageGeometry(voxel_num_x = ag_cone.pixel_num_h,\n",
    "                        voxel_size_x = ag_cone.pixel_size_h / mag,\n",
    "                        voxel_num_y = ag_cone.pixel_num_h,\n",
    "                        voxel_size_y = ag_cone.pixel_size_h / mag,\n",
    "                        voxel_num_z = ag_cone.pixel_num_v,\n",
    "                        voxel_size_z = ag_cone.pixel_size_v / mag)\n",
    "\n",
    "print('Cone-beam image geometry:\\n{}'.format(ig_cone))\n",
    "print('Dimension labels:\\n{}'.format(ig_cone.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating AcquisitionData and ImageData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AcquisiionData` and `ImageData` inherit from the same parent `DataContainer` class, therefore they behave the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are algebraic operations defined for both `AcquisiionData` and `ImageData`. Following operations are defined:\n",
    "\n",
    "binary operations (between two DataContainers or scalar and DataContainer)\n",
    "- \\+ addition\n",
    "- \\- subtraction\n",
    "- \\/ division\n",
    "- \\* multiplication\n",
    "- \\** power\n",
    "- maximum\n",
    "- minimum\n",
    "    \n",
    "in-place operations\n",
    "- \\+= \n",
    "- \\-= \n",
    "- \\*=\n",
    "- \\**=\n",
    "- /=     \n",
    "\n",
    "unary operations\n",
    "- abs\n",
    "- sqrt\n",
    "- sign\n",
    "- conjugate\n",
    "    \n",
    "reductions\n",
    "- minimum\n",
    "- maximum\n",
    "- sum\n",
    "- norm\n",
    "- dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are examples of operations listed above for `AcquistionData`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ag_par.allocate(2)\n",
    "b = ag_par.allocate(3)\n",
    "print('a \\n{}\\n'.format(a.as_array()))\n",
    "print('b \\n{}\\n'.format(b.as_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## binary operations\n",
    "c = a + b\n",
    "print('a + b \\n{}\\n'.format(c.as_array()))\n",
    "# or alternatively\n",
    "c = a.add(b)\n",
    "print('a + b \\n{}\\n'.format(c.as_array()))\n",
    "d = 3 ** a\n",
    "print('3 ** a \\n{}\\n'.format(d.as_array()))\n",
    "e = a ** b\n",
    "print('a ** b \\n{}\\n'.format(e.as_array()))\n",
    "f = a.maximum(b)\n",
    "print('max(a,b) \\n{}\\n'.format(f.as_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in-place operations\n",
    "b **= b\n",
    "print('b ** b \\n{}\\n'.format(b.as_array()))\n",
    "a += b\n",
    "print('a + b \\n{}\\n'.format(a.as_array()))\n",
    "# or alternatively\n",
    "a.add(b, out = a)\n",
    "print('a + b \\n{}\\n'.format(a.as_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## unary operation\n",
    "g = a.sign()\n",
    "print('sign(a) \\n{}\\n'.format(g.as_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reductions\n",
    "h = a.norm()\n",
    "print('norm(a) \\n{}\\n'.format(h))\n",
    "i = a.dot(b)\n",
    "print('dot(a,b) \\n{}\\n'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few examples for `ImageData`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ig_par.allocate(2)\n",
    "b = ig_par.allocate(3)\n",
    "print('a \\n{}\\n'.format(a.as_array()))\n",
    "print('b \\n{}\\n'.format(b.as_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "print('a + b \\n{}\\n'.format(c.as_array()))\n",
    "b **= b\n",
    "print('b ** b \\n{}\\n'.format(b.as_array()))\n",
    "a += b\n",
    "print('a + b \\n{}\\n'.format(a.as_array()))\n",
    "g = a.sign()\n",
    "print('sign(a) \\n{}\\n'.format(g.as_array()))\n",
    "h = a.norm()\n",
    "print('norm(a) \\n{}\\n'.format(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='image_data'></a>\n",
    "The Framework provides a number of test images\n",
    "\n",
    "- BOAT = 'boat.tiff'\n",
    "- CAMERA = 'camera.png'\n",
    "- PEPPERS = 'peppers.tiff'\n",
    "- RESOLUTION_CHART = 'resolution_chart.tiff'\n",
    "- SIMPLE_PHANTOM_2D = 'hotdog'\n",
    "- SHAPES = 'shapes.png'\n",
    "    \n",
    "Here we load a 'hotdog' image (a simple CT phantom consisting of 2 materials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ccpi.framework import TestData\n",
    "import os, sys\n",
    "\n",
    "# initialise loader\n",
    "loader = TestData(data_dir=os.path.join(sys.prefix, 'share','ccpi'))\n",
    "data = loader.load(TestData.SIMPLE_PHANTOM_2D, size=(200,200))\n",
    "# get ImageGeometry\n",
    "ig = data.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ImageGeometry:\\n{}'.format(ig))\n",
    "print('Dimensions and Labels = {}, {}'.format(data.shape, data.dimension_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plotter2D(data,\n",
    "          \"Image\",\n",
    "          cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AcquisitionData` and `ImageData` provide a simple method to produce a subset of itself based on the axis we would like to have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose data using subset method\n",
    "data_subset = data.subset(['horizontal_y', 'horizontal_x'])\n",
    "\n",
    "plotter2D(data_subset,\n",
    "          \"Transposed image\",\n",
    "          cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract single row\n",
    "data_profile = data_subset.subset(horizontal_y=100)\n",
    "\n",
    "plt.plot(data_profile.as_array())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A middle slice of [cone-beam data](#cone_beam_geometry) has [fan-beam geometry](#fan_beam_geometry). Get middle slice from cone-beam `AcquisitionData` using the subset method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate cone-beam acquisition data\n",
    "ad_cone = ag_cone.allocate('random', seed=5)\n",
    "\n",
    "print('3D cone image geometry:\\n {}'.format(ad_cone.geometry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract middle-slice\n",
    "# AcquisitionData container\n",
    "ad_middle_slice = ad_cone.subset(vertical=ad_cone.geometry.pixel_num_v // 2)\n",
    "# and corresponding AcquisitionGeometry\n",
    "ag_middle_slice = ad_middle_slice.geometry\n",
    "\n",
    "print('Dimensions and Labels = {}, {}\\n'.format(ad_middle_slice.shape, ad_middle_slice.dimension_labels))\n",
    "print('Middle slice acquisition geometry:\\n {}'.format(ag_middle_slice))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read/ write AcquisitionData and ImageData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Framework provides classes to read and write `AcquisitionData` and `ImageData` as NEXUS files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ccpi.io import NEXUSDataWriter, NEXUSDataReader\n",
    "\n",
    "# initialise NEXUS Writer\n",
    "writer = NEXUSDataWriter()\n",
    "writer.set_up(file_name='/home/sirfuser/tmp_nexus.nxs',\n",
    "              data_container=ad_middle_slice)\n",
    "# write data\n",
    "writer.write_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "# initialize NEXUS reader\n",
    "reader = NEXUSDataReader()\n",
    "reader.set_up(nexus_file='/home/sirfuser/tmp_nexus.nxs')\n",
    "# load data\n",
    "ad1 = reader.load_data()\n",
    "# get AcquisiionGeometry\n",
    "ag1 = reader.get_geometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensions and Labels = {}, {}\\n'.format(ad1.shape, ad1.dimension_labels))\n",
    "print('Acquisition geometry:\\n {}'.format(ag1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor\n",
    "`Processor` takes as an input `DataContainer` and returns either another `DataContainer` or some number. The aim of this class is to simplify the writing of processing pipelines.\n",
    "\n",
    "#### Resizer\n",
    "Quite often we need either crop or downsample data; `Resizer` provides a convinient way to perform these operations for both `ImageData` and `AcquisitionData`. Here we use an image from this [example](#image_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ccpi.processors import Resizer\n",
    "\n",
    "# show test image\n",
    "plotter2D(data,\n",
    "          \"Test image\",\n",
    "          cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop ImageData along 1st dimension\n",
    "# initialise Resizer\n",
    "resizer_crop = Resizer(binning = [1, 1], roi = [-1, (20,180)])\n",
    "# pass DataContainer\n",
    "resizer_crop.input = data\n",
    "data_cropped = resizer_crop.process()\n",
    "# get new ImageGeometry\n",
    "ig_data_cropped = data_cropped.geometry\n",
    "\n",
    "plotter2D([data, data_cropped],\n",
    "          [\"Original image\", \"Cropped image\"],\n",
    "          cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original image, dimensions and Labels = {}, {}\\n'.format(data.shape, data.dimension_labels))\n",
    "print('Cropped image, dimensions and Labels = {}, {}\\n'.format(data_cropped.shape, data_cropped.dimension_labels))\n",
    "print('Original ImageGeometry:\\n{}'.format(ig))\n",
    "print('Cropped ImageGeometry:\\n{}'.format(ig_data_cropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-bin ImageData\n",
    "# initialise Resizer\n",
    "resizer_rebin = Resizer(binning = [3, 5], roi = -1)\n",
    "# pass the image\n",
    "resizer_rebin.input = data\n",
    "data_rebinned = resizer_rebin.process()\n",
    "# get new ImageGeometry\n",
    "ig_data_rebinned = data_rebinned.geometry\n",
    "\n",
    "plotter2D([data, data_rebinned],\n",
    "          [\"Original image\", \"Rebinned image\"],\n",
    "          cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original image, dimensions and Labels = {}, {}\\n'.format(data.shape, data.dimension_labels))\n",
    "print('Rebinned image, dimensions and Labels = {}, {}\\n'.format(data_rebinned.shape, data_rebinned.dimension_labels))\n",
    "print('Original ImageGeometry:\\n{}'.format(ig))\n",
    "print('Rebinned ImageGeometry:\\n{}'.format(ig_data_rebinned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculation of Center of Rotation\n",
    "In the ideal alignment of a CT instrument, orthogonal projection of an axis of rotation onto a detector has to coincide with a vertical midline of the detector. This is barely feasible in practice due to misalignment and/ or kinematic errors in positioning of CT instrument components. A slight offset of the center of rotation with respect to the theoretical position will contribute to the loss of resolution; in more severe cases, it will cause severe artifacts in the reconstructed volume (double-borders). `CenterOfRotationFinder` allows to estimate offset of center of rotation from theoretical. In the current release of the Framework `CenterOfRotationFinder` supports only parallel geometry. Here we use sinogram from this [example](#acquisition_data). \n",
    "\n",
    "`CenterOfRotationFinder` is based on [Nghia Vo's method](https://doi.org/10.1364/OE.22.019078)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from ccpi.processors import CenterOfRotationFinder\n",
    "\n",
    "plotter2D(ad_par,\n",
    "          \"Sinogram acquired with paralell geometry\",\n",
    "          cmap=\"gray\")\n",
    "\n",
    "# initialise CenterOfRotationFinder\n",
    "cor = CenterOfRotationFinder()\n",
    "cor.set_input(ad_par)\n",
    "center_of_rotation = cor.get_output()\n",
    "print(center_of_rotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-channel data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `AcquisitionGeometry`/ `AcquisitionData` and `ImageGeometry`/ `ImageData` can be defined for multi-channel (spectral) CT data using `channels` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-channel fan-beam geometry\n",
    "ag_fan_mc = AcquisitionGeometry(geom_type='cone', \n",
    "                                 dimension='2D', \n",
    "                                 angles=angles, \n",
    "                                 pixel_num_h=N, \n",
    "                                 pixel_size_h=1, \n",
    "                                 dist_source_center=200, \n",
    "                                 dist_center_detector=300,\n",
    "                                 channels=10)\n",
    "\n",
    "print('Multi-channel fan-beam geometry:\\n{}'.format(ag_fan_mc))\n",
    "print('Number of channels:{}\\n'.format(ag_fan_mc.channels))\n",
    "print('Labels = {}'.format(im_data3.dimension_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define multi-channel 2D ImageGeometry\n",
    "ig3 = ImageGeometry(voxel_num_y=5, \n",
    "                    voxel_num_x=4, \n",
    "                    channels=2)\n",
    "\n",
    "# create random integer in [0, max_value] ImageData with the specific geometry\n",
    "im_data3 = ig3.allocate('random_int', seed=10, max_value=500)\n",
    "\n",
    "print('Allocate with random integers \\n{}\\n'.format(im_data3.as_array()))\n",
    "print('Dimensions and Labels  =  {}, {}'.format(im_data3.shape, im_data3.dimension_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the methods described for single-channel data in the sections above are also defined for multi-channel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate multi-channel fan-beam acquisition data\n",
    "ad_fan_mc = ag_fan_mc.allocate()\n",
    "\n",
    "# extract single channel from multi-channel fan-beam data using subset method\n",
    "ad_fan_sc = ad_fan_mc.subset(channel=5)\n",
    "ag_fan_sc = ad_fan_mc.geometry\n",
    "\n",
    "print('Geometry:\\n{}'.format(ag_fan_sc))\n",
    "print('Dimensions and Labels  =  {}, {}'.format(ad_fan_sc.shape, ad_fan_sc.dimension_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
