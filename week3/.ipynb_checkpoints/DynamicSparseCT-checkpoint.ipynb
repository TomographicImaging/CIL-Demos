{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361e939b-553e-4ae2-aa7e-9a01c456b0f7",
   "metadata": {},
   "source": [
    "<h1><center> Dynamic Sparse CT </center></h1>\n",
    "\n",
    "In this demo, **add details about the phtnaom**\n",
    "\n",
    "<h2><center><u> Learning objectives </u></center></h2>   \n",
    "\n",
    "- Create an Acquisition geometry for 2D dynamic tomographic data\n",
    "- Create **Sparse data** using the `Slicer` processor.\n",
    "- Run FBP reconstruction for every time-channel\n",
    "- Setup PDHG for 2 different regularisers: Total Variation and **directional Total Variation**\n",
    "\n",
    "We first import all the necessary libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bb69cd-f82a-4d1d-8291-7be94724866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from cil.framework import AcquisitionGeometry\n",
    "from cil.plugins.astra.operators import ProjectionOperator\n",
    "# from cil.plugins.tigre import ProjectionOperator \n",
    "from cil.io import NEXUSDataWriter, NEXUSDataReader\n",
    "from cil.processors import Slicer\n",
    "from cil.utilities.display import show2D\n",
    "from cil.plugins.astra.processors import FBP\n",
    "from cil.optimisation.algorithms import PDHG\n",
    "from cil.optimisation.operators import GradientOperator, BlockOperator\n",
    "from cil.optimisation.functions import IndicatorBox, BlockFunction, L2NormSquared, MixedL21Norm\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_dTV \n",
    "\n",
    "from utilities import read_frames, read_extra_frames, download_zenodo\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd63f72-a421-433a-9b3f-bc7705e26431",
   "metadata": {},
   "source": [
    "1) First, we download the dynamic tomographic data from \n",
    "[Zenodo](https://zenodo.org/record/3696817#.YGyJMxMzb_Q).\n",
    "\n",
    "2) There are resolutions available of size 256 x 256 or 512 x 512, (GelPhantomData_b4.mat and GelPhantomData_b2.mat respectively). For the paper, we use the 256x256 resolution for simplicity.\n",
    "\n",
    "3) Two additional data are provided in GelPhantom_extra_frames.mat with dense sampled measurements from the first time step and the last (18th) time step.\n",
    "\n",
    "**Note that the pixel size of the detector is wrong. The correct pixel size should be doubled.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f0fa3c-4a7e-4aff-9f0f-c5fbe0de6ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from Zenodo\n",
    "download_zenodo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801e69e-46ad-435a-9709-eaaa1d8cd59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read matlab files for the 17 frames\n",
    "name = \"GelPhantomData_b4\"\n",
    "path = \"MatlabData/\"\n",
    "file_info = read_frames(path, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c48d1ca-3d74-4ce3-84df-7a3ae1230c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sinograms + metadata\n",
    "sinograms = file_info['sinograms']\n",
    "frames = sinograms.shape[0]\n",
    "angles = file_info['angles']\n",
    "distanceOriginDetector = file_info['distanceOriginDetector']\n",
    "distanceSourceOrigin = file_info['distanceSourceOrigin']\n",
    "pixelSize = 2*file_info['pixelSize']\n",
    "numDetectors = file_info['numDetectors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e9acc-2839-4c4f-8f7c-f8abcb3fa3fc",
   "metadata": {},
   "source": [
    "## Exercise 1: Create acquisition and image geometries\n",
    "\n",
    "For this dataset, we have a 2D cone geometry with 17 time channels. Using the metadata above, we can define the acquisition geometry `ag` with \n",
    "\n",
    "<div class=\"alert alert-block\" \n",
    "     style=\"background-color: green; \n",
    "            width: 800px;margin: auto\">\n",
    "    \n",
    "\n",
    "```python    \n",
    "    \n",
    "    ag = AcquisitionGeometry.create_Cone2D(source_position = [0, distanceSourceOrigin],\n",
    "                                           detector_position = [0, -distanceOriginDetector])\\\n",
    "                                        .set_panel(numDetectors, pixelSize)\\\n",
    "                                        .set_channels(frames)\\\n",
    "                                        .set_angles(angles, angle_unit=\"radian\")\\\n",
    "                                        .set_labels(['channel','angle', 'horizontal'])\n",
    "\n",
    "    \n",
    "```\n",
    "\n",
    "</div>\n",
    "<br></br>\n",
    "\n",
    "For the image geometry `ig` we use the following code and crop our image domain to `[256,256]`:\n",
    "<div class=\"alert alert-block\" \n",
    "     style=\"background-color: green; \n",
    "            width: 300px;margin: auto\">\n",
    "    \n",
    "\n",
    "```python    \n",
    "    \n",
    "    ig = ag.get_ImageGeometry()\n",
    "    \n",
    "```\n",
    "\n",
    "</div>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e8a23-f9f1-4c2f-b299-38c1ae4ecbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create acquisition + image geometries\n",
    "ag = AcquisitionGeometry.create_Cone2D(source_position = [0, distanceSourceOrigin],\n",
    "                                       detector_position = [0, -distanceOriginDetector])\\\n",
    "                                    .set_panel(numDetectors, pixelSize)\\\n",
    "                                    .set_channels(frames)\\\n",
    "                                    .set_angles(angles, angle_unit=\"radian\")\\\n",
    "                                    .set_labels(['channel','angle', 'horizontal'])\n",
    "\n",
    "ig = ag.get_ImageGeometry()\n",
    "ig.voxel_num_x = 256\n",
    "ig.voxel_num_y = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb5244-f172-44f6-8e54-dd87626e1805",
   "metadata": {},
   "source": [
    "Then, we create an `AcquisitionData` object by allocating space from the acquisition geometry `ag`. This is filled with every **sinogram per time channel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc31ca-d229-4721-9310-09745d644cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ag.allocate()\n",
    "\n",
    "for i in range(frames):\n",
    "   data.fill(sinograms[i], channel = i) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdaffa7-02fd-44b4-8f1b-c2dc50e41c67",
   "metadata": {},
   "source": [
    "## Show sinogram in 4 different time frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790295f8-877c-406e-bbf2-7c93020885b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sinos = [data.subset(channel=i) for i in [0,5,10,16]]\n",
    "titles_sinos = [\"Time-frame {}\".format(i) for i in [0,5,10,16]]\n",
    "show2D(show_sinos, cmap=\"inferno\", num_cols=4, title=titles_sinos, size=(25,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000a898-da61-4bac-ab5a-ab13b910589d",
   "metadata": {},
   "source": [
    "Then we create 3 different sparse sinogram data, i.e., from the total of 360 projections we select a number of projections depending on the size of the `step`.\n",
    "\n",
    "- step = 1 --> 360/1 projections\n",
    "- step = 5 --> 360/5 = 72 projections\n",
    "- step = 10 --> 360/10 = 36 projections\n",
    "- step = 20 --> 360/20 = 18 projections\n",
    "\n",
    "For every case, we show the dynamic data for 4 different time frames and save them using the `NEXUSDataWriter` processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a613e-44ab-40fe-aff2-906abdd98ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save Sparse Data with different angular sampling: 18, 36, 72, 360 projections\n",
    "for step in [1, 5, 10, 20]:\n",
    "    \n",
    "    name_proj = \"data_{}\".format(int(360/i))\n",
    "    new_data = Slicer(roi={'angle':(0,360,step)})(data)\n",
    "    ag = new_data.geometry\n",
    "    ig = ag.get_ImageGeometry()\n",
    "    ig.voxel_num_x = 256\n",
    "    ig.voxel_num_y = 256 \n",
    "    \n",
    "    show2D(new_data, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",\n",
    "                       cmap=\"inferno\", title=\"Projections {}\".format(int(360/i)), size=(25, 20))    \n",
    "    \n",
    "    writer = NEXUSDataWriter(file_name = \"SparseData/\"+name_proj+\".nxs\",\n",
    "                         data = new_data)\n",
    "    writer.write()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa28bc0-8b06-4abb-ae0d-67fba045d2f2",
   "metadata": {},
   "source": [
    "For the following reconstruction, we use the `data_36` sparse data ( **36 projections** ) and perform\n",
    "\n",
    "- FBP reconstruction per time frame,\n",
    "- Spatiotemporal TV reconstruction,\n",
    "- Directional Total variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b65ce57-be14-4994-bf69-ee5b6b7c4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_proj = 36   \n",
    "reader = NEXUSDataReader(file_name = \"SparseData/data_{}.nxs\".format(num_proj) )\n",
    "data = reader.load_data()\n",
    "    \n",
    "ag = data.geometry\n",
    "ig = ag.get_ImageGeometry()\n",
    "ig.voxel_num_x = 256\n",
    "ig.voxel_num_y = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db351f1-809d-4661-b274-75a5d2049afb",
   "metadata": {},
   "source": [
    "## Channelwise FBP\n",
    "\n",
    "For the **channelwise** FBP reconstruction, we do the following steps\n",
    "\n",
    "- Allocate a space using the full image geometry (2D+channels) `ig` geometry. \n",
    "- Extract 2D acquisition and image geometries, using `ag.subset(channel=0)`, `ig.subset(channel=0)`.\n",
    "- Run FBP reconstruction using the 2D sinogram data for every time frame.\n",
    "- Fill the 2D FBP reconstruction with respect to the `channel=i` using the `fill` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3adba47-2b6e-4510-8d83-00aab10d504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbp_recon = ig.allocate()\n",
    "\n",
    "ag2D = ag.subset(channel=0)\n",
    "ig2D = ig.subset(channel=0)\n",
    "\n",
    "for i in range(ig.channels):\n",
    "    \n",
    "    tmp = FBP(ig2D, ag2D)(data.subset(channel=i))\n",
    "    fbp_recon.fill(tmp, channel=i)\n",
    "    print(\"Finish FBP reconstruction for time frame {}\".format(i), end='\\r')\n",
    "        \n",
    "show2D(fbp_recon, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=\"Projections {}\".format(i), size=(25, 20))\n",
    "    \n",
    "name_fbp = \"FBP_projections_{}\".format(num_proj)\n",
    "writer = NEXUSDataWriter(file_name = \"FBP_reconstructions/\"+name_fbp+\".nxs\",\n",
    "                     data = fbp_recon)\n",
    "writer.write()      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abe4be-8b5a-47a1-b066-cc74ad700bc1",
   "metadata": {},
   "source": [
    "## Exercise 2: Total Variation reconstruction\n",
    "\n",
    "For the TV reconstruction, we use the explicit formulation of the PDHG algorithm. See the [PDHG notebook](appendix.ipynb/#PDHG) for more information.\n",
    "\n",
    "- Define the `ProjectionOperator` and the `GradientOperator` using `correlation=SpaceChannels`.\n",
    "\n",
    "<div class=\"alert alert-block\" \n",
    "     style=\"background-color: green; \n",
    "            width: 600px;margin: auto\">\n",
    "    \n",
    "\n",
    "```python    \n",
    "\n",
    "        A = ProjectionOperator(ig, ag, 'gpu')        \n",
    "        Grad = GradientOperator(ig, correlation = \"SpaceChannels\") \n",
    "    \n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "- Use the `BlockOperator` to define the operator $K$. \n",
    "\n",
    "<div class=\"alert alert-block\" \n",
    "     style=\"background-color: green; \n",
    "            width: 370px;margin: auto\">\n",
    "    \n",
    "\n",
    "```python    \n",
    "\n",
    "        K = BlockOperator(A, Grad)        \n",
    "            \n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "- Use the `BlockFunction` to define the function $\\mathcal{F}$ that contains the fidelity term `L2NormSquared(b=data)` and the regularisation term `alpha_tv * MixedL21Norm()`, with `alpha_tv = 0.00063`. Finally, use the `IndicatorBox(lower=0.0)` to enforce a non-negativity constraint for the function $\\mathcal{G}$.\n",
    "\n",
    "<div class=\"alert alert-block\" \n",
    "     style=\"background-color: green; \n",
    "            width: 750px;margin: auto\">\n",
    "    \n",
    "\n",
    "```python    \n",
    "\n",
    "        F = BlockFunction(0.5*L2NormSquared(b=data), alpha_tv * MixedL21Norm()) \n",
    "        G = IndicatorBox(lower=0)\n",
    "            \n",
    "```\n",
    "\n",
    "</div>\n",
    "<br></br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b8d75-0be2-4d6f-b3e7-06b937a5d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ProjectionOperator(ig, ag, 'gpu')        \n",
    "Grad = GradientOperator(ig, correlation = \"SpaceChannels\") \n",
    "\n",
    "K = BlockOperator(A, Grad)\n",
    "\n",
    "alpha_tv = 0.00063\n",
    "F = BlockFunction(0.5*L2NormSquared(b=data), alpha_tv * MixedL21Norm())  \n",
    "G = IndicatorBox(lower=0)\n",
    "\n",
    "normK = K.norm()\n",
    "sigma = 1./normK\n",
    "tau = 1./normK\n",
    "\n",
    "pdhg = PDHG(f = F, g = G, operator=K, max_iteration = 300,\n",
    "            update_objective_interval = 100)    \n",
    "pdhg.run(verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1859d906-95dc-4288-ad5c-38091897b280",
   "metadata": {},
   "source": [
    "## Show TV reconstruction for 4 different time frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8fd5a6-d608-49f8-bea2-ab6b90a2fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(pdhg.solution, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))\n",
    "    \n",
    "name_tv = \"TV_reconstruction_projections\".format(num_proj)\n",
    "writer = NEXUSDataWriter(file_name = \"TV_reconstructions/\"+name_tv+\".nxs\",\n",
    "                     data = pdhg.solution)\n",
    "writer.write()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd8bd6-93df-4d40-878f-db7a89da9e98",
   "metadata": {},
   "source": [
    "## Directional Total Variation\n",
    "\n",
    "For our final reconstruction, we use a **structure-based prior**, namely the directional Total Variation (dTV) introduced in [Ehrhardt MJ, Arridge SR](https://doi.org/10.1109/tip.2013.2277775).\n",
    "\n",
    "In comparison with the Total variation regulariser, \n",
    "$$\n",
    "\\mathrm{TV}(u) = \\|\\nabla u\\|_{2,1} = \\sum |\\nabla u\\|_{2},\n",
    "$$\n",
    "\n",
    "in the Direction Total variation, a weight in front of the gradient is used based on a **reference image**, which acts as prior information from which edge structures are propagated into the reconstruction process. For example, an image from another modality, e.g., MRI, , can be used in the PET reconstruction, see [Ehrhardt2016](https://ieeexplore.ieee.org/document/7452643/), [Ehrhardt2016MRI](https://epubs.siam.org/doi/10.1137/15M1047325). Another popular setup, is to use either both modalities or even channels in a joint reconstruction problem simultaneously, improving significantly the quality of the image, see for instance [Knoll et al](https://ieeexplore.ieee.org/document/7466848), [Kazantsev_2018](https://doi.org/10.1088/1361-6420/aaba86).\n",
    "\n",
    "**Definition:** The dTV regulariser of the image $u$ given the reference image $v$ is defined as \n",
    "\n",
    "$$\\begin{equation}\n",
    "d\\mathrm{TV}(u,v)  := \\|D_{v}\\nabla u\\|_{2,1} = \\sum_{i,j=1}^{M,N} \\big(|D_{v}\\nabla u|_{2}\\big)_{i,j},\n",
    "\\label{dTV_definition}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where the weight $D_{v}$ depends on the normalised gradient $\\xi_{v}$ of the reference image $v$, \n",
    "\n",
    "$$\\begin{equation}\n",
    "D_{v} = \\mathbb{I}_{2\\times2} - \\xi_{v}\\xi_{v}^T, \\quad \\xi_{v} = \\frac{\\nabla v}{\\sqrt{\\eta^{2} + |\\nabla v|_{2}^{2}}}, \\quad \\eta>0.\n",
    "\\label{weight_D}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In this dynamic sparse CT framework, we apply the dTV regulariser for each time frame $t$ which results to the following minimisation problem:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "u^{*}_{t} = \\underset{u}{\\operatorname{argmin}}  \\, \\frac{1}{2}\\| A_\\text{sc} u_{t}  - b_{t} \\|^{2} + \\alpha \\, d\\mathrm{TV}(u_{t}, v_{t})\\quad \\mbox{(Dynamic dTV)}, \\label{dynamic_dtv_problem}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $A_\\text{sc}$, $b_{t}$, $u^{*}_{t}$, denote the single channel `ProjectionOperator`, the sinogram data and the reconstructed image for the time frame $t$ respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d62bab-eacc-45be-ac6d-29d79fbccbd5",
   "metadata": {},
   "source": [
    "## Reference images\n",
    "\n",
    "In terms of the reference images $(v_{t})_{t=0}^{16}$, we are going to use the FBP reconstructions of the additional tomographic data. There are two datasets in `GelPhantom_extra_frames.mat` with dense sampled measurements from the first and last (18th) time steps:\n",
    "\n",
    "- Pre-scan data with 720 projections\n",
    "- Post-scan data with 1600 projections\n",
    "\n",
    "We first read the matlab files and create the following acquisition data:\n",
    "\n",
    "- data_pre_scan\n",
    "- data_post_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648bd45a-a05a-4299-8cc6-aace38cce93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read matlab files for the extra frames\n",
    "name = \"GelPhantom_extra_frames\"\n",
    "path = \"MatlabData/\"\n",
    "pre_scan_info = read_extra_frames(path, name, \"GelPhantomFrame1_b4\")\n",
    "post_scan_info = read_extra_frames(path, name, \"GelPhantomFrame18_b4\")\n",
    "\n",
    "# Acquisition geometry for the 1st frame: 720 projections\n",
    "ag2D_pre_scan = AcquisitionGeometry.create_Cone2D(source_position = [0,   pre_scan_info['distanceSourceOrigin']],\n",
    "                                       detector_position = [0, -pre_scan_info['distanceOriginDetector']])\\\n",
    "                                    .set_panel(num_pixels = pre_scan_info['numDetectors'], pixel_size = 2*pre_scan_info['pixelSize'])\\\n",
    "                                    .set_angles(pre_scan_info['angles'])\\\n",
    "                                    .set_labels(['angle', 'horizontal'])\n",
    "\n",
    "# Acquisition geometry for the 18th frame: 1600 projections\n",
    "ag2D_post_scan = AcquisitionGeometry.create_Cone2D(source_position = [0,   post_scan_info['distanceSourceOrigin']],\n",
    "                                       detector_position = [0, -post_scan_info['distanceOriginDetector']])\\\n",
    "                                    .set_panel(num_pixels = post_scan_info['numDetectors'], pixel_size = 2*post_scan_info['pixelSize'])\\\n",
    "                                    .set_angles(post_scan_info['angles'])\\\n",
    "                                    .set_labels(['angle', 'horizontal'])\n",
    "\n",
    "data_pre_scan = ag2D_pre_scan.allocate()\n",
    "data_pre_scan.fill(pre_scan_info['sinograms'])\n",
    "\n",
    "data_post_scan = ag2D_post_scan.allocate()\n",
    "data_post_scan.fill(post_scan_info['sinograms'])\n",
    "\n",
    "\n",
    "show2D([data_pre_scan,data_post_scan], title=[\"Pre-scan 720 projections\", \"Post-scan 1600 projections\"], cmap=\"inferno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d73521-626f-479d-a667-23d73aabb388",
   "metadata": {},
   "source": [
    "## FBP reconstruction (Reference images)\n",
    "\n",
    "For the FBP reconstruction of the pre/post scan we use the 2D image geometry, `ig2D` and the corresponding acquisition geometries `ag2D_pre_scan` and `ag2D_post_scan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62345b-bf6b-4a1e-88b9-97ef8b34ce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "fbp_recon_pre_scan = FBP(ig2D, ag2D_pre_scan)(data_pre_scan)\n",
    "fbp_recon_post_scan = FBP(ig2D, ag2D_post_scan)(data_post_scan)\n",
    "\n",
    "show2D([fbp_recon_pre_scan,fbp_recon_post_scan], \n",
    "       title=[\"FBP: Pre-scan\", \"FBP: Post-scan\"], cmap=\"inferno\", origin=\"upper\", fix_range=(0,0.065))\n",
    "\n",
    "name_fbp_pre_scan = \"FBP_pre_scan\"\n",
    "writer = NEXUSDataWriter(file_name = \"FBP_reconstructions/\"+name_fbp_pre_scan+\".nxs\",\n",
    "                     data = fbp_recon_pre_scan)\n",
    "writer.write() \n",
    "\n",
    "name_fbp_post_scan = \"FBP_post_scan\"\n",
    "writer = NEXUSDataWriter(file_name = \"FBP_reconstructions/\"+name_fbp_post_scan+\".nxs\",\n",
    "                     data = fbp_recon_post_scan)\n",
    "writer.write() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da9dea-48cd-4b1a-b4f7-cab06bbf5c8f",
   "metadata": {},
   "source": [
    "## Edge information from the normalised gradient $\\,\\xi_{v}$\n",
    "\n",
    "In the following we compute the normalised gradient $\\,\\xi_{v}$ for the two reference images using different $\\eta$ values:\n",
    "\n",
    "$$\\xi_{v} = \\frac{\\nabla v}{\\sqrt{\\eta^{2} + |\\nabla v|_{2}^{2}}}, \\quad \\eta>0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed73e9-7851-4d2e-a93b-8f974d316af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xi_vector_field(image, eta):\n",
    "    \n",
    "    ig = image.geometry\n",
    "    ig.voxel_size_x = 1.\n",
    "    ig.voxel_size_y = 1.\n",
    "    G = GradientOperator(ig)\n",
    "    numerator = G.direct(image)\n",
    "    denominator = np.sqrt(eta**2 + numerator.get_item(0)**2 + numerator.get_item(1)**2)\n",
    "    xi = numerator/denominator\n",
    "                \n",
    "    return (xi.get_item(0)**2 + xi.get_item(1)**2).sqrt()\n",
    "\n",
    "etas = [0.001, 0.005]\n",
    "\n",
    "xi_post_scan = []\n",
    "xi_pre_scan = []\n",
    "\n",
    "for i in etas:\n",
    "    \n",
    "    xi_post_scan.append(xi_vector_field(fbp_recon_post_scan, i))\n",
    "    xi_pre_scan.append(xi_vector_field(fbp_recon_pre_scan, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765ffe34-e99f-468d-903c-fb21925ed2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_etas = [\"$\\eta$ = {}\".format(eta) for eta in etas]\n",
    "show2D(xi_pre_scan, cmap=\"inferno\", title=title_etas, origin=\"upper\", num_cols=2, size=(10,10))\n",
    "show2D(xi_post_scan, cmap=\"inferno\", title=title_etas, origin=\"upper\", num_cols=2,size=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c301d-7059-461c-990f-0009672f6878",
   "metadata": {},
   "source": [
    "## Setup and run implicit PDHG reconstruction, using the dTV regulariser\n",
    "\n",
    "In total we have 17 time frames, and we need 17 reference images. \n",
    "\n",
    "Due to a slight movement of the sample at the beginning of the experiment, we apply the pre-scan reference image for the first time frame and use the post-scan reference image for the remaining time frames. \n",
    "\n",
    "One could apply other configurations for the reference image in the intermediate time frames. For example, in order to reconstruct the $(t+1)$th time frame, one could use the $t$th time frame reconstruction as reference. A more sophisticated reference selection approach is applied in hyperspectral computed tomography in [Kazantsev_2018](https://iopscience.iop.org/article/10.1088/1361-6420/aaba86).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc15c1-67f7-4d0d-b42a-bbc9a466002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "K = ProjectionOperator(ig2D, ag2D, 'gpu') \n",
    "\n",
    "normK = K.norm()\n",
    "sigma = 1./normK\n",
    "tau = 1./normK  \n",
    "\n",
    "dtv_recon = ig.allocate()\n",
    "\n",
    "alpha_dtv = 0.0072\n",
    "eta = 0.005\n",
    "\n",
    "max_iterations = 100\n",
    "\n",
    "for tf in range(17):\n",
    "    \n",
    "    if tf==0:        \n",
    "        G = alpha_dtv * FGP_dTV(reference = fbp_recon_pre_scan, eta=eta, device='gpu')  \n",
    "    else:        \n",
    "        G = alpha_dtv * FGP_dTV(reference = fbp_recon_post_scan, eta=eta, device='gpu')\n",
    "        \n",
    "    F = 0.5 * L2NormSquared(b=data.subset(channel=tf))\n",
    "    \n",
    "    \n",
    "    pdhg = PDHG(f=F, g=G, operator = K, tau = tau, sigma = sigma,\n",
    "            max_iteration = max_iterations, update_objective_interval=100)\n",
    "    pdhg.run(verbose = 0)\n",
    "    clear_output(wait=True)\n",
    "    print(\"Finish dTV regularisation for the frame {} with {} projections\\n\".format(tf,num_proj), end='\\r')\n",
    "    dtv_recon.fill(pdhg.solution, channel=tf)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2eaf6c-0479-4f78-8670-6a68f5074a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(dtv_recon, slice_list = [0,5,10,16], num_cols=4, origin=\"upper\",fix_range=(0,0.065),\n",
    "                   cmap=\"inferno\", title=titles_sinos, size=(25, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cil21_tigre_astra] *",
   "language": "python",
   "name": "conda-env-cil21_tigre_astra-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
