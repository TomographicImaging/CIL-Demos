{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#  Copyright 2019 - 2024 United Kingdom Research and Innovation\n",
    "#  Copyright 2019 - 2022 The University of Manchester\n",
    "#  Copyright 2019 - 2024 Technical University of Denmark\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "#   Authored by:    Jakob S. JÃ¸rgensen (DTU)\n",
    "#                   Gemma Fardell (UKRI-STFC)     \n",
    "#                   Laura Murgatroyd (UKRI-STFC)\n",
    "#                   Margaret Duff (UKRI-STFC)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of optimisation-based reconstruction in CIL\n",
    "\n",
    "### The case is a 3D parallel-beam synchrotron dataset of a steel wire.\n",
    "\n",
    "This exercise introduces you to regularised reconstructions. By using prior knowledge of the sample we can choose the most suitable regulariser for the problem. As we introduce different priors we need to use different algorithms to solve the optimisation problem.\n",
    "\n",
    "Learning objectives are:\n",
    "- Load a dataset and reconstruct with FBP\n",
    "- Set-up a least-squares problem to solve using CIL's algorithms, a projection operator and objective function \n",
    "- Add regularisation to the least-squares problem and compare the results: Tikhonov, Non-negativity, L1-Norm, Total-Variation\n",
    "- Solve the optimisation problem with the appropriate algorithm: Gradient Descent, FISTA, PDHG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import everything we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import algorithms, operators and functions from CIL optimisation module\n",
    "from cil.optimisation.algorithms import GD, FISTA, PDHG\n",
    "from cil.optimisation.operators import BlockOperator, GradientOperator,\\\n",
    "                                       GradientOperator\n",
    "from cil.optimisation.functions import IndicatorBox, MixedL21Norm, L2NormSquared, \\\n",
    "                                       BlockFunction, L1Norm, LeastSquares, \\\n",
    "                                       OperatorCompositionFunction, TotalVariation, \\\n",
    "\n",
    "# Import CIL Processors for preprocessing\n",
    "from cil.processors import CentreOfRotationCorrector, Slicer, TransmissionAbsorptionConverter\n",
    "\n",
    "# Import CIL display function\n",
    "from cil.utilities.display import show2D\n",
    "\n",
    "# Import from CIL ASTRA plugin\n",
    "from cil.plugins.astra import ProjectionOperator\n",
    "\n",
    "# Import FBP from CIL recon class\n",
    "from cil.recon import FBP\n",
    "\n",
    "#Import Total Variation from the regularisation toolkit plugin\n",
    "from cil.plugins.ccpi_regularisation.functions import FGP_TV\n",
    "\n",
    "# All external imports\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, pre-process and reconstruct the data using FBP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly as in the notebook [1_Introduction/03_preprocessing](../1_Introduction/03_preprocessing.ipynb), we load the steel-wire demonstration data provided as part of CIL, carry out some preprocessing and FBP reconstructions for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example data set\n",
    "from cil.utilities.dataexample import SYNCHROTRON_PARALLEL_BEAM_DATA\n",
    "data_sync = SYNCHROTRON_PARALLEL_BEAM_DATA.get()\n",
    "\n",
    "# Preprocessing\n",
    "scale = data_sync.get_slice(vertical=20).mean()\n",
    "data_sync = data_sync/scale\n",
    "data_sync = TransmissionAbsorptionConverter()(data_sync)\n",
    "data_sync = CentreOfRotationCorrector.xcorrelation(slice_index='centre')(data_sync)\n",
    "\n",
    "# Crop data and reorder for ASTRA backend\n",
    "data90 = Slicer(roi={'angle':(0,90), \n",
    "                     'horizontal':(20,140,1)})(data_sync)\n",
    "data90.reorder(order='astra')\n",
    "\n",
    "# Set up and run FBP for 90-angle dataset\n",
    "recon90 = FBP(data90, backend='astra').run(verbose=0)\n",
    "\n",
    "# Set up and run FBP for 15-angle dataset\n",
    "data15 = Slicer(roi={'angle': (0,90,6)})(data90)\n",
    "recon15 = FBP(data15, backend='astra').run(verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom parameters for show2D for visualizing all reconstructions consistently\n",
    "sx = 44\n",
    "sz = 103\n",
    "ca1 = -0.01\n",
    "ca2 =  0.11\n",
    "slices = [('horizontal_x',sx),('vertical',sz)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show a slice of the 90-degree FBP reconstruction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(recon90, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the 15-projection FBP reconstruction, which contains severe streak artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(recon15, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Gradient Descent to solve a Least Squares problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimisation-based reconstruction is based on a fully discretized model that is conventionally assumed to be linear:\n",
    "$$Au = b$$\n",
    "where $A$  is the linear operator known as the system matrix representing forward projection of an image to its sinogram, $b$ is the sinogram data, and $u$ is the unknown image to be reconstructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we are going to need is the LinearOperator representing forward and back-projections. We set up the ProjectionOperator from the ASTRA plugin by passing the 15-projection acquisition geometry, and image geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ag = data15.geometry\n",
    "ig = ag.get_ImageGeometry()\n",
    "A = ProjectionOperator(ig, ag, device=\"gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to work with the 15-projection dataset here and refer to it by `b` for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = data15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, as we normally have noise, model errors and other inconsistencies in the data, we cannot expect a solution exists to $Au = b$.  We therefore relax the problem and aim to find a solution that is as close as possible to fitting the data. This is conventionally measured in a least-squares sense in that we solve the least-squares problem\n",
    "$$ \\min_u \\|Au - b \\|_2^2 $$\n",
    "where\n",
    "$$\\|y \\|_2^2 = \\sum_i y_i^2.$$\n",
    "The function that is to be minimized is called the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reconstruction is the image $u$ that is the solution to the optimisation problem, i.e., that results in the lowest possible value of the objective function, in this case of the (squared) residual norm $\\|Au - b \\|_2^2$. In order to find the solution we use an iterative optimisation algorithm. Many exist, perhaps the most basic one is the gradient descent method, which is available in CIL as the [`GD`](https://tomographicimaging.github.io/CIL/nightly/optimisation.html#gd) algorithm. To set it up we need to specify the objective function in terms of a CIL Function. For [`LeastSquares`](https://tomographicimaging.github.io/CIL/nightly/optimisation.html#least-squares) this is simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = LeastSquares(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In iterative algorithms we must provide an initial point from which to start, here we choose the zero image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = ig.allocate(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`f1` is a CIL Function and CIL Functions can be evaluated for particular input images, for example we can evaluate it (which is the starting objective value of the optimisation problem)  for `x0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIL Functions provide a number of methods that are used by optimisation algorithms, most notably, if a function is smooth (continuously differentiable), then a CIL Function provides the `gradient` method using which the gradient of the function can be evaluated at a particular input image. For example we can evaluate the gradient at `x0` and since it contains an element for each voxel, we can display it as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(f1.gradient(x0),slice_list=slices,origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the gradient descent algorithm, we specify:\n",
    " - `initial` - the initial point \n",
    " - `f` - the objective function\n",
    " - `step_size` - whether to use a fixed step size or a back-tracking line search (None) or  a function that takes the initialised algorithm and returns a step size which can vary e.g. on objective value or iteration number. For more information see: https://tomographicimaging.github.io/CIL/nightly/optimisation/#step-size-methods. \n",
    " - `update_objective_interval` - how often to evaluate the objective function value\n",
    " - `preconditioner` (optional) a functional that takes a calculated gradient and the initialised algorithm and *preconditions* the gradient, e.g. multiplies it by a matrix to provide a (hopefully) more effective descent direction. For more information see, https://tomographicimaging.github.io/CIL/nightly/optimisation/#preconditioners. \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_LS = GD(initial=x0, \n",
    "             f=f1, \n",
    "             step_size=None, \n",
    "             update_objective_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the algorithm is set up, we can run it for a specified number of iterations and here using `verbose=1` to print out progress information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_LS.run(300, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once done, the latest image/solution in the algorithm can be shown as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(myGD_LS.solution, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add an L2-Norm penalty to the solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a basic least-squares example. We can specify more involved optimisation problems by combining multiple CIL Functions through addition, scalar multiplication as well as composition with CIL operators. For example, as an alternative to using CGLS to solve the Tikhonov problem with gradient operator D, i.e.,  \n",
    "$$\\min_u \\|Au-b\\|_2^2 + \\alpha^2\\|Du\\|_2^2$$\n",
    "\n",
    "Tikhonov regularisation is more explicit in that a regularisation term is added to the least squares fitting term, specifically a squared 2-norm. This is covered in detail by the next notebook [02_tikhonov_block_framework.ipynb](02_tikhonov_block_framework.ipynb)\n",
    "\n",
    "We can set this objective function up step by step. First, we set again the least-squares data fitting term as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = LeastSquares(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify the operator `D` in the regularisation term and the value of the regularisation parameter `alpha`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = GradientOperator(ig)\n",
    "alpha = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can construct the regularisation term by composing the squared L2-norm with the operator D as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = OperatorCompositionFunction(L2NormSquared(),D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we form the full optimisation problem from the components defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = f1 + (alpha**2)*f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can set up a gradient descent algorithm to solve this problem. This time, we determine a fixed step size, the reciprical of the Lipschitz constant of `f`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_tikh = GD(initial=x0, \n",
    "               f=f, \n",
    "               step_size=1/f.L, \n",
    "               update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myGD_tikh.run(200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(myGD_tikh.solution, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook we solve the Tikhonov problem using CGLS. As an exercise  you can compare the result and performance of the two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use FISTA to solve a Least Squares problem with a non-negativity constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many useful reconstruction methods involve minimisation of functions that are NOT smooth and in those cases we need dedicated optimisation algorithms for non-smooth problems. In this notebook we consider optimisation problems that can be written in the form\n",
    "$$\\min_u f(u) + g(u)  $$\n",
    "where $f$ is a smooth function as before, but $g$ may now be non-smooth. $g$ further needs to be \"simple\", in a certain sense, namely it should have a proximal mapping that is easy to evaluate. Proximal mapping can be thought of a generalisation of the gradient for a non-smooth function.\n",
    "\n",
    "For this problem class the algorithm FISTA (Fast iterative shrinkage thresholding algorithm) can be employed. It is also known as the accelerated proximal gradient method.\n",
    "\n",
    "We consider a couple of examples for different functions $g$. First we consider again least-squares but with a non-negativity constraint on all pixels. This problem can be written \n",
    "$$\\min_u \\|Au-b\\|_2^2 + I_{C}(u) $$\n",
    "where $I_{C}(u)$ is a special convex function known as an indicator function, which takes on the value 0 in its convex domain C (which we here take to be the set of images with only nonnegative pixel values), and the (extended) value of $\\infty$ outside its domain. This can be specified in CIL using an `IndicatorBox` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = LeastSquares(A, b)\n",
    "G = IndicatorBox(lower=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A FISTA algorithm instance can be set up similarly to the GD algorithm but specifying the $f$ and $g$ functions separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTANN = FISTA(f=F, \n",
    "                  g=G, \n",
    "                  initial=x0, \n",
    "                  update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run it and display the resulting solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTANN.run(300, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(myFISTANN.solution, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the non-negativity constraint, as expected, prevents any negative values. Furthermore, this has a positive effect of removing some of the streak artifacts in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use FISTA to solve a Least Squares problem with an L1-norm penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is sparsity regularisation which we can achieve by choosing $g$ as the L1-norm multiplied by a regularisation parameter $\\alpha$ to balance the strength of fitting to the data and enforcing regularisation:\n",
    "$$g(u) = \\alpha\\|u\\|_1 = \\alpha\\sum_u |u_i| $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set up the fista algorithm as we did before. But replace the `IndicatorBox()` with `alpha * L1Norm()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 30\n",
    "myFISTAL1 = FISTA(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uncomment and run the cells to see the solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'snippets/01_ex1_a.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run 300 iterations of `myFISTAL1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'snippets/01_ex1_b.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the solution of L1 regularised least-squares produced by FISTA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(myFISTAL1.solution, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all small values of the background, and the lowest-density parts of the sample, have been forced to be zero by the sparsity regularisation term, keeping only the pixel values of the largest magnitude. Sparsity regularisation does not directly enforce smoothing, which is seen in the image by neighbouring pixel values being rather different in the part of the image that is not zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use FISTA to solve a Least Squares problem with Total Variation regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a better option is to enforce sparsity of the image gradient. This is known as Total Variation (TV) regularisation and tends to enforce piecewise constant areas separated by sharp edges. Recall that for example Tikhonov regularisation may reduce noise but tends to blur edges, so TV may have an advantage if the image to be reconstructed is known to consist of relatively homogeneous areas separated by sharp edges. In CIL, TV is available as the `TotalVariation` function and from the CCPi regularisation toolkit as `FGP_TV` which can be run on the `GPU`. We can set up and solve the TV-regularised problem in the same way as before:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Change the strength of the regularisation and see what effect high and very low values of alpha have on the reconstruction. Try to find a value that smooths the streaks but preserves the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load snippets/01_ex2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "GTV = alpha*FGP_TV(device='gpu', nonnegativity=True) \n",
    "myFISTATV = FISTA(f=F, \n",
    "                  g=GTV, \n",
    "                  initial=x0 ,\n",
    "                  update_objective_interval = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "Show the slices of the TV reconstruction by FISTA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the proximal mapping of Total Variation is not simple but needs to be evaluated numerically, but this is handled by the `TotalVariation` and `FGP_TV` functions, however it does take a while to run which is why we use the `GPU` implementation on real data (approximately 3 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFISTATV.run(200,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(myFISTATV.solution, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that TV-regularisation successfully compensates for the streak artifacts caused by few projections, suppresses noise and preserves sharp edges in the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if math.isclose(myFISTATV.g.alpha, 0.02, abs_tol=0.006) and myFISTATV.iteration > 199:\n",
    "    print(\"Good job, carry on!\")\n",
    "else:\n",
    "    raise ValueError(\"Try again: run 200 iterations with alpha of 0.02\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PDHG to solve a Least Squares problem with Total Variation regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even more flexible algorithm for non-smooth problems is the Primal Dual Hybrid Gradient (PDHG) algorithm, which also goes under other names such as the Chambolle-Pock algorithm. In PDHG we can split complicated functionals into simpler parts for which the proximal mapping can be evaluated. PDHG will be covered in more detail in a separate notebook [03_PDHG.ipynb](03_PDHG.ipynb), here it is demonstrated how to set up the same TV-regularised problem we just solved with FISTA. Note how `BlockFunctions` and `BlockOperators` are used to specify multiple terms/operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.02\n",
    "F = BlockFunction(L2NormSquared(b=b), \n",
    "                  alpha*MixedL21Norm())\n",
    "K = BlockOperator(A, \n",
    "                  D)\n",
    "G = IndicatorBox(lower=0.0)\n",
    "myPDHG = PDHG(f=F, \n",
    "              g=G, \n",
    "              operator=K, \n",
    "              update_objective_interval = 10,\n",
    "              check_convergence = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the algorithm for a specified number of iterations with increased verbosity/amount of printing to screen.\n",
    "\n",
    "Here we run for 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPDHG.run(500,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the TV-regularised solution obtained by the PDHG Algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(myPDHG.solution, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence of the algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIL Algorithms can record history of objective values (primal and dual for PDHG) for monitoring convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.dual_objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.primal_dual_gap[1:])\n",
    "plt.ylim((1e0,1e4))\n",
    "plt.legend(['PDHG primal','PDHG dual','PDHG gap'])\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Objective value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDHG supplies the primal objective, the dual objective, and the primal-dual gap (difference of primal of dual objectives) which helps for monitoring convergence. We see that the primal-dual gap is tending towards zero as the algorithm approaches the solution. \n",
    "\n",
    "Although in practice fewer iterations are often sufficient for a visually converged image to see a very well converged primal-dual gap we want to run for more iterations. It will take a few minutes to run an additional 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPDHG.run(500,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show2D(myPDHG.solution, \n",
    "     slice_list=slices, cmap='inferno', fix_range=(ca1,ca2), origin='upper-left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the resulting image matches the FISTA solution, and plotting the conververgence we observe that the two algorithms achieve approximately the same (primal) objective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.loglog(myFISTATV.iterations[1:], myFISTATV.objective[1:])\n",
    "plt.loglog(myPDHG.iterations[1:], myPDHG.objective[1:])\n",
    "plt.legend(['FISTA','PDHG primal'])\n",
    "plt.grid()\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Objective value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil_demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
