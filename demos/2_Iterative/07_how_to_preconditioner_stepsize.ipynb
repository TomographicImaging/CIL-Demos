{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#  Copyright 2024 -  United Kingdom Research and Innovation\n",
    "#  Copyright 2024 -  The University of Manchester\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "#\n",
    "#  Authored by:    Margaret Duff (STFC-UKRI)\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIL Preconditioner and step size methods demonstration \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cil.utilities import dataexample\n",
    "from cil.utilities.display import show2D\n",
    "from cil.recon import FDK\n",
    "from cil.processors import TransmissionAbsorptionConverter, Slicer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from cil.plugins.tigre import ProjectionOperator\n",
    "from cil.optimisation.algorithms import GD\n",
    "from cil.optimisation.functions import LeastSquares, L2NormSquared\n",
    "from cil.optimisation.operators import MatrixOperator\n",
    "from cil.optimisation.utilities import callbacks, StepSizeMethods, preconditioner\n",
    "from cil.framework import  VectorData\n",
    "\n",
    "\n",
    "# set up default colour map for visualisation\n",
    "cmap = \"gray\"\n",
    "\n",
    "# set the backend for FBP and the ProjectionOperator\n",
    "device = 'gpu'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "In this example, we utilize CIL's simulated sphere data. To accelerate computations in this notebook, we extract a 2D slice from the 3D dataset. Additionally, we select a subset of angles to create a limited-angle reconstruction scenario. We will then compare the ground truth data with a filtered back projection (FBP) reconstruction under these limited-angle conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = dataexample.SIMULATED_SPHERE_VOLUME.get()\n",
    "\n",
    "data = dataexample.SIMULATED_CONE_BEAM_DATA.get()\n",
    "\n",
    "data = data.get_slice(vertical='centre')\n",
    "ground_truth = ground_truth.get_slice(vertical='centre')\n",
    "\n",
    "absorption = TransmissionAbsorptionConverter()(data)\n",
    "absorption = Slicer(roi={'angle':(0, -1, 5)})(absorption)\n",
    "\n",
    "ig = ground_truth.geometry\n",
    "\n",
    "recon = FDK(absorption, image_geometry=ig).run()\n",
    "show2D([ground_truth, recon], title = ['Ground Truth', 'FDK Reconstruction'], origin = 'upper', num_cols = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent with a fixed step size \n",
    "\n",
    "We first consider regularising this limited angle CT reconstruction problem with Tikhonov regularisation:\n",
    "$$ \\arg\\min_x \\|Ax-y\\|_2^2 + \\alpha \\|x\\|_2^2 $$ \n",
    "where $x$ is the image we wish to reconstruct, $A$ the forward CT operator and $y$ the measured data. The regularisation parameter $\\alpha$ is chosen to balance the first, data discrepancy, term and the second, regularisation, term. \n",
    "\n",
    "As a starting point, consider solving this optimisation problem with an arbitrary fixed step size, 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1   \n",
    "A = ProjectionOperator(image_geometry=ig, \n",
    "                       acquisition_geometry=absorption.geometry)\n",
    "\n",
    "F = 0.5*LeastSquares(A = A, b = absorption)+ alpha*L2NormSquared()\n",
    "algo_GD_fixed=GD(initial=ig.allocate(0), objective_function=F, step_size=1e-6)\n",
    "algo_GD_fixed.run(50)\n",
    "show2D([ground_truth, recon, algo_GD_fixed.solution], title = ['Ground Truth', 'FDK Reconstruction', 'L2 regularised solution'], origin = 'upper', num_cols = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the objective value, $\\|Ax-y\\|_2^2 + \\alpha \\|x\\|_2^2$, against iteration number to look at the speed of convergence of this algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2,51),algo_GD_fixed.objective[2:], label='Fixed step size = 1e-6')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a more sensible choice of fixed step size, the reciprocal of the Lipschitz constant of $\\|Ax-y\\|_2^2 + \\alpha \\|x\\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "algo_GD_lip=GD(initial=ig.allocate(0), objective_function=F, step_size=1/F.L )\n",
    "algo_GD_lip.run(50)\n",
    "show2D([ground_truth, recon, algo_GD_lip.solution], title = ['Ground Truth', 'FDK Reconstruction', 'L2 regularised solution'], origin = 'upper', num_cols = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two step size choices, we can see that the reciprocal of the Lipschitz constant provides faster convergence rates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2,51),algo_GD_fixed.objective[2:], label='Fixed step size = 1e-6')\n",
    "plt.plot(range(2,51),algo_GD_lip.objective[2:], label='Fixed step size = 1/L')\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent default behaviour \n",
    "\n",
    "The default behaviour of gradient descent is to use the Armijo step size rule. This is a \"backtracking\" line search method that iteratively reduces the step size until a sufficient decrease in the objective function is achieved. The Armijo rule ensures that the step size chosen at each iteration satisfies the condition:\n",
    "\n",
    "$$ f(x_k + \\alpha_k \\nabla f(x_k)) \\leq f(x_k) + c \\alpha_k \\nabla f(x_k)^T f(x_k) $$\n",
    "\n",
    "where $ f $ is the objective function, $ x_k $ is the current point, $ \\alpha_k $ is the step size, $\\nabla f(x_k)$ is the search direction, and $c $ is a constant typically chosen in the interval $ (0, 1) $. This condition guarantees that the step size provides a sufficient decrease in the objective function, balancing between making progress and ensuring stability in the optimization process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "A = ProjectionOperator(image_geometry=ig, \n",
    "                       acquisition_geometry=absorption.geometry)\n",
    "\n",
    "F = 0.5*LeastSquares(A = A, b = absorption)+ alpha*L2NormSquared()\n",
    "algo_default=GD(initial=ig.allocate(0), objective_function=F , alpha=1e8) # TODO: once #1934 is merged remove the alpha argument\n",
    "algo_default.run(50)\n",
    "show2D([ground_truth, recon, algo_default.solution], title = ['Ground Truth', 'FDK Reconstruction', 'Tik solution'], origin = 'upper', num_cols = 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not work because in 40 iterations, the Armijo step size rule as not found a suitable step size. We can alter the number of iterations in the step size rule to allow it to run without error: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2,51),algo_GD_fixed.objective[2:], label='Fixed step size = 1e-6')\n",
    "plt.plot(range(2,51),algo_GD_lip.objective[2:], label='Fixed step size = 1/L')\n",
    "plt.plot(range(2,51),algo_default.objective[2:], label='Armijo rule')\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see faster convergence for the default step size rule, with the Armijo backtracking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent custom step size rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `shrinking_step_size` class is a custom implementation of a step size rule for optimization algorithms, inheriting from the `StepSizeMethods.StepSizeRule` base class. This class defines a step size that decreases multiplicatively with each iteration of the algorithm, which can be useful for ensuring convergence in iterative optimization methods.\n",
    "\n",
    "Constructor:\n",
    "- `__init__(self, initial=0.1, shrinkage=0.999)`: Initializes the step size rule with an initial step size and a shrinkage factor.\n",
    "  - `initial` (float): The initial step size to be used in the first iteration. Default is `0.1`.\n",
    "  - `shrinkage` (float): The factor by which the step size is multiplied at each iteration. Default is `0.999`.\n",
    "\n",
    "Methods:\n",
    "- `get_step_size(self, algorithm)`: Computes the step size for the current iteration of the algorithm.\n",
    "  - `algorithm` (object): The optimization algorithm instance, which is expected to have an `iteration` attribute indicating the current iteration number.\n",
    "  - Returns: The step size for the current iteration, calculated as `initial * shrinkage^iteration`.\n",
    "\n",
    "This class is particularly useful in scenarios where a gradually decreasing step size is desired to ensure that the optimization algorithm makes smaller adjustments as it approaches a solution, thereby improving stability and convergence. It is also useful in cases where the Lipschitz constant is not available or expensive to calculate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class shrinking_step_size(StepSizeMethods.StepSizeRule):\n",
    "    def __init__(self, initial=0.1, shrinkage=0.99):\n",
    "        self.shrinkage=shrinkage\n",
    "        self.initial=initial\n",
    "    \n",
    "    def get_step_size(self, algorithm):\n",
    "        return self.initial*self.shrinkage**algorithm.iteration\n",
    "    \n",
    "alpha=0.1\n",
    "A = ProjectionOperator(image_geometry=ig, \n",
    "                       acquisition_geometry=absorption.geometry)\n",
    "\n",
    "F = 0.5*LeastSquares(A = A, b = absorption)+ alpha*L2NormSquared()\n",
    "algo_custom=GD(initial=ig.allocate(0), objective_function=F, step_size=shrinking_step_size(initial=1e-6) )\n",
    "algo_custom.run(50)\n",
    "show2D([ground_truth, recon, algo_custom.solution], title = ['Ground Truth', 'FDK Reconstruction', 'Tik solution'], origin = 'upper', num_cols = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(5,51),algo_GD_fixed.objective[5:], label='Fixed step size = 1e-6')\n",
    "plt.plot(range(5,51),algo_GD_lip.objective[5:], label='Fixed step size = 1/L')\n",
    "plt.plot(range(5,51),algo_default.objective[5:], label='Armijio rule')\n",
    "plt.plot(range(5,51),algo_custom.objective[5:], label='Custom rule')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that within 15 iterations, the custom step size rule is able to achieve a similar objective value to the Armijo rule, without the additional calculations of the objective and without knowing the Lipschitz constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preconditioners \n",
    "\n",
    "To explain the concept of preconditioners, first look at the following toy problem. \n",
    "\n",
    "Consider solving, $$Ax^*=b$$ such that  $b=(0,0)^T$, $A=\\begin{pmatrix}\n",
    "1 & 0.0 \\\\\n",
    "0.0 & 0.1 \n",
    "\\end{pmatrix}$. \n",
    "\n",
    "The unique solution to this is $x^*=(0,0)^T$. \n",
    "\n",
    "To visualise this problem we can plot the contours of  $f(x)=\\| Ax-b\\|_2^2$ for $x=[x_1,x_2]^T$ and see the minimum point, the green star,  at $x^*=[x_1^*,x_2^*]^T=(0,0)$. \n",
    "\n",
    "Note: The contour plot is a bit hard to interpret: the more yellow the lines the higher that point is above the minimum, the bottom of the valley. All points on the same line are the same height. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return np.linalg.norm(np.matmul(np.array([[1,0.0],[0.0, 0.1]]), np.array([x,y]))-np.array([0,0]))\n",
    "\n",
    "\n",
    "x_ = np.linspace(-0.5, 0.5, num=200)\n",
    "y_ = np.linspace(-0.5, 0.5, num=200)\n",
    "x,y = np.meshgrid(x_, y_)\n",
    "\n",
    "levels = np.zeros((200,200))\n",
    "for i in range(200):\n",
    "    for j in range(200):\n",
    "        levels[i,j]=f(y_[j], x_[i])\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf=ax.plot_surface(x, y, levels, cmap='viridis')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "c = plt.contour(x, y, levels, 30)\n",
    "plt.scatter([0], [0],marker='*', color='green', s=100)\n",
    "plt.colorbar()\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Non-preconditioned loss landscape')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason this is an ellipse is due to the fact that the matrix $A$ is ill-conditions, it acts with a greater magnitude in some directions than other directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find a solution to this inverse problem by gradient descent, minimising $f(x)=\\| Ax-b\\|_2^2$ and plot the result using a custom callback. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class plot_iterates(callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, f):\n",
    "      x_ = np.linspace(-0.5, 0.5, num=200)\n",
    "      y_ = np.linspace(-0.5, 0.5, num=200)\n",
    "      x,y = np.meshgrid(x_, y_)\n",
    "\n",
    "      levels = np.zeros((200,200))\n",
    "      for i in range(200):\n",
    "          for j in range(200):\n",
    "              levels[i,j]=f(y_[j], x_[i])\\\n",
    "                \n",
    "      plt.contour(x, y, levels, 30)\n",
    "      plt.colorbar()\n",
    "      plt.xlabel('x1')\n",
    "      plt.ylabel('x2')\n",
    "      plt.scatter([0], [0],marker='*', color='green', s=100)\n",
    "      self.save_points_x=[]\n",
    "      self.save_points_y=[]\n",
    "    \n",
    "    def __call__(self, algorithm):\n",
    "      self.save_points_x.append(algorithm.solution.as_array()[0])\n",
    "      self.save_points_y.append(algorithm.solution.as_array()[1])\n",
    "      plt.plot(self.save_points_x, self.save_points_y, color='red', marker='o')\n",
    "  \n",
    "\n",
    "\n",
    "initial = VectorData(np.array([0.3,0.4]))\n",
    "b = VectorData(np.array([0.,0.]))\n",
    "A = MatrixOperator(np.array([[1.,0.0],[0., 0.1]]))\n",
    "F = 0.5*LeastSquares(A = A, b = b)\n",
    "cb=plot_iterates(f)\n",
    "algo=GD(initial=initial, objective_function=F, step_size=1/F.L)\n",
    "algo.run(50, callbacks=[cb])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see after an initial large step, the algorithm now slows down as it hits the centre valley of the objective function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We precondition by the sensitivity of the matrix $A$ given by a vector $1/(A^T \\mathbf{1})$ and can see that this stretches the loss landscape, making it \"rounder\" with less narrow valleys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "precon=preconditioner.Sensitivity(operator=MatrixOperator(np.array([[1,0.0],[0.0, 0.1]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_precon(x,y):\n",
    "    return np.linalg.norm(np.sqrt(precon.array.as_array())*(np.matmul(np.array([[1,0.0],[0.0, 0.1]]), np.array([x,y]))-np.array([0,0])))\n",
    "plt.figure()\n",
    "\n",
    "x_ = np.linspace(-0.5, 0.5, num=200)\n",
    "y_ = np.linspace(-0.5, 0.5, num=200)\n",
    "x,y = np.meshgrid(x_, y_)\n",
    "\n",
    "levels = np.zeros((200,200))\n",
    "for i in range(200):\n",
    "    for j in range(200):\n",
    "        levels[i,j]=f_precon(y_[j], x_[i])\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "surf=ax.plot_surface(x, y, levels, cmap='viridis')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "c = plt.contour(x, y, levels, 30)\n",
    "plt.scatter([0], [0],marker='*', color='green', s=100)\n",
    "plt.colorbar()\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Non-preconditioned loss landscape')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can plot the iterates of preconditioned gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial = VectorData(np.array([0.3,0.4]))\n",
    "b = VectorData(np.array([0.,0.]))\n",
    "A = MatrixOperator(np.array([[1.,0],[0., 0.1]]))\n",
    "F = 0.5*LeastSquares(A = A, b = b)\n",
    "\n",
    "cb=plot_iterates(f_precon)\n",
    "algo_precon=GD(initial=initial, objective_function=F,  preconditioner=preconditioner.Sensitivity(operator=A), step_size=(1/F.L))\n",
    "algo_precon.run(50, callbacks=[cb])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preconditioned gradient descent converges quicker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,51),algo.objective[1:], label='Not preconditioned')\n",
    "plt.plot(range(1,51),algo_precon.objective[1:], label='Preconditioned')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective value')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a callback, we can see the progress of the algorithm and we see that the initial steps of the preconditioned algorithm get is much closer than in the non-preconditioned case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preconditioning CT example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return to our CT example above. Recall solving the least squares with Tikhonov regularisation objective with gradient descent with default step sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "A = ProjectionOperator(image_geometry=ig, \n",
    "                       acquisition_geometry=absorption.geometry)\n",
    "\n",
    "F = 0.5*LeastSquares(A = A, b = absorption)+ alpha*L2NormSquared()\n",
    "algo=GD(initial=ig.allocate(0), objective_function=F , alpha=1e8) # TODO: once #1934 is merged remove the alpha argument\n",
    "algo.run(100)\n",
    "show2D([ground_truth, recon, algo.solution], title = ['Ground Truth', 'FDK Reconstruction', 'Tik solution'], origin = 'upper', num_cols = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now add a preconditioner. This time a CIL default `AdaptiveSensitivity`, in each call to the preconditioner the `gradient` is multiplied by $(x+\\delta) /(A^T \\mathbf{1})$ where $A$ is an operator,  $\\mathbf{1}$ is an object in the range of the operator filled with ones. The point $x$ is the current iteration, or a reference image,  and $\\delta$ is a small positive float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "A = ProjectionOperator(image_geometry=ig, \n",
    "                       acquisition_geometry=absorption.geometry)\n",
    "\n",
    "F = 0.5*LeastSquares(A = A, b = absorption)+ alpha*L2NormSquared()\n",
    "precon=preconditioner.AdaptiveSensitivity(operator=A)\n",
    "\n",
    "algo_precon=GD(initial=ig.allocate(0), objective_function=F , preconditioner=precon, alpha=1e8) # TODO: once #1934 is merged remove the alpha argument \n",
    "algo_precon.run(100)\n",
    "show2D([ground_truth, recon, algo.solution], title = ['Ground Truth', 'FDK Reconstruction', 'Tik solution'], origin = 'upper', num_cols = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the objective values of the two algorithms: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(2,101),algo.objective[2:], label='Not preconditioned')\n",
    "plt.plot(range(2,101),algo_precon.objective[2:], label='Preconditioned')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective value')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see slightly improved convergence with the AdaptiveSensitivity preconditioner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil_testing2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
